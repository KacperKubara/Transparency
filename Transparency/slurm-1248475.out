Running SLURM prolog script on indigo51.cluster.local
===============================================================================
Job started on Sun Jan 17 19:34:59 GMT 2021
Job ID          : 1248475
Job name        : run_all_non_english.sh
WorkDir         : /mainfs/home/kjk1n18/Transparency/Transparency
Command         : /mainfs/home/kjk1n18/Transparency/Transparency/batch/run_all_non_english.sh
Partition       : gpu
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : indigo51
Job Output Follows ...
===============================================================================
/tmp/slurmd/job1248475/slurm_script: line 12: activate: No such file or directory
cls_en vanilla_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:35:04,852 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:35:04,853 - type = vanillalstm
INFO - 2021-01-17 19:35:04,853 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:35:04,872 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:35:05,114 - vocab_size = 666
INFO - 2021-01-17 19:35:05,114 - embed_size = 200
INFO - 2021-01-17 19:35:05,114 - hidden_size = 128
INFO - 2021-01-17 19:35:05,114 - pre_embed = None
INFO - 2021-01-17 19:35:08,187 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:35:08,187 - hidden_size = 256
INFO - 2021-01-17 19:35:08,188 - output_size = 1
INFO - 2021-01-17 19:35:08,188 - use_attention = True
INFO - 2021-01-17 19:35:08,188 - regularizer_attention = None
INFO - 2021-01-17 19:35:08,188 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b3f916895d0> and extras set()
INFO - 2021-01-17 19:35:08,188 - attention.type = tanh
INFO - 2021-01-17 19:35:08,188 - type = tanh
INFO - 2021-01-17 19:35:08,188 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b3f916895d0> and extras set()
INFO - 2021-01-17 19:35:08,188 - attention.hidden_size = 256
INFO - 2021-01-17 19:35:08,188 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.702, BCE loss: 0.702, Diversity Loss: 0.416                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.703, BCE loss: 0.703, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.648, BCE loss: 0.648, Diversity Loss: 0.609                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.728, BCE loss: 0.728, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.759, BCE loss: 0.759, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.716, BCE loss: 0.716, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.732, BCE loss: 0.732, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.735, BCE loss: 0.735, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.424                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.423                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.412                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.716, BCE loss: 0.716, Diversity Loss: 0.427                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.457                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.398                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.701, BCE loss: 0.701, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.710, BCE loss: 0.710, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.708, BCE loss: 0.708, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.435                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.408                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.707, BCE loss: 0.707, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.707, BCE loss: 0.707, Diversity Loss: 0.407                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.395                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.658, BCE loss: 0.658, Diversity Loss: 0.423                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.722, BCE loss: 0.722, Diversity Loss: 0.689                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.665, BCE loss: 0.665, Diversity Loss: 0.406                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.582                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.626, BCE loss: 0.626, Diversity Loss: 0.451                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.673, BCE loss: 0.673, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.416                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.780, BCE loss: 0.780, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.648, BCE loss: 0.648, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.597, BCE loss: 0.597, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.734, BCE loss: 0.734, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.664, BCE loss: 0.664, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.521, BCE loss: 0.521, Diversity Loss: 0.668                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.731, BCE loss: 0.731, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.726                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.809, BCE loss: 0.809, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.617, BCE loss: 0.617, Diversity Loss: 0.530                     (Diversity_weight = 0)
{'accuracy': 0.6816816816816816, 'roc_auc': 0.7317360885128539, 'pr_auc': 0.771038647025504, 'conicity_mean': 0.5599236, 'conicity_std': 0.11736499}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.575    0.746      0.682      0.660         0.671
precision    0.696    0.675      0.682      0.686         0.684
recall       0.490    0.832      0.682      0.661         0.682
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7317360885128539
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:35:08_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.470                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.541, BCE loss: 0.541, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.708, BCE loss: 0.708, Diversity Loss: 0.647                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.631, BCE loss: 0.631, Diversity Loss: 0.659                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.544, BCE loss: 0.544, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.573, BCE loss: 0.573, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.623, BCE loss: 0.623, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.620, BCE loss: 0.620, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.598, BCE loss: 0.598, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.621, BCE loss: 0.621, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.665, BCE loss: 0.665, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.622, BCE loss: 0.622, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.596, BCE loss: 0.596, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.533, BCE loss: 0.533, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.615, BCE loss: 0.615, Diversity Loss: 0.629                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.413, BCE loss: 0.413, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.640, BCE loss: 0.640, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.565, BCE loss: 0.565, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.599, BCE loss: 0.599, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.687                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.444, BCE loss: 0.444, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.450, BCE loss: 0.450, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.949, BCE loss: 0.949, Diversity Loss: 0.789                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.522, BCE loss: 0.522, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.463, BCE loss: 0.463, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.567, BCE loss: 0.567, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.570, BCE loss: 0.570, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.631, BCE loss: 0.631, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.627, BCE loss: 0.627, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.658                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.607, BCE loss: 0.607, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.553, BCE loss: 0.553, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.518, BCE loss: 0.518, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.613, BCE loss: 0.613, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.594                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.526, BCE loss: 0.526, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.715, BCE loss: 0.715, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.454, BCE loss: 0.454, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.529, BCE loss: 0.529, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.602, BCE loss: 0.602, Diversity Loss: 0.697                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.464, BCE loss: 0.464, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.510, BCE loss: 0.510, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.594, BCE loss: 0.594, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.541, BCE loss: 0.541, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.578, BCE loss: 0.578, Diversity Loss: 0.617                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.580, BCE loss: 0.580, Diversity Loss: 0.656                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.486, BCE loss: 0.486, Diversity Loss: 0.673                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.655                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.545, BCE loss: 0.545, Diversity Loss: 0.653                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.526, BCE loss: 0.526, Diversity Loss: 0.458                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.645, BCE loss: 0.645, Diversity Loss: 0.645                     (Diversity_weight = 0)
{'accuracy': 0.6666666666666666, 'roc_auc': 0.7732997071265865, 'pr_auc': 0.8177447946534143, 'conicity_mean': 0.5106131, 'conicity_std': 0.12922902}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.700    0.625      0.667      0.663         0.658
precision    0.579    0.845      0.667      0.712         0.728
recall       0.884    0.496      0.667      0.690         0.667
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7732997071265865
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:35:08_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.522, BCE loss: 0.522, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.562, BCE loss: 0.562, Diversity Loss: 0.420                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.456, BCE loss: 0.456, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.410, BCE loss: 0.410, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.377, BCE loss: 0.377, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.580, BCE loss: 0.580, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.460, BCE loss: 0.460, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.544, BCE loss: 0.544, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.687                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.229, BCE loss: 0.229, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.491, BCE loss: 0.491, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.470, BCE loss: 0.470, Diversity Loss: 0.678                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.460, BCE loss: 0.460, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.428, BCE loss: 0.428, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.593, BCE loss: 0.593, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.429, BCE loss: 0.429, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.314, BCE loss: 0.314, Diversity Loss: 0.774                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.423, BCE loss: 0.423, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.544, BCE loss: 0.544, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.534, BCE loss: 0.534, Diversity Loss: 0.584                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.517, BCE loss: 0.517, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.490, BCE loss: 0.490, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.374, BCE loss: 0.374, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.525, BCE loss: 0.525, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.411, BCE loss: 0.411, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.286, BCE loss: 0.286, Diversity Loss: 0.612                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.336, BCE loss: 0.336, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.437, BCE loss: 0.437, Diversity Loss: 0.595                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.595                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.471, BCE loss: 0.471, Diversity Loss: 0.631                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.379, BCE loss: 0.379, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.584, BCE loss: 0.584, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.417, BCE loss: 0.417, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.660, BCE loss: 0.660, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.429, BCE loss: 0.429, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.359, BCE loss: 0.359, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.540, BCE loss: 0.540, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.339, BCE loss: 0.339, Diversity Loss: 0.471                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.470, BCE loss: 0.470, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.390, BCE loss: 0.390, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.706, BCE loss: 0.706, Diversity Loss: 0.692                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.685                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.351, BCE loss: 0.351, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.394, BCE loss: 0.394, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.331, BCE loss: 0.331, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.435, BCE loss: 0.435, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.695                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.396, BCE loss: 0.396, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.361, BCE loss: 0.361, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.383, BCE loss: 0.383, Diversity Loss: 0.579                     (Diversity_weight = 0)
{'accuracy': 0.7037037037037037, 'roc_auc': 0.7808005206638464, 'pr_auc': 0.8237541488412411, 'conicity_mean': 0.5205505, 'conicity_std': 0.124692}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.700    0.708      0.704      0.704         0.704
precision    0.631    0.792      0.704      0.711         0.721
recall       0.786    0.639      0.704      0.713         0.704
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7808005206638464
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:35:08_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.394, BCE loss: 0.394, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.330, BCE loss: 0.330, Diversity Loss: 0.443                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.624                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.391, BCE loss: 0.391, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.366, BCE loss: 0.366, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.289, BCE loss: 0.289, Diversity Loss: 0.735                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.384, BCE loss: 0.384, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.230, BCE loss: 0.230, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.445, BCE loss: 0.445, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.291, BCE loss: 0.291, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.432, BCE loss: 0.432, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.262, BCE loss: 0.262, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.399, BCE loss: 0.399, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.434, BCE loss: 0.434, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.505, BCE loss: 0.505, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.583, BCE loss: 0.583, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.386, BCE loss: 0.386, Diversity Loss: 0.454                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.353, BCE loss: 0.353, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.452, BCE loss: 0.452, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.325, BCE loss: 0.325, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.314, BCE loss: 0.314, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.686                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.407, BCE loss: 0.407, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.450, BCE loss: 0.450, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.322, BCE loss: 0.322, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.336, BCE loss: 0.336, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.305, BCE loss: 0.305, Diversity Loss: 0.471                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.442, BCE loss: 0.442, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.420, BCE loss: 0.420, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.296, BCE loss: 0.296, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.457, BCE loss: 0.457, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.252, BCE loss: 0.252, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.215, BCE loss: 0.215, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.440, BCE loss: 0.440, Diversity Loss: 0.636                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.736, BCE loss: 0.736, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.602, BCE loss: 0.602, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.453, BCE loss: 0.453, Diversity Loss: 0.680                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.369, BCE loss: 0.369, Diversity Loss: 0.595                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.508, BCE loss: 0.508, Diversity Loss: 0.574                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.483, BCE loss: 0.483, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.232, BCE loss: 0.232, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.351, BCE loss: 0.351, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.331, BCE loss: 0.331, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.300, BCE loss: 0.300, Diversity Loss: 0.423                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.397, BCE loss: 0.397, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.344, BCE loss: 0.344, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.328, BCE loss: 0.328, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.293, BCE loss: 0.293, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.483, BCE loss: 0.483, Diversity Loss: 0.624                     (Diversity_weight = 0)
{'accuracy': 0.7287287287287287, 'roc_auc': 0.8044988610478361, 'pr_auc': 0.8333439992446534, 'conicity_mean': 0.53591174, 'conicity_std': 0.12850189}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.667    0.771      0.729      0.719         0.725
precision    0.723    0.732      0.729      0.728         0.728
recall       0.620    0.814      0.729      0.717         0.729
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.8044988610478361
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:35:08_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.266, BCE loss: 0.266, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.661                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.316, BCE loss: 0.316, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.353, BCE loss: 0.353, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.284, BCE loss: 0.284, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.522, BCE loss: 0.522, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.705                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.337, BCE loss: 0.337, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.273, BCE loss: 0.273, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.262, BCE loss: 0.262, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.263, BCE loss: 0.263, Diversity Loss: 0.451                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.225, BCE loss: 0.225, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.416, BCE loss: 0.416, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.505, BCE loss: 0.505, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.418                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.427, BCE loss: 0.427, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.245, BCE loss: 0.245, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.217, BCE loss: 0.217, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.228, BCE loss: 0.228, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.313, BCE loss: 0.313, Diversity Loss: 0.453                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.247, BCE loss: 0.247, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.255, BCE loss: 0.255, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.394, BCE loss: 0.394, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.244, BCE loss: 0.244, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.278, BCE loss: 0.278, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.253, BCE loss: 0.253, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.316, BCE loss: 0.316, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.284, BCE loss: 0.284, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.384, BCE loss: 0.384, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.295, BCE loss: 0.295, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.462, BCE loss: 0.462, Diversity Loss: 0.574                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.174, BCE loss: 0.174, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.442                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.452, BCE loss: 0.452, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.323, BCE loss: 0.323, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.240, BCE loss: 0.240, Diversity Loss: 0.533                     (Diversity_weight = 0)
{'accuracy': 0.7087087087087087, 'roc_auc': 0.7917995444191344, 'pr_auc': 0.8245318505706649, 'conicity_mean': 0.5791734, 'conicity_std': 0.12705475}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.617    0.765      0.709      0.691         0.700
precision    0.731    0.698      0.709      0.715         0.713
recall       0.533    0.846      0.709      0.690         0.709
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7917995444191344
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:35:08_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.595                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.240, BCE loss: 0.240, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.192, BCE loss: 0.192, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.225, BCE loss: 0.225, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.244, BCE loss: 0.244, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.187, BCE loss: 0.187, Diversity Loss: 0.657                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.268, BCE loss: 0.268, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.443                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.165, BCE loss: 0.165, Diversity Loss: 0.471                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.439                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.294, BCE loss: 0.294, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.253, BCE loss: 0.253, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.128, BCE loss: 0.128, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.710                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.504, BCE loss: 0.504, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.240, BCE loss: 0.240, Diversity Loss: 0.663                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.419                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.124, BCE loss: 0.124, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.150, BCE loss: 0.150, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.446                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.266, BCE loss: 0.266, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.396, BCE loss: 0.396, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.504, BCE loss: 0.504, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.244, BCE loss: 0.244, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.519                     (Diversity_weight = 0)
{'accuracy': 0.7247247247247247, 'roc_auc': 0.7908680442564269, 'pr_auc': 0.8220814036633856, 'conicity_mean': 0.5130966, 'conicity_std': 0.11603097}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.685    0.756      0.725      0.720         0.725
precision    0.689    0.752      0.725      0.721         0.724
recall       0.681    0.759      0.725      0.720         0.725
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7908680442564269
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:35:08_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.446                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.226, BCE loss: 0.226, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.150, BCE loss: 0.150, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.110, BCE loss: 0.110, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.613                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.436                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.446                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.427, BCE loss: 0.427, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.439                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.437                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.454                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.433                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.279, BCE loss: 0.279, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.696                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.268, BCE loss: 0.268, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.308, BCE loss: 0.308, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.450                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.442                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.134, BCE loss: 0.134, Diversity Loss: 0.462                     (Diversity_weight = 0)
{'accuracy': 0.7027027027027027, 'roc_auc': 0.7845061828831761, 'pr_auc': 0.8249173493299489, 'conicity_mean': 0.49187848, 'conicity_std': 0.12099554}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.710    0.695      0.703      0.703         0.702
precision    0.621    0.818      0.703      0.720         0.732
recall       0.829    0.604      0.703      0.716         0.703
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7845061828831761
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:35:08_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.409                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.437                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.460                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.649                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.708                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.443                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.466                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.446                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.238, BCE loss: 0.238, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.124, BCE loss: 0.124, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.280, BCE loss: 0.280, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.109, BCE loss: 0.109, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.165, BCE loss: 0.165, Diversity Loss: 0.435                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.098, BCE loss: 0.098, Diversity Loss: 0.401                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.418                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.430                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.301, BCE loss: 0.301, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.272, BCE loss: 0.272, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.400                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.050, BCE loss: 0.050, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.414                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.430                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.138, BCE loss: 0.138, Diversity Loss: 0.411                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.138, BCE loss: 0.138, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.471                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.424                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.429                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.397                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.260, BCE loss: 0.260, Diversity Loss: 0.450                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.424                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.429                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.403                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.451                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.405                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.445                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.392                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.236, BCE loss: 0.236, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.114, BCE loss: 0.114, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.240, BCE loss: 0.240, Diversity Loss: 0.499                     (Diversity_weight = 0)
{'accuracy': 0.7177177177177178, 'roc_auc': 0.7838065408395705, 'pr_auc': 0.8146001321165104, 'conicity_mean': 0.47297096, 'conicity_std': 0.10874877}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.674    0.751      0.718      0.713         0.717
precision    0.684    0.743      0.718      0.713         0.717
recall       0.665    0.759      0.718      0.712         0.718
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7838065408395705
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:35:08_2021
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:36:03,197 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:36:03,197 - type = vanillalstm
INFO - 2021-01-17 19:36:03,198 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:36:03,198 - vocab_size = 666
INFO - 2021-01-17 19:36:03,198 - embed_size = 200
INFO - 2021-01-17 19:36:03,198 - hidden_size = 128
INFO - 2021-01-17 19:36:03,198 - pre_embed = None
INFO - 2021-01-17 19:36:03,213 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:36:03,213 - hidden_size = 256
INFO - 2021-01-17 19:36:03,213 - output_size = 1
INFO - 2021-01-17 19:36:03,214 - use_attention = True
INFO - 2021-01-17 19:36:03,214 - regularizer_attention = None
INFO - 2021-01-17 19:36:03,214 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b3f916b0450> and extras set()
INFO - 2021-01-17 19:36:03,214 - attention.type = tanh
INFO - 2021-01-17 19:36:03,214 - type = tanh
INFO - 2021-01-17 19:36:03,214 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b3f916b0450> and extras set()
INFO - 2021-01-17 19:36:03,214 - attention.hidden_size = 256
INFO - 2021-01-17 19:36:03,214 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7277277277277278, 'roc_auc': 0.8244817229506997, 'pr_auc': 0.7861346043010929, 'conicity_mean': '0.53063166', 'conicity_std': '0.12521784'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.728    0.727      0.728      0.728         0.728
precision    0.829    0.648      0.728      0.739         0.750
recall       0.649    0.829      0.728      0.739         0.728
support    561.000  438.000    999.000    999.000       999.000
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:36:04,658 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:36:04,659 - type = vanillalstm
INFO - 2021-01-17 19:36:04,659 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:36:04,659 - vocab_size = 666
INFO - 2021-01-17 19:36:04,659 - embed_size = 200
INFO - 2021-01-17 19:36:04,659 - hidden_size = 128
INFO - 2021-01-17 19:36:04,659 - pre_embed = None
INFO - 2021-01-17 19:36:04,673 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:36:04,673 - hidden_size = 256
INFO - 2021-01-17 19:36:04,673 - output_size = 1
INFO - 2021-01-17 19:36:04,673 - use_attention = True
INFO - 2021-01-17 19:36:04,673 - regularizer_attention = None
INFO - 2021-01-17 19:36:04,673 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b3f7c5f3b50> and extras set()
INFO - 2021-01-17 19:36:04,674 - attention.type = tanh
INFO - 2021-01-17 19:36:04,674 - type = tanh
INFO - 2021-01-17 19:36:04,674 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b3f7c5f3b50> and extras set()
INFO - 2021-01-17 19:36:04,674 - attention.hidden_size = 256
INFO - 2021-01-17 19:36:04,674 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7277277277277278, 'roc_auc': 0.8244817229506997, 'pr_auc': 0.7861346043010929, 'conicity_mean': '0.53063166', 'conicity_std': '0.12521784'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.728    0.727      0.728      0.728         0.728
precision    0.829    0.648      0.728      0.739         0.750
recall       0.649    0.829      0.728      0.739         0.728
support    561.000  438.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_en ortho_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:36:15,122 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:36:15,122 - type = ortholstm
INFO - 2021-01-17 19:36:15,122 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:36:15,141 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:36:15,382 - vocab_size = 666
INFO - 2021-01-17 19:36:15,382 - embed_size = 200
INFO - 2021-01-17 19:36:15,382 - hidden_size = 128
INFO - 2021-01-17 19:36:15,382 - pre_embed = None
INFO - 2021-01-17 19:36:18,467 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:36:18,468 - hidden_size = 256
INFO - 2021-01-17 19:36:18,468 - output_size = 1
INFO - 2021-01-17 19:36:18,468 - use_attention = True
INFO - 2021-01-17 19:36:18,468 - regularizer_attention = None
INFO - 2021-01-17 19:36:18,468 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b192436b7d0> and extras set()
INFO - 2021-01-17 19:36:18,468 - attention.type = tanh
INFO - 2021-01-17 19:36:18,468 - type = tanh
INFO - 2021-01-17 19:36:18,469 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b192436b7d0> and extras set()
INFO - 2021-01-17 19:36:18,469 - attention.hidden_size = 256
INFO - 2021-01-17 19:36:18,469 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.701, BCE loss: 0.701, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.253                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.701, BCE loss: 0.701, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.705, BCE loss: 0.705, Diversity Loss: 0.100                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.118                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.652, BCE loss: 0.652, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.718, BCE loss: 0.718, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.239                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.644, BCE loss: 0.644, Diversity Loss: 0.285                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.600, BCE loss: 0.600, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.701, BCE loss: 0.701, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.706, BCE loss: 0.706, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.666, BCE loss: 0.666, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.669, BCE loss: 0.669, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.631, BCE loss: 0.631, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.630, BCE loss: 0.630, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.655, BCE loss: 0.655, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.733, BCE loss: 0.733, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.629, BCE loss: 0.629, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.652, BCE loss: 0.652, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.708, BCE loss: 0.708, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.652, BCE loss: 0.652, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.106                     (Diversity_weight = 0)
{'accuracy': 0.5565565565565566, 'roc_auc': 0.6921005532053368, 'pr_auc': 0.723740748397412, 'conicity_mean': 0.17958283, 'conicity_std': 0.046462115}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.634    0.439      0.557      0.536         0.524
precision    0.497    0.755      0.557      0.626         0.642
recall       0.872    0.309      0.557      0.591         0.557
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.6921005532053368
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:36:18_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.589, BCE loss: 0.589, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.268                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.628, BCE loss: 0.628, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.609, BCE loss: 0.609, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.622, BCE loss: 0.622, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.721, BCE loss: 0.721, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.486, BCE loss: 0.486, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.564, BCE loss: 0.564, Diversity Loss: 0.239                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.522, BCE loss: 0.522, Diversity Loss: 0.123                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.753, BCE loss: 0.753, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.562, BCE loss: 0.562, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.258                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.634, BCE loss: 0.634, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.536, BCE loss: 0.536, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.229                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.654, BCE loss: 0.654, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.641, BCE loss: 0.641, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.570, BCE loss: 0.570, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.673, BCE loss: 0.673, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.608, BCE loss: 0.608, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.594, BCE loss: 0.594, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.729, BCE loss: 0.729, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.521, BCE loss: 0.521, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.619, BCE loss: 0.619, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.624, BCE loss: 0.624, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.648, BCE loss: 0.648, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.571, BCE loss: 0.571, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.567, BCE loss: 0.567, Diversity Loss: 0.273                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.522, BCE loss: 0.522, Diversity Loss: 0.301                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.579, BCE loss: 0.579, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.551, BCE loss: 0.551, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.531, BCE loss: 0.531, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.480, BCE loss: 0.480, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.614, BCE loss: 0.614, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.613, BCE loss: 0.613, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.620, BCE loss: 0.620, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.558, BCE loss: 0.558, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.721, BCE loss: 0.721, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.572, BCE loss: 0.572, Diversity Loss: 0.110                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.553, BCE loss: 0.553, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.655, BCE loss: 0.655, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.652, BCE loss: 0.652, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.673, BCE loss: 0.673, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.619, BCE loss: 0.619, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.479, BCE loss: 0.479, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.434, BCE loss: 0.434, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.755, BCE loss: 0.755, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.464, BCE loss: 0.464, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.800, BCE loss: 0.800, Diversity Loss: 0.201                     (Diversity_weight = 0)
{'accuracy': 0.6726726726726727, 'roc_auc': 0.7300032541490401, 'pr_auc': 0.7775275866635267, 'conicity_mean': 0.1844417, 'conicity_std': 0.047466535}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.661    0.683      0.673      0.672         0.674
precision    0.606    0.746      0.673      0.676         0.685
recall       0.727    0.630      0.673      0.679         0.673
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7300032541490401
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:36:18_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.583, BCE loss: 0.583, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.526, BCE loss: 0.526, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.445, BCE loss: 0.445, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.522, BCE loss: 0.522, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.462, BCE loss: 0.462, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.372, BCE loss: 0.372, Diversity Loss: 0.240                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.494, BCE loss: 0.494, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.539, BCE loss: 0.539, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.483, BCE loss: 0.483, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.388, BCE loss: 0.388, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.590, BCE loss: 0.590, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.458, BCE loss: 0.458, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.560, BCE loss: 0.560, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.382, BCE loss: 0.382, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.440, BCE loss: 0.440, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.557, BCE loss: 0.557, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.328, BCE loss: 0.328, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.409, BCE loss: 0.409, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.383, BCE loss: 0.383, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.344, BCE loss: 0.344, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.470, BCE loss: 0.470, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.554, BCE loss: 0.554, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.484, BCE loss: 0.484, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.435, BCE loss: 0.435, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.445, BCE loss: 0.445, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.424, BCE loss: 0.424, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.485, BCE loss: 0.485, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.426, BCE loss: 0.426, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.426, BCE loss: 0.426, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.539, BCE loss: 0.539, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.419, BCE loss: 0.419, Diversity Loss: 0.255                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.473, BCE loss: 0.473, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.312                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.604, BCE loss: 0.604, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.274, BCE loss: 0.274, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.491, BCE loss: 0.491, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.547, BCE loss: 0.547, Diversity Loss: 0.281                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.526, BCE loss: 0.526, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.390, BCE loss: 0.390, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.596, BCE loss: 0.596, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.479, BCE loss: 0.479, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.251                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.382, BCE loss: 0.382, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.479, BCE loss: 0.479, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.575, BCE loss: 0.575, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.592, BCE loss: 0.592, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.391, BCE loss: 0.391, Diversity Loss: 0.284                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.593, BCE loss: 0.593, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.608, BCE loss: 0.608, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.300, BCE loss: 0.300, Diversity Loss: 0.242                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.629, BCE loss: 0.629, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.445, BCE loss: 0.445, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.349, BCE loss: 0.349, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.440, BCE loss: 0.440, Diversity Loss: 0.195                     (Diversity_weight = 0)
{'accuracy': 0.6796796796796797, 'roc_auc': 0.7468434754311748, 'pr_auc': 0.7805827449071145, 'conicity_mean': 0.19147384, 'conicity_std': 0.048005305}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.655    0.701       0.68      0.678         0.681
precision    0.622    0.735       0.68      0.678         0.685
recall       0.692    0.670       0.68      0.681         0.680
support    439.000  560.000     999.00    999.000       999.000
Model Saved on  roc_auc 0.7468434754311748
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:36:18_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.393, BCE loss: 0.393, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.296, BCE loss: 0.296, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.332, BCE loss: 0.332, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.332, BCE loss: 0.332, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.331, BCE loss: 0.331, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.255, BCE loss: 0.255, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.255                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.294, BCE loss: 0.294, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.319, BCE loss: 0.319, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.397, BCE loss: 0.397, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.335, BCE loss: 0.335, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.210, BCE loss: 0.210, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.393, BCE loss: 0.393, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.245                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.247, BCE loss: 0.247, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.441, BCE loss: 0.441, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.253, BCE loss: 0.253, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.257, BCE loss: 0.257, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.209, BCE loss: 0.209, Diversity Loss: 0.237                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.572, BCE loss: 0.572, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.254, BCE loss: 0.254, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.681, BCE loss: 0.681, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.353, BCE loss: 0.353, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.536, BCE loss: 0.536, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.273, BCE loss: 0.273, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.479, BCE loss: 0.479, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.399, BCE loss: 0.399, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.307, BCE loss: 0.307, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.542, BCE loss: 0.542, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.562, BCE loss: 0.562, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.364, BCE loss: 0.364, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.314, BCE loss: 0.314, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.320, BCE loss: 0.320, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.254, BCE loss: 0.254, Diversity Loss: 0.257                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.335, BCE loss: 0.335, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.273, BCE loss: 0.273, Diversity Loss: 0.278                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.225, BCE loss: 0.225, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.481, BCE loss: 0.481, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.307, BCE loss: 0.307, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.318, BCE loss: 0.318, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.302                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.396, BCE loss: 0.396, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.372, BCE loss: 0.372, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.377, BCE loss: 0.377, Diversity Loss: 0.113                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.264, BCE loss: 0.264, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.340, BCE loss: 0.340, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.373, BCE loss: 0.373, Diversity Loss: 0.265                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.200, BCE loss: 0.200, Diversity Loss: 0.100                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.344, BCE loss: 0.344, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.193                     (Diversity_weight = 0)
{'accuracy': 0.6936936936936937, 'roc_auc': 0.7619386592905955, 'pr_auc': 0.7917816922193961, 'conicity_mean': 0.1892779, 'conicity_std': 0.047031175}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.640    0.733      0.694      0.687         0.692
precision    0.662    0.716      0.694      0.689         0.692
recall       0.620    0.752      0.694      0.686         0.694
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7619386592905955
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:36:18_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.225, BCE loss: 0.225, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.220, BCE loss: 0.220, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.252, BCE loss: 0.252, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.219, BCE loss: 0.219, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.242, BCE loss: 0.242, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.305, BCE loss: 0.305, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.216, BCE loss: 0.216, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.210, BCE loss: 0.210, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.326, BCE loss: 0.326, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.309                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.323, BCE loss: 0.323, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.210, BCE loss: 0.210, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.575, BCE loss: 0.575, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.224, BCE loss: 0.224, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.238                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.124, BCE loss: 0.124, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.165, BCE loss: 0.165, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.297, BCE loss: 0.297, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.104, BCE loss: 0.104, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.302, BCE loss: 0.302, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.310, BCE loss: 0.310, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.252, BCE loss: 0.252, Diversity Loss: 0.117                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.254, BCE loss: 0.254, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.329, BCE loss: 0.329, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.312, BCE loss: 0.312, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.339, BCE loss: 0.339, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.266                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.241, BCE loss: 0.241, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.245                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.155, BCE loss: 0.155, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.229, BCE loss: 0.229, Diversity Loss: 0.245                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.310, BCE loss: 0.310, Diversity Loss: 0.120                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.233, BCE loss: 0.233, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.165, BCE loss: 0.165, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.159, BCE loss: 0.159, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.264, BCE loss: 0.264, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.355, BCE loss: 0.355, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.198, BCE loss: 0.198, Diversity Loss: 0.275                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.344, BCE loss: 0.344, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.241, BCE loss: 0.241, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.210, BCE loss: 0.210, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.118, BCE loss: 0.118, Diversity Loss: 0.096                     (Diversity_weight = 0)
{'accuracy': 0.6936936936936937, 'roc_auc': 0.7437805076472502, 'pr_auc': 0.7771174309716309, 'conicity_mean': 0.18694341, 'conicity_std': 0.046272714}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.674    0.711      0.694      0.693         0.695
precision    0.633    0.754      0.694      0.694         0.701
recall       0.720    0.673      0.694      0.697         0.694
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7437805076472502
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:36:18_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.204, BCE loss: 0.204, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.188, BCE loss: 0.188, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.096                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.094, BCE loss: 0.094, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.172, BCE loss: 0.172, Diversity Loss: 0.123                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.239                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.249, BCE loss: 0.249, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.253                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.296, BCE loss: 0.296, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.275                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.228                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.253                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.267                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.302                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.114                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.206, BCE loss: 0.206, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.188                     (Diversity_weight = 0)
{'accuracy': 0.7037037037037037, 'roc_auc': 0.7509396355353075, 'pr_auc': 0.7736236631577197, 'conicity_mean': 0.18919496, 'conicity_std': 0.04616754}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.682    0.722      0.704      0.702         0.705
precision    0.645    0.761      0.704      0.703         0.710
recall       0.724    0.688      0.704      0.706         0.704
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7509396355353075
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:36:18_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.260                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.159, BCE loss: 0.159, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.251                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.248                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.224, BCE loss: 0.224, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.241                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.270, BCE loss: 0.270, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.299                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.240, BCE loss: 0.240, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.118, BCE loss: 0.118, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.229                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.127, BCE loss: 0.127, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.096                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.430, BCE loss: 0.430, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.114                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.276                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.167, BCE loss: 0.167, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.223                     (Diversity_weight = 0)
{'accuracy': 0.7057057057057057, 'roc_auc': 0.7364017246989912, 'pr_auc': 0.7580342759115412, 'conicity_mean': 0.18576019, 'conicity_std': 0.04505778}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.670    0.734      0.706      0.702         0.706
precision    0.660    0.744      0.706      0.702         0.707
recall       0.681    0.725      0.706      0.703         0.706
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7364017246989912
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:36:18_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.207, BCE loss: 0.207, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.303                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.270                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.207, BCE loss: 0.207, Diversity Loss: 0.234                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.165, BCE loss: 0.165, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.268, BCE loss: 0.268, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.273                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.094, BCE loss: 0.094, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.247                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.008, BCE loss: 0.008, Diversity Loss: 0.101                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.239                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.094, BCE loss: 0.094, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.111                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.225                     (Diversity_weight = 0)
{'accuracy': 0.6866866866866866, 'roc_auc': 0.7386918320859095, 'pr_auc': 0.751008626094023, 'conicity_mean': 0.18586007, 'conicity_std': 0.04446799}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.625    0.731      0.687      0.678         0.684
precision    0.659    0.705      0.687      0.682         0.685
recall       0.595    0.759      0.687      0.677         0.687
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7386918320859095
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:36:18_2021
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:85: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:86: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:37:50,246 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:37:50,246 - type = ortholstm
INFO - 2021-01-17 19:37:50,246 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:37:50,246 - vocab_size = 666
INFO - 2021-01-17 19:37:50,247 - embed_size = 200
INFO - 2021-01-17 19:37:50,247 - hidden_size = 128
INFO - 2021-01-17 19:37:50,247 - pre_embed = None
INFO - 2021-01-17 19:37:50,261 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:37:50,262 - hidden_size = 256
INFO - 2021-01-17 19:37:50,262 - output_size = 1
INFO - 2021-01-17 19:37:50,262 - use_attention = True
INFO - 2021-01-17 19:37:50,262 - regularizer_attention = None
INFO - 2021-01-17 19:37:50,262 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b1924399390> and extras set()
INFO - 2021-01-17 19:37:50,262 - attention.type = tanh
INFO - 2021-01-17 19:37:50,262 - type = tanh
INFO - 2021-01-17 19:37:50,262 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b1924399390> and extras set()
INFO - 2021-01-17 19:37:50,262 - attention.hidden_size = 256
INFO - 2021-01-17 19:37:50,262 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7227227227227228, 'roc_auc': 0.7873619352265606, 'pr_auc': 0.704037262798243, 'conicity_mean': '0.18717572', 'conicity_std': '0.044550836'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.739    0.704      0.723      0.722         0.724
precision    0.784    0.661      0.723      0.723         0.730
recall       0.699    0.753      0.723      0.726         0.723
support    561.000  438.000    999.000    999.000       999.000
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:37:52,512 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:37:52,512 - type = ortholstm
INFO - 2021-01-17 19:37:52,512 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:37:52,512 - vocab_size = 666
INFO - 2021-01-17 19:37:52,512 - embed_size = 200
INFO - 2021-01-17 19:37:52,513 - hidden_size = 128
INFO - 2021-01-17 19:37:52,513 - pre_embed = None
INFO - 2021-01-17 19:37:52,527 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:37:52,527 - hidden_size = 256
INFO - 2021-01-17 19:37:52,527 - output_size = 1
INFO - 2021-01-17 19:37:52,527 - use_attention = True
INFO - 2021-01-17 19:37:52,527 - regularizer_attention = None
INFO - 2021-01-17 19:37:52,527 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b1924399850> and extras set()
INFO - 2021-01-17 19:37:52,527 - attention.type = tanh
INFO - 2021-01-17 19:37:52,527 - type = tanh
INFO - 2021-01-17 19:37:52,528 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b1924399850> and extras set()
INFO - 2021-01-17 19:37:52,528 - attention.hidden_size = 256
INFO - 2021-01-17 19:37:52,528 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7227227227227228, 'roc_auc': 0.7873619352265606, 'pr_auc': 0.704037262798243, 'conicity_mean': '0.18717572', 'conicity_std': '0.044550836'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.739    0.704      0.723      0.722         0.724
precision    0.784    0.661      0.723      0.723         0.730
recall       0.699    0.753      0.723      0.726         0.723
support    561.000  438.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_en diversity_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:38:05,940 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:38:05,940 - type = vanillalstm
INFO - 2021-01-17 19:38:05,940 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:38:05,960 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:38:06,200 - vocab_size = 666
INFO - 2021-01-17 19:38:06,200 - embed_size = 200
INFO - 2021-01-17 19:38:06,200 - hidden_size = 128
INFO - 2021-01-17 19:38:06,200 - pre_embed = None
INFO - 2021-01-17 19:38:09,273 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:38:09,273 - hidden_size = 256
INFO - 2021-01-17 19:38:09,274 - output_size = 1
INFO - 2021-01-17 19:38:09,274 - use_attention = True
INFO - 2021-01-17 19:38:09,274 - regularizer_attention = None
INFO - 2021-01-17 19:38:09,274 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b0efc2da790> and extras set()
INFO - 2021-01-17 19:38:09,274 - attention.type = tanh
INFO - 2021-01-17 19:38:09,274 - type = tanh
INFO - 2021-01-17 19:38:09,274 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b0efc2da790> and extras set()
INFO - 2021-01-17 19:38:09,274 - attention.hidden_size = 256
INFO - 2021-01-17 19:38:09,275 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 1.020, BCE loss: 0.715, Diversity Loss: 0.609                     (Diversity_weight = 0.5)
Epoch: 0 Step: 1 Total Loss: 0.911, BCE loss: 0.674, Diversity Loss: 0.474                     (Diversity_weight = 0.5)
Epoch: 0 Step: 2 Total Loss: 0.897, BCE loss: 0.672, Diversity Loss: 0.450                     (Diversity_weight = 0.5)
Epoch: 0 Step: 3 Total Loss: 0.904, BCE loss: 0.714, Diversity Loss: 0.380                     (Diversity_weight = 0.5)
Epoch: 0 Step: 4 Total Loss: 0.905, BCE loss: 0.697, Diversity Loss: 0.416                     (Diversity_weight = 0.5)
Epoch: 0 Step: 5 Total Loss: 0.843, BCE loss: 0.579, Diversity Loss: 0.529                     (Diversity_weight = 0.5)
Epoch: 0 Step: 6 Total Loss: 0.850, BCE loss: 0.702, Diversity Loss: 0.296                     (Diversity_weight = 0.5)
Epoch: 0 Step: 7 Total Loss: 0.835, BCE loss: 0.694, Diversity Loss: 0.283                     (Diversity_weight = 0.5)
Epoch: 0 Step: 8 Total Loss: 0.855, BCE loss: 0.698, Diversity Loss: 0.314                     (Diversity_weight = 0.5)
Epoch: 0 Step: 9 Total Loss: 0.822, BCE loss: 0.689, Diversity Loss: 0.266                     (Diversity_weight = 0.5)
Epoch: 0 Step: 10 Total Loss: 0.919, BCE loss: 0.690, Diversity Loss: 0.457                     (Diversity_weight = 0.5)
Epoch: 0 Step: 11 Total Loss: 0.810, BCE loss: 0.685, Diversity Loss: 0.251                     (Diversity_weight = 0.5)
Epoch: 0 Step: 12 Total Loss: 0.789, BCE loss: 0.677, Diversity Loss: 0.225                     (Diversity_weight = 0.5)
Epoch: 0 Step: 13 Total Loss: 0.803, BCE loss: 0.693, Diversity Loss: 0.220                     (Diversity_weight = 0.5)
Epoch: 0 Step: 14 Total Loss: 0.844, BCE loss: 0.702, Diversity Loss: 0.283                     (Diversity_weight = 0.5)
Epoch: 0 Step: 15 Total Loss: 0.858, BCE loss: 0.682, Diversity Loss: 0.353                     (Diversity_weight = 0.5)
Epoch: 0 Step: 16 Total Loss: 0.798, BCE loss: 0.686, Diversity Loss: 0.222                     (Diversity_weight = 0.5)
Epoch: 0 Step: 17 Total Loss: 0.796, BCE loss: 0.700, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 0 Step: 18 Total Loss: 0.798, BCE loss: 0.689, Diversity Loss: 0.219                     (Diversity_weight = 0.5)
Epoch: 0 Step: 19 Total Loss: 0.858, BCE loss: 0.703, Diversity Loss: 0.310                     (Diversity_weight = 0.5)
Epoch: 0 Step: 20 Total Loss: 0.823, BCE loss: 0.695, Diversity Loss: 0.256                     (Diversity_weight = 0.5)
Epoch: 0 Step: 21 Total Loss: 0.867, BCE loss: 0.697, Diversity Loss: 0.341                     (Diversity_weight = 0.5)
Epoch: 0 Step: 22 Total Loss: 0.796, BCE loss: 0.661, Diversity Loss: 0.270                     (Diversity_weight = 0.5)
Epoch: 0 Step: 23 Total Loss: 0.785, BCE loss: 0.687, Diversity Loss: 0.196                     (Diversity_weight = 0.5)
Epoch: 0 Step: 24 Total Loss: 0.782, BCE loss: 0.678, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 0 Step: 25 Total Loss: 0.809, BCE loss: 0.703, Diversity Loss: 0.213                     (Diversity_weight = 0.5)
Epoch: 0 Step: 26 Total Loss: 0.795, BCE loss: 0.690, Diversity Loss: 0.208                     (Diversity_weight = 0.5)
Epoch: 0 Step: 27 Total Loss: 0.804, BCE loss: 0.704, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 0 Step: 28 Total Loss: 0.800, BCE loss: 0.683, Diversity Loss: 0.235                     (Diversity_weight = 0.5)
Epoch: 0 Step: 29 Total Loss: 0.782, BCE loss: 0.694, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 0 Step: 30 Total Loss: 0.779, BCE loss: 0.690, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 0 Step: 31 Total Loss: 0.783, BCE loss: 0.688, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 0 Step: 32 Total Loss: 0.823, BCE loss: 0.676, Diversity Loss: 0.295                     (Diversity_weight = 0.5)
Epoch: 0 Step: 33 Total Loss: 0.775, BCE loss: 0.686, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 0 Step: 34 Total Loss: 0.771, BCE loss: 0.692, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 0 Step: 35 Total Loss: 0.768, BCE loss: 0.676, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 0 Step: 36 Total Loss: 0.796, BCE loss: 0.688, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 0 Step: 37 Total Loss: 0.855, BCE loss: 0.687, Diversity Loss: 0.335                     (Diversity_weight = 0.5)
Epoch: 0 Step: 38 Total Loss: 0.772, BCE loss: 0.699, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 0 Step: 39 Total Loss: 0.794, BCE loss: 0.679, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 0 Step: 40 Total Loss: 0.763, BCE loss: 0.670, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 0 Step: 41 Total Loss: 0.771, BCE loss: 0.668, Diversity Loss: 0.205                     (Diversity_weight = 0.5)
Epoch: 0 Step: 42 Total Loss: 0.767, BCE loss: 0.692, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 0 Step: 43 Total Loss: 0.804, BCE loss: 0.675, Diversity Loss: 0.257                     (Diversity_weight = 0.5)
Epoch: 0 Step: 44 Total Loss: 0.762, BCE loss: 0.685, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 0 Step: 45 Total Loss: 0.765, BCE loss: 0.687, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 0 Step: 46 Total Loss: 0.787, BCE loss: 0.656, Diversity Loss: 0.262                     (Diversity_weight = 0.5)
Epoch: 0 Step: 47 Total Loss: 0.760, BCE loss: 0.687, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 0 Step: 48 Total Loss: 0.762, BCE loss: 0.674, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 0 Step: 49 Total Loss: 0.751, BCE loss: 0.685, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 0 Step: 50 Total Loss: 0.758, BCE loss: 0.696, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 0 Step: 51 Total Loss: 0.763, BCE loss: 0.685, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 0 Step: 52 Total Loss: 0.745, BCE loss: 0.685, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 0 Step: 53 Total Loss: 0.783, BCE loss: 0.662, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 0 Step: 54 Total Loss: 0.742, BCE loss: 0.681, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 0 Step: 55 Total Loss: 0.722, BCE loss: 0.643, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 0 Step: 56 Total Loss: 0.766, BCE loss: 0.606, Diversity Loss: 0.322                     (Diversity_weight = 0.5)
Epoch: 0 Step: 57 Total Loss: 0.773, BCE loss: 0.685, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 0 Step: 58 Total Loss: 0.851, BCE loss: 0.789, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 0 Step: 59 Total Loss: 0.689, BCE loss: 0.622, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 0 Step: 60 Total Loss: 0.794, BCE loss: 0.711, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 0 Step: 61 Total Loss: 0.743, BCE loss: 0.669, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 0 Step: 62 Total Loss: 0.747, BCE loss: 0.649, Diversity Loss: 0.196                     (Diversity_weight = 0.5)
{'accuracy': 0.5915915915915916, 'roc_auc': 0.7103319232020826, 'pr_auc': 0.7267517785743127, 'conicity_mean': 0.1728165, 'conicity_std': 0.06359372}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.629    0.546      0.592      0.587         0.582
precision    0.523    0.725      0.592      0.624         0.636
recall       0.788    0.438      0.592      0.613         0.592
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7103319232020826
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:38:09_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.745, BCE loss: 0.665, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 1 Step: 1 Total Loss: 0.720, BCE loss: 0.643, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 1 Step: 2 Total Loss: 0.728, BCE loss: 0.603, Diversity Loss: 0.249                     (Diversity_weight = 0.5)
Epoch: 1 Step: 3 Total Loss: 0.732, BCE loss: 0.655, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 1 Step: 4 Total Loss: 0.724, BCE loss: 0.663, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 5 Total Loss: 0.722, BCE loss: 0.611, Diversity Loss: 0.221                     (Diversity_weight = 0.5)
Epoch: 1 Step: 6 Total Loss: 0.785, BCE loss: 0.726, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 1 Step: 7 Total Loss: 0.748, BCE loss: 0.690, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 1 Step: 8 Total Loss: 0.760, BCE loss: 0.638, Diversity Loss: 0.244                     (Diversity_weight = 0.5)
Epoch: 1 Step: 9 Total Loss: 0.783, BCE loss: 0.713, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 1 Step: 10 Total Loss: 0.766, BCE loss: 0.679, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 1 Step: 11 Total Loss: 0.685, BCE loss: 0.535, Diversity Loss: 0.298                     (Diversity_weight = 0.5)
Epoch: 1 Step: 12 Total Loss: 0.718, BCE loss: 0.622, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 1 Step: 13 Total Loss: 0.835, BCE loss: 0.660, Diversity Loss: 0.348                     (Diversity_weight = 0.5)
Epoch: 1 Step: 14 Total Loss: 0.736, BCE loss: 0.642, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 1 Step: 15 Total Loss: 0.719, BCE loss: 0.639, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 1 Step: 16 Total Loss: 0.753, BCE loss: 0.653, Diversity Loss: 0.201                     (Diversity_weight = 0.5)
Epoch: 1 Step: 17 Total Loss: 0.698, BCE loss: 0.559, Diversity Loss: 0.278                     (Diversity_weight = 0.5)
Epoch: 1 Step: 18 Total Loss: 0.746, BCE loss: 0.654, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 1 Step: 19 Total Loss: 0.658, BCE loss: 0.576, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 1 Step: 20 Total Loss: 0.694, BCE loss: 0.635, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 1 Step: 21 Total Loss: 0.797, BCE loss: 0.692, Diversity Loss: 0.210                     (Diversity_weight = 0.5)
Epoch: 1 Step: 22 Total Loss: 0.779, BCE loss: 0.714, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 1 Step: 23 Total Loss: 0.788, BCE loss: 0.698, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 1 Step: 24 Total Loss: 0.674, BCE loss: 0.608, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 1 Step: 25 Total Loss: 0.824, BCE loss: 0.748, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 1 Step: 26 Total Loss: 0.651, BCE loss: 0.570, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 1 Step: 27 Total Loss: 0.659, BCE loss: 0.569, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 1 Step: 28 Total Loss: 0.727, BCE loss: 0.648, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 1 Step: 29 Total Loss: 0.712, BCE loss: 0.643, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 1 Step: 30 Total Loss: 0.645, BCE loss: 0.551, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 1 Step: 31 Total Loss: 0.771, BCE loss: 0.705, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 1 Step: 32 Total Loss: 0.764, BCE loss: 0.678, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 1 Step: 33 Total Loss: 0.788, BCE loss: 0.670, Diversity Loss: 0.236                     (Diversity_weight = 0.5)
Epoch: 1 Step: 34 Total Loss: 0.741, BCE loss: 0.672, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 1 Step: 35 Total Loss: 0.686, BCE loss: 0.614, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 1 Step: 36 Total Loss: 0.710, BCE loss: 0.646, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 1 Step: 37 Total Loss: 0.665, BCE loss: 0.544, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 1 Step: 38 Total Loss: 0.651, BCE loss: 0.544, Diversity Loss: 0.214                     (Diversity_weight = 0.5)
Epoch: 1 Step: 39 Total Loss: 0.606, BCE loss: 0.524, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 1 Step: 40 Total Loss: 0.668, BCE loss: 0.600, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 1 Step: 41 Total Loss: 0.658, BCE loss: 0.557, Diversity Loss: 0.204                     (Diversity_weight = 0.5)
Epoch: 1 Step: 42 Total Loss: 0.654, BCE loss: 0.580, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 1 Step: 43 Total Loss: 0.694, BCE loss: 0.624, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 1 Step: 44 Total Loss: 0.711, BCE loss: 0.655, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 1 Step: 45 Total Loss: 0.757, BCE loss: 0.679, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 1 Step: 46 Total Loss: 0.656, BCE loss: 0.568, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 1 Step: 47 Total Loss: 0.648, BCE loss: 0.560, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 1 Step: 48 Total Loss: 0.575, BCE loss: 0.492, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 1 Step: 49 Total Loss: 0.601, BCE loss: 0.539, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 1 Step: 50 Total Loss: 0.706, BCE loss: 0.646, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 1 Step: 51 Total Loss: 0.604, BCE loss: 0.534, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 1 Step: 52 Total Loss: 0.683, BCE loss: 0.606, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 1 Step: 53 Total Loss: 0.657, BCE loss: 0.538, Diversity Loss: 0.237                     (Diversity_weight = 0.5)
Epoch: 1 Step: 54 Total Loss: 0.695, BCE loss: 0.623, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 1 Step: 55 Total Loss: 0.652, BCE loss: 0.558, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 1 Step: 56 Total Loss: 0.586, BCE loss: 0.524, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 57 Total Loss: 0.694, BCE loss: 0.632, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 58 Total Loss: 0.501, BCE loss: 0.375, Diversity Loss: 0.253                     (Diversity_weight = 0.5)
Epoch: 1 Step: 59 Total Loss: 0.680, BCE loss: 0.604, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 1 Step: 60 Total Loss: 0.646, BCE loss: 0.560, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 1 Step: 61 Total Loss: 0.708, BCE loss: 0.648, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 1 Step: 62 Total Loss: 0.559, BCE loss: 0.497, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
{'accuracy': 0.6926926926926927, 'roc_auc': 0.7487715587373902, 'pr_auc': 0.7843647017280513, 'conicity_mean': 0.16164815, 'conicity_std': 0.0582954}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.634    0.735      0.693      0.685         0.691
precision    0.665    0.711      0.693      0.688         0.691
recall       0.606    0.761      0.693      0.683         0.693
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7487715587373902
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:38:09_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.651, BCE loss: 0.561, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 2 Step: 1 Total Loss: 0.613, BCE loss: 0.563, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 2 Step: 2 Total Loss: 0.616, BCE loss: 0.485, Diversity Loss: 0.263                     (Diversity_weight = 0.5)
Epoch: 2 Step: 3 Total Loss: 0.548, BCE loss: 0.464, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 2 Step: 4 Total Loss: 0.491, BCE loss: 0.417, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 2 Step: 5 Total Loss: 0.524, BCE loss: 0.440, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 2 Step: 6 Total Loss: 0.696, BCE loss: 0.636, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 2 Step: 7 Total Loss: 0.537, BCE loss: 0.444, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 2 Step: 8 Total Loss: 0.703, BCE loss: 0.629, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 2 Step: 9 Total Loss: 0.655, BCE loss: 0.586, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 2 Step: 10 Total Loss: 0.591, BCE loss: 0.465, Diversity Loss: 0.250                     (Diversity_weight = 0.5)
Epoch: 2 Step: 11 Total Loss: 0.722, BCE loss: 0.634, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 2 Step: 12 Total Loss: 0.572, BCE loss: 0.511, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 2 Step: 13 Total Loss: 0.480, BCE loss: 0.355, Diversity Loss: 0.250                     (Diversity_weight = 0.5)
Epoch: 2 Step: 14 Total Loss: 0.649, BCE loss: 0.580, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 2 Step: 15 Total Loss: 0.597, BCE loss: 0.502, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 2 Step: 16 Total Loss: 0.419, BCE loss: 0.290, Diversity Loss: 0.257                     (Diversity_weight = 0.5)
Epoch: 2 Step: 17 Total Loss: 0.690, BCE loss: 0.608, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 2 Step: 18 Total Loss: 0.492, BCE loss: 0.425, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 2 Step: 19 Total Loss: 0.621, BCE loss: 0.543, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 2 Step: 20 Total Loss: 0.628, BCE loss: 0.560, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 2 Step: 21 Total Loss: 0.558, BCE loss: 0.477, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 2 Step: 22 Total Loss: 0.610, BCE loss: 0.554, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 2 Step: 23 Total Loss: 0.516, BCE loss: 0.433, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 2 Step: 24 Total Loss: 0.607, BCE loss: 0.546, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 2 Step: 25 Total Loss: 0.594, BCE loss: 0.508, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 2 Step: 26 Total Loss: 0.745, BCE loss: 0.681, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 2 Step: 27 Total Loss: 0.769, BCE loss: 0.661, Diversity Loss: 0.216                     (Diversity_weight = 0.5)
Epoch: 2 Step: 28 Total Loss: 0.544, BCE loss: 0.478, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 2 Step: 29 Total Loss: 0.534, BCE loss: 0.426, Diversity Loss: 0.216                     (Diversity_weight = 0.5)
Epoch: 2 Step: 30 Total Loss: 0.531, BCE loss: 0.438, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 2 Step: 31 Total Loss: 0.572, BCE loss: 0.494, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 2 Step: 32 Total Loss: 0.658, BCE loss: 0.602, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 2 Step: 33 Total Loss: 0.650, BCE loss: 0.482, Diversity Loss: 0.335                     (Diversity_weight = 0.5)
Epoch: 2 Step: 34 Total Loss: 0.613, BCE loss: 0.536, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 2 Step: 35 Total Loss: 0.498, BCE loss: 0.389, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 2 Step: 36 Total Loss: 0.796, BCE loss: 0.741, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 2 Step: 37 Total Loss: 0.663, BCE loss: 0.558, Diversity Loss: 0.210                     (Diversity_weight = 0.5)
Epoch: 2 Step: 38 Total Loss: 0.597, BCE loss: 0.513, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 2 Step: 39 Total Loss: 0.485, BCE loss: 0.413, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 2 Step: 40 Total Loss: 0.652, BCE loss: 0.596, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 2 Step: 41 Total Loss: 0.559, BCE loss: 0.503, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 2 Step: 42 Total Loss: 0.455, BCE loss: 0.375, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 2 Step: 43 Total Loss: 0.518, BCE loss: 0.445, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 2 Step: 44 Total Loss: 0.508, BCE loss: 0.441, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 2 Step: 45 Total Loss: 0.627, BCE loss: 0.564, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 2 Step: 46 Total Loss: 0.532, BCE loss: 0.466, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 2 Step: 47 Total Loss: 0.583, BCE loss: 0.518, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 2 Step: 48 Total Loss: 0.600, BCE loss: 0.522, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 2 Step: 49 Total Loss: 0.519, BCE loss: 0.427, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 2 Step: 50 Total Loss: 0.676, BCE loss: 0.620, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 2 Step: 51 Total Loss: 0.646, BCE loss: 0.553, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 2 Step: 52 Total Loss: 0.436, BCE loss: 0.323, Diversity Loss: 0.228                     (Diversity_weight = 0.5)
Epoch: 2 Step: 53 Total Loss: 0.546, BCE loss: 0.452, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 2 Step: 54 Total Loss: 0.614, BCE loss: 0.534, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 2 Step: 55 Total Loss: 0.557, BCE loss: 0.472, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 2 Step: 56 Total Loss: 0.667, BCE loss: 0.591, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 2 Step: 57 Total Loss: 0.619, BCE loss: 0.528, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 2 Step: 58 Total Loss: 0.553, BCE loss: 0.483, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 2 Step: 59 Total Loss: 0.563, BCE loss: 0.494, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 2 Step: 60 Total Loss: 0.531, BCE loss: 0.451, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 2 Step: 61 Total Loss: 0.739, BCE loss: 0.600, Diversity Loss: 0.277                     (Diversity_weight = 0.5)
Epoch: 2 Step: 62 Total Loss: 0.554, BCE loss: 0.446, Diversity Loss: 0.216                     (Diversity_weight = 0.5)
{'accuracy': 0.6746746746746747, 'roc_auc': 0.7668605597136348, 'pr_auc': 0.8091323221842963, 'conicity_mean': 0.16021979, 'conicity_std': 0.058175664}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.691    0.657      0.675      0.674         0.672
precision    0.593    0.804      0.675      0.698         0.711
recall       0.827    0.555      0.675      0.691         0.675
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7668605597136348
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:38:09_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.456, BCE loss: 0.368, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 3 Step: 1 Total Loss: 0.569, BCE loss: 0.492, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 3 Step: 2 Total Loss: 0.396, BCE loss: 0.329, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 3 Step: 3 Total Loss: 0.445, BCE loss: 0.371, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 4 Total Loss: 0.599, BCE loss: 0.543, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 3 Step: 5 Total Loss: 0.617, BCE loss: 0.528, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 3 Step: 6 Total Loss: 0.546, BCE loss: 0.453, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 3 Step: 7 Total Loss: 0.483, BCE loss: 0.429, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 3 Step: 8 Total Loss: 0.416, BCE loss: 0.353, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 3 Step: 9 Total Loss: 0.491, BCE loss: 0.420, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 3 Step: 10 Total Loss: 0.517, BCE loss: 0.463, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 3 Step: 11 Total Loss: 0.401, BCE loss: 0.314, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 3 Step: 12 Total Loss: 0.342, BCE loss: 0.223, Diversity Loss: 0.239                     (Diversity_weight = 0.5)
Epoch: 3 Step: 13 Total Loss: 0.505, BCE loss: 0.430, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 3 Step: 14 Total Loss: 0.510, BCE loss: 0.447, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 3 Step: 15 Total Loss: 0.414, BCE loss: 0.335, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 3 Step: 16 Total Loss: 0.296, BCE loss: 0.227, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 3 Step: 17 Total Loss: 0.487, BCE loss: 0.402, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 3 Step: 18 Total Loss: 0.521, BCE loss: 0.466, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 3 Step: 19 Total Loss: 0.310, BCE loss: 0.210, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 3 Step: 20 Total Loss: 0.427, BCE loss: 0.352, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 3 Step: 21 Total Loss: 0.498, BCE loss: 0.446, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 3 Step: 22 Total Loss: 0.660, BCE loss: 0.567, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 3 Step: 23 Total Loss: 0.384, BCE loss: 0.309, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 3 Step: 24 Total Loss: 0.553, BCE loss: 0.490, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 3 Step: 25 Total Loss: 0.535, BCE loss: 0.439, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 3 Step: 26 Total Loss: 0.490, BCE loss: 0.349, Diversity Loss: 0.282                     (Diversity_weight = 0.5)
Epoch: 3 Step: 27 Total Loss: 0.553, BCE loss: 0.488, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 3 Step: 28 Total Loss: 0.513, BCE loss: 0.429, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 3 Step: 29 Total Loss: 0.522, BCE loss: 0.442, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 3 Step: 30 Total Loss: 0.569, BCE loss: 0.486, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 3 Step: 31 Total Loss: 0.503, BCE loss: 0.416, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 3 Step: 32 Total Loss: 0.515, BCE loss: 0.450, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 3 Step: 33 Total Loss: 0.566, BCE loss: 0.458, Diversity Loss: 0.214                     (Diversity_weight = 0.5)
Epoch: 3 Step: 34 Total Loss: 0.522, BCE loss: 0.436, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 3 Step: 35 Total Loss: 0.725, BCE loss: 0.647, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 3 Step: 36 Total Loss: 0.614, BCE loss: 0.520, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 3 Step: 37 Total Loss: 0.420, BCE loss: 0.306, Diversity Loss: 0.227                     (Diversity_weight = 0.5)
Epoch: 3 Step: 38 Total Loss: 0.589, BCE loss: 0.534, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 3 Step: 39 Total Loss: 0.514, BCE loss: 0.419, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 3 Step: 40 Total Loss: 0.417, BCE loss: 0.346, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 3 Step: 41 Total Loss: 0.402, BCE loss: 0.302, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 3 Step: 42 Total Loss: 0.531, BCE loss: 0.451, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 3 Step: 43 Total Loss: 0.538, BCE loss: 0.462, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 3 Step: 44 Total Loss: 0.632, BCE loss: 0.576, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 3 Step: 45 Total Loss: 0.554, BCE loss: 0.488, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 3 Step: 46 Total Loss: 0.314, BCE loss: 0.250, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 3 Step: 47 Total Loss: 0.600, BCE loss: 0.442, Diversity Loss: 0.315                     (Diversity_weight = 0.5)
Epoch: 3 Step: 48 Total Loss: 0.420, BCE loss: 0.366, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 3 Step: 49 Total Loss: 0.521, BCE loss: 0.444, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 3 Step: 50 Total Loss: 0.359, BCE loss: 0.234, Diversity Loss: 0.249                     (Diversity_weight = 0.5)
Epoch: 3 Step: 51 Total Loss: 0.479, BCE loss: 0.369, Diversity Loss: 0.220                     (Diversity_weight = 0.5)
Epoch: 3 Step: 52 Total Loss: 0.442, BCE loss: 0.382, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 3 Step: 53 Total Loss: 0.464, BCE loss: 0.399, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 3 Step: 54 Total Loss: 0.369, BCE loss: 0.253, Diversity Loss: 0.233                     (Diversity_weight = 0.5)
Epoch: 3 Step: 55 Total Loss: 0.581, BCE loss: 0.521, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 3 Step: 56 Total Loss: 0.484, BCE loss: 0.413, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 3 Step: 57 Total Loss: 0.497, BCE loss: 0.437, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 3 Step: 58 Total Loss: 0.486, BCE loss: 0.431, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 3 Step: 59 Total Loss: 0.289, BCE loss: 0.199, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 3 Step: 60 Total Loss: 0.502, BCE loss: 0.426, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 3 Step: 61 Total Loss: 0.263, BCE loss: 0.156, Diversity Loss: 0.213                     (Diversity_weight = 0.5)
Epoch: 3 Step: 62 Total Loss: 0.417, BCE loss: 0.343, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
{'accuracy': 0.7077077077077077, 'roc_auc': 0.7796371623820371, 'pr_auc': 0.8154583882883959, 'conicity_mean': 0.1613566, 'conicity_std': 0.05696261}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.638    0.755      0.708      0.696         0.703
precision    0.700    0.712      0.708      0.706         0.707
recall       0.585    0.804      0.708      0.694         0.708
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7796371623820371
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:38:09_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.423, BCE loss: 0.349, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 4 Step: 1 Total Loss: 0.242, BCE loss: 0.117, Diversity Loss: 0.250                     (Diversity_weight = 0.5)
Epoch: 4 Step: 2 Total Loss: 0.412, BCE loss: 0.339, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 4 Step: 3 Total Loss: 0.295, BCE loss: 0.212, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 4 Step: 4 Total Loss: 0.411, BCE loss: 0.319, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 4 Step: 5 Total Loss: 0.333, BCE loss: 0.258, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 4 Step: 6 Total Loss: 0.388, BCE loss: 0.316, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 4 Step: 7 Total Loss: 0.397, BCE loss: 0.340, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 4 Step: 8 Total Loss: 0.506, BCE loss: 0.449, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 4 Step: 9 Total Loss: 0.353, BCE loss: 0.275, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 4 Step: 10 Total Loss: 0.431, BCE loss: 0.343, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 4 Step: 11 Total Loss: 0.219, BCE loss: 0.135, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 4 Step: 12 Total Loss: 0.538, BCE loss: 0.483, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 4 Step: 13 Total Loss: 0.269, BCE loss: 0.215, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 4 Step: 14 Total Loss: 0.339, BCE loss: 0.251, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 4 Step: 15 Total Loss: 0.465, BCE loss: 0.397, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 4 Step: 16 Total Loss: 0.361, BCE loss: 0.295, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 4 Step: 17 Total Loss: 0.374, BCE loss: 0.292, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 4 Step: 18 Total Loss: 0.350, BCE loss: 0.251, Diversity Loss: 0.198                     (Diversity_weight = 0.5)
Epoch: 4 Step: 19 Total Loss: 0.359, BCE loss: 0.264, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 4 Step: 20 Total Loss: 0.263, BCE loss: 0.198, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 4 Step: 21 Total Loss: 0.368, BCE loss: 0.291, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 4 Step: 22 Total Loss: 0.329, BCE loss: 0.223, Diversity Loss: 0.212                     (Diversity_weight = 0.5)
Epoch: 4 Step: 23 Total Loss: 0.387, BCE loss: 0.274, Diversity Loss: 0.226                     (Diversity_weight = 0.5)
Epoch: 4 Step: 24 Total Loss: 0.241, BCE loss: 0.153, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 4 Step: 25 Total Loss: 0.307, BCE loss: 0.195, Diversity Loss: 0.224                     (Diversity_weight = 0.5)
Epoch: 4 Step: 26 Total Loss: 0.445, BCE loss: 0.377, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 4 Step: 27 Total Loss: 0.457, BCE loss: 0.400, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 4 Step: 28 Total Loss: 0.498, BCE loss: 0.424, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 4 Step: 29 Total Loss: 0.444, BCE loss: 0.353, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 4 Step: 30 Total Loss: 0.255, BCE loss: 0.136, Diversity Loss: 0.237                     (Diversity_weight = 0.5)
Epoch: 4 Step: 31 Total Loss: 0.342, BCE loss: 0.265, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 4 Step: 32 Total Loss: 0.214, BCE loss: 0.142, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 4 Step: 33 Total Loss: 0.281, BCE loss: 0.199, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 4 Step: 34 Total Loss: 0.266, BCE loss: 0.207, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 4 Step: 35 Total Loss: 0.223, BCE loss: 0.158, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 4 Step: 36 Total Loss: 0.357, BCE loss: 0.279, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 4 Step: 37 Total Loss: 0.284, BCE loss: 0.205, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 4 Step: 38 Total Loss: 0.200, BCE loss: 0.103, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 4 Step: 39 Total Loss: 0.205, BCE loss: 0.139, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 4 Step: 40 Total Loss: 0.440, BCE loss: 0.377, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 4 Step: 41 Total Loss: 0.273, BCE loss: 0.208, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 4 Step: 42 Total Loss: 0.508, BCE loss: 0.458, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 4 Step: 43 Total Loss: 0.451, BCE loss: 0.388, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 4 Step: 44 Total Loss: 0.308, BCE loss: 0.171, Diversity Loss: 0.274                     (Diversity_weight = 0.5)
Epoch: 4 Step: 45 Total Loss: 0.691, BCE loss: 0.537, Diversity Loss: 0.309                     (Diversity_weight = 0.5)
Epoch: 4 Step: 46 Total Loss: 0.544, BCE loss: 0.457, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 4 Step: 47 Total Loss: 0.296, BCE loss: 0.182, Diversity Loss: 0.228                     (Diversity_weight = 0.5)
Epoch: 4 Step: 48 Total Loss: 0.419, BCE loss: 0.358, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 4 Step: 49 Total Loss: 0.329, BCE loss: 0.265, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 4 Step: 50 Total Loss: 0.361, BCE loss: 0.291, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 4 Step: 51 Total Loss: 0.260, BCE loss: 0.152, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 4 Step: 52 Total Loss: 0.300, BCE loss: 0.213, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 4 Step: 53 Total Loss: 0.284, BCE loss: 0.221, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 4 Step: 54 Total Loss: 0.374, BCE loss: 0.312, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 4 Step: 55 Total Loss: 0.450, BCE loss: 0.363, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 4 Step: 56 Total Loss: 0.362, BCE loss: 0.254, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 4 Step: 57 Total Loss: 0.461, BCE loss: 0.387, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 4 Step: 58 Total Loss: 0.328, BCE loss: 0.252, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 4 Step: 59 Total Loss: 0.414, BCE loss: 0.329, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 4 Step: 60 Total Loss: 0.445, BCE loss: 0.367, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 4 Step: 61 Total Loss: 0.422, BCE loss: 0.351, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 4 Step: 62 Total Loss: 0.449, BCE loss: 0.391, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
{'accuracy': 0.7207207207207207, 'roc_auc': 0.7886552229092092, 'pr_auc': 0.8166594934955943, 'conicity_mean': 0.15964769, 'conicity_std': 0.05533379}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.680    0.752      0.721      0.716         0.720
precision    0.685    0.748      0.721      0.716         0.720
recall       0.674    0.757      0.721      0.716         0.721
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7886552229092092
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:38:09_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.349, BCE loss: 0.277, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 5 Step: 1 Total Loss: 0.287, BCE loss: 0.220, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 5 Step: 2 Total Loss: 0.187, BCE loss: 0.123, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 5 Step: 3 Total Loss: 0.245, BCE loss: 0.167, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 5 Step: 4 Total Loss: 0.316, BCE loss: 0.219, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 5 Step: 5 Total Loss: 0.216, BCE loss: 0.103, Diversity Loss: 0.226                     (Diversity_weight = 0.5)
Epoch: 5 Step: 6 Total Loss: 0.229, BCE loss: 0.151, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 5 Step: 7 Total Loss: 0.226, BCE loss: 0.164, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 5 Step: 8 Total Loss: 0.271, BCE loss: 0.208, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 5 Step: 9 Total Loss: 0.344, BCE loss: 0.292, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 5 Step: 10 Total Loss: 0.209, BCE loss: 0.105, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 5 Step: 11 Total Loss: 0.163, BCE loss: 0.085, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 5 Step: 12 Total Loss: 0.370, BCE loss: 0.284, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 5 Step: 13 Total Loss: 0.227, BCE loss: 0.133, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 5 Step: 14 Total Loss: 0.176, BCE loss: 0.121, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 5 Step: 15 Total Loss: 0.131, BCE loss: 0.056, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 5 Step: 16 Total Loss: 0.271, BCE loss: 0.156, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 5 Step: 17 Total Loss: 0.176, BCE loss: 0.082, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 5 Step: 18 Total Loss: 0.177, BCE loss: 0.090, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 5 Step: 19 Total Loss: 0.293, BCE loss: 0.245, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 5 Step: 20 Total Loss: 0.216, BCE loss: 0.156, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 5 Step: 21 Total Loss: 0.305, BCE loss: 0.244, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 5 Step: 22 Total Loss: 0.315, BCE loss: 0.243, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 5 Step: 23 Total Loss: 0.182, BCE loss: 0.125, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 5 Step: 24 Total Loss: 0.495, BCE loss: 0.368, Diversity Loss: 0.254                     (Diversity_weight = 0.5)
Epoch: 5 Step: 25 Total Loss: 0.203, BCE loss: 0.152, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 5 Step: 26 Total Loss: 0.314, BCE loss: 0.245, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 5 Step: 27 Total Loss: 0.299, BCE loss: 0.208, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 5 Step: 28 Total Loss: 0.225, BCE loss: 0.138, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 5 Step: 29 Total Loss: 0.245, BCE loss: 0.190, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 5 Step: 30 Total Loss: 0.195, BCE loss: 0.118, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 5 Step: 31 Total Loss: 0.311, BCE loss: 0.233, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 5 Step: 32 Total Loss: 0.216, BCE loss: 0.122, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 5 Step: 33 Total Loss: 0.173, BCE loss: 0.102, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 5 Step: 34 Total Loss: 0.313, BCE loss: 0.237, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 5 Step: 35 Total Loss: 0.217, BCE loss: 0.151, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 5 Step: 36 Total Loss: 0.277, BCE loss: 0.138, Diversity Loss: 0.278                     (Diversity_weight = 0.5)
Epoch: 5 Step: 37 Total Loss: 0.316, BCE loss: 0.215, Diversity Loss: 0.201                     (Diversity_weight = 0.5)
Epoch: 5 Step: 38 Total Loss: 0.253, BCE loss: 0.189, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 5 Step: 39 Total Loss: 0.245, BCE loss: 0.193, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 5 Step: 40 Total Loss: 0.256, BCE loss: 0.190, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 5 Step: 41 Total Loss: 0.129, BCE loss: 0.059, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 5 Step: 42 Total Loss: 0.198, BCE loss: 0.076, Diversity Loss: 0.243                     (Diversity_weight = 0.5)
Epoch: 5 Step: 43 Total Loss: 0.208, BCE loss: 0.103, Diversity Loss: 0.211                     (Diversity_weight = 0.5)
Epoch: 5 Step: 44 Total Loss: 0.258, BCE loss: 0.177, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 5 Step: 45 Total Loss: 0.393, BCE loss: 0.286, Diversity Loss: 0.213                     (Diversity_weight = 0.5)
Epoch: 5 Step: 46 Total Loss: 0.273, BCE loss: 0.195, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 5 Step: 47 Total Loss: 0.146, BCE loss: 0.073, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 5 Step: 48 Total Loss: 0.345, BCE loss: 0.260, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 5 Step: 49 Total Loss: 0.177, BCE loss: 0.068, Diversity Loss: 0.219                     (Diversity_weight = 0.5)
Epoch: 5 Step: 50 Total Loss: 0.268, BCE loss: 0.192, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 5 Step: 51 Total Loss: 0.361, BCE loss: 0.305, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 5 Step: 52 Total Loss: 0.325, BCE loss: 0.251, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 5 Step: 53 Total Loss: 0.257, BCE loss: 0.195, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 5 Step: 54 Total Loss: 0.298, BCE loss: 0.216, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 5 Step: 55 Total Loss: 0.292, BCE loss: 0.212, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 5 Step: 56 Total Loss: 0.135, BCE loss: 0.073, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 5 Step: 57 Total Loss: 0.233, BCE loss: 0.149, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 5 Step: 58 Total Loss: 0.318, BCE loss: 0.255, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 5 Step: 59 Total Loss: 0.499, BCE loss: 0.343, Diversity Loss: 0.311                     (Diversity_weight = 0.5)
Epoch: 5 Step: 60 Total Loss: 0.232, BCE loss: 0.152, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 5 Step: 61 Total Loss: 0.452, BCE loss: 0.368, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 5 Step: 62 Total Loss: 0.216, BCE loss: 0.145, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
{'accuracy': 0.7097097097097097, 'roc_auc': 0.7755613407094046, 'pr_auc': 0.8116789154834655, 'conicity_mean': 0.16059262, 'conicity_std': 0.056952152}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.700    0.719       0.71      0.709         0.711
precision    0.641    0.786       0.71      0.714         0.722
recall       0.770    0.662       0.71      0.716         0.710
support    439.000  560.000     999.00    999.000       999.000
Model not saved on  roc_auc 0.7755613407094046
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:38:09_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.244, BCE loss: 0.141, Diversity Loss: 0.205                     (Diversity_weight = 0.5)
Epoch: 6 Step: 1 Total Loss: 0.227, BCE loss: 0.138, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 6 Step: 2 Total Loss: 0.169, BCE loss: 0.098, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 6 Step: 3 Total Loss: 0.152, BCE loss: 0.092, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 6 Step: 4 Total Loss: 0.209, BCE loss: 0.146, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 6 Step: 5 Total Loss: 0.184, BCE loss: 0.115, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 6 Step: 6 Total Loss: 0.227, BCE loss: 0.131, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 6 Step: 7 Total Loss: 0.250, BCE loss: 0.175, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 6 Step: 8 Total Loss: 0.172, BCE loss: 0.113, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 6 Step: 9 Total Loss: 0.175, BCE loss: 0.104, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 6 Step: 10 Total Loss: 0.337, BCE loss: 0.264, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 6 Step: 11 Total Loss: 0.170, BCE loss: 0.093, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 6 Step: 12 Total Loss: 0.150, BCE loss: 0.073, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 6 Step: 13 Total Loss: 0.182, BCE loss: 0.082, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 6 Step: 14 Total Loss: 0.152, BCE loss: 0.037, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 6 Step: 15 Total Loss: 0.141, BCE loss: 0.076, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 6 Step: 16 Total Loss: 0.215, BCE loss: 0.149, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 6 Step: 17 Total Loss: 0.209, BCE loss: 0.129, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 6 Step: 18 Total Loss: 0.335, BCE loss: 0.221, Diversity Loss: 0.227                     (Diversity_weight = 0.5)
Epoch: 6 Step: 19 Total Loss: 0.244, BCE loss: 0.175, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 6 Step: 20 Total Loss: 0.162, BCE loss: 0.055, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 6 Step: 21 Total Loss: 0.246, BCE loss: 0.143, Diversity Loss: 0.206                     (Diversity_weight = 0.5)
Epoch: 6 Step: 22 Total Loss: 0.127, BCE loss: 0.064, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 6 Step: 23 Total Loss: 0.218, BCE loss: 0.103, Diversity Loss: 0.232                     (Diversity_weight = 0.5)
Epoch: 6 Step: 24 Total Loss: 0.107, BCE loss: 0.055, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 6 Step: 25 Total Loss: 0.195, BCE loss: 0.116, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 6 Step: 26 Total Loss: 0.175, BCE loss: 0.096, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 6 Step: 27 Total Loss: 0.173, BCE loss: 0.085, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 6 Step: 28 Total Loss: 0.165, BCE loss: 0.105, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 6 Step: 29 Total Loss: 0.161, BCE loss: 0.039, Diversity Loss: 0.245                     (Diversity_weight = 0.5)
Epoch: 6 Step: 30 Total Loss: 0.202, BCE loss: 0.127, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 6 Step: 31 Total Loss: 0.191, BCE loss: 0.102, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 6 Step: 32 Total Loss: 0.293, BCE loss: 0.229, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 6 Step: 33 Total Loss: 0.142, BCE loss: 0.067, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 6 Step: 34 Total Loss: 0.183, BCE loss: 0.126, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 6 Step: 35 Total Loss: 0.219, BCE loss: 0.164, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 6 Step: 36 Total Loss: 0.218, BCE loss: 0.143, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 6 Step: 37 Total Loss: 0.109, BCE loss: 0.056, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 6 Step: 38 Total Loss: 0.115, BCE loss: 0.045, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 6 Step: 39 Total Loss: 0.292, BCE loss: 0.222, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 6 Step: 40 Total Loss: 0.151, BCE loss: 0.062, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 6 Step: 41 Total Loss: 0.219, BCE loss: 0.143, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 6 Step: 42 Total Loss: 0.239, BCE loss: 0.159, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 6 Step: 43 Total Loss: 0.187, BCE loss: 0.086, Diversity Loss: 0.201                     (Diversity_weight = 0.5)
Epoch: 6 Step: 44 Total Loss: 0.258, BCE loss: 0.184, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 6 Step: 45 Total Loss: 0.304, BCE loss: 0.219, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 6 Step: 46 Total Loss: 0.381, BCE loss: 0.222, Diversity Loss: 0.318                     (Diversity_weight = 0.5)
Epoch: 6 Step: 47 Total Loss: 0.112, BCE loss: 0.041, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 6 Step: 48 Total Loss: 0.178, BCE loss: 0.111, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 6 Step: 49 Total Loss: 0.267, BCE loss: 0.126, Diversity Loss: 0.282                     (Diversity_weight = 0.5)
Epoch: 6 Step: 50 Total Loss: 0.444, BCE loss: 0.382, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 6 Step: 51 Total Loss: 0.171, BCE loss: 0.110, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 6 Step: 52 Total Loss: 0.419, BCE loss: 0.364, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 6 Step: 53 Total Loss: 0.270, BCE loss: 0.179, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 6 Step: 54 Total Loss: 0.118, BCE loss: 0.065, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 6 Step: 55 Total Loss: 0.249, BCE loss: 0.162, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 6 Step: 56 Total Loss: 0.189, BCE loss: 0.130, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 6 Step: 57 Total Loss: 0.266, BCE loss: 0.212, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 6 Step: 58 Total Loss: 0.124, BCE loss: 0.037, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 6 Step: 59 Total Loss: 0.200, BCE loss: 0.144, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 6 Step: 60 Total Loss: 0.192, BCE loss: 0.113, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 6 Step: 61 Total Loss: 0.127, BCE loss: 0.053, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 6 Step: 62 Total Loss: 0.199, BCE loss: 0.120, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
{'accuracy': 0.6756756756756757, 'roc_auc': 0.7710624796615684, 'pr_auc': 0.8152392493829141, 'conicity_mean': 0.16017263, 'conicity_std': 0.054206073}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.688    0.662      0.676      0.675         0.674
precision    0.596    0.796      0.676      0.696         0.708
recall       0.815    0.566      0.676      0.691         0.676
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7710624796615684
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:38:09_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.154, BCE loss: 0.090, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 7 Step: 1 Total Loss: 0.141, BCE loss: 0.054, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 7 Step: 2 Total Loss: 0.142, BCE loss: 0.047, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 7 Step: 3 Total Loss: 0.215, BCE loss: 0.141, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 7 Step: 4 Total Loss: 0.177, BCE loss: 0.100, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 7 Step: 5 Total Loss: 0.124, BCE loss: 0.055, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 7 Step: 6 Total Loss: 0.096, BCE loss: 0.035, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 7 Step: 7 Total Loss: 0.165, BCE loss: 0.107, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 7 Step: 8 Total Loss: 0.142, BCE loss: 0.071, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 7 Step: 9 Total Loss: 0.147, BCE loss: 0.080, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 7 Step: 10 Total Loss: 0.217, BCE loss: 0.131, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 7 Step: 11 Total Loss: 0.147, BCE loss: 0.087, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 7 Step: 12 Total Loss: 0.123, BCE loss: 0.045, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 7 Step: 13 Total Loss: 0.119, BCE loss: 0.034, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 7 Step: 14 Total Loss: 0.168, BCE loss: 0.092, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 7 Step: 15 Total Loss: 0.141, BCE loss: 0.064, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 7 Step: 16 Total Loss: 0.234, BCE loss: 0.102, Diversity Loss: 0.264                     (Diversity_weight = 0.5)
Epoch: 7 Step: 17 Total Loss: 0.234, BCE loss: 0.180, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 7 Step: 18 Total Loss: 0.119, BCE loss: 0.037, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 7 Step: 19 Total Loss: 0.127, BCE loss: 0.036, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 7 Step: 20 Total Loss: 0.089, BCE loss: 0.015, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 7 Step: 21 Total Loss: 0.143, BCE loss: 0.077, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 7 Step: 22 Total Loss: 0.167, BCE loss: 0.098, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 7 Step: 23 Total Loss: 0.100, BCE loss: 0.047, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 7 Step: 24 Total Loss: 0.274, BCE loss: 0.152, Diversity Loss: 0.243                     (Diversity_weight = 0.5)
Epoch: 7 Step: 25 Total Loss: 0.138, BCE loss: 0.031, Diversity Loss: 0.213                     (Diversity_weight = 0.5)
Epoch: 7 Step: 26 Total Loss: 0.166, BCE loss: 0.073, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 7 Step: 27 Total Loss: 0.104, BCE loss: 0.046, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 7 Step: 28 Total Loss: 0.108, BCE loss: 0.023, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 7 Step: 29 Total Loss: 0.188, BCE loss: 0.090, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 7 Step: 30 Total Loss: 0.122, BCE loss: 0.051, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 7 Step: 31 Total Loss: 0.107, BCE loss: 0.026, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 7 Step: 32 Total Loss: 0.159, BCE loss: 0.096, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 7 Step: 33 Total Loss: 0.187, BCE loss: 0.113, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 7 Step: 34 Total Loss: 0.153, BCE loss: 0.100, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 7 Step: 35 Total Loss: 0.306, BCE loss: 0.195, Diversity Loss: 0.221                     (Diversity_weight = 0.5)
Epoch: 7 Step: 36 Total Loss: 0.175, BCE loss: 0.118, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 7 Step: 37 Total Loss: 0.094, BCE loss: 0.034, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 7 Step: 38 Total Loss: 0.269, BCE loss: 0.112, Diversity Loss: 0.313                     (Diversity_weight = 0.5)
Epoch: 7 Step: 39 Total Loss: 0.231, BCE loss: 0.142, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 7 Step: 40 Total Loss: 0.081, BCE loss: 0.023, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 7 Step: 41 Total Loss: 0.090, BCE loss: 0.030, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 7 Step: 42 Total Loss: 0.101, BCE loss: 0.037, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 7 Step: 43 Total Loss: 0.067, BCE loss: 0.018, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 7 Step: 44 Total Loss: 0.209, BCE loss: 0.109, Diversity Loss: 0.201                     (Diversity_weight = 0.5)
Epoch: 7 Step: 45 Total Loss: 0.237, BCE loss: 0.165, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 7 Step: 46 Total Loss: 0.094, BCE loss: 0.031, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 7 Step: 47 Total Loss: 0.310, BCE loss: 0.240, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 7 Step: 48 Total Loss: 0.193, BCE loss: 0.125, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 7 Step: 49 Total Loss: 0.126, BCE loss: 0.015, Diversity Loss: 0.222                     (Diversity_weight = 0.5)
Epoch: 7 Step: 50 Total Loss: 0.141, BCE loss: 0.091, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 7 Step: 51 Total Loss: 0.160, BCE loss: 0.073, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 7 Step: 52 Total Loss: 0.150, BCE loss: 0.031, Diversity Loss: 0.238                     (Diversity_weight = 0.5)
Epoch: 7 Step: 53 Total Loss: 0.099, BCE loss: 0.023, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 7 Step: 54 Total Loss: 0.168, BCE loss: 0.088, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 7 Step: 55 Total Loss: 0.084, BCE loss: 0.020, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 7 Step: 56 Total Loss: 0.215, BCE loss: 0.135, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 7 Step: 57 Total Loss: 0.145, BCE loss: 0.059, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 7 Step: 58 Total Loss: 0.170, BCE loss: 0.102, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 7 Step: 59 Total Loss: 0.210, BCE loss: 0.137, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 7 Step: 60 Total Loss: 0.119, BCE loss: 0.062, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 7 Step: 61 Total Loss: 0.127, BCE loss: 0.061, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 7 Step: 62 Total Loss: 0.127, BCE loss: 0.020, Diversity Loss: 0.214                     (Diversity_weight = 0.5)
{'accuracy': 0.7047047047047047, 'roc_auc': 0.7645094370322161, 'pr_auc': 0.8021038422331705, 'conicity_mean': 0.16012347, 'conicity_std': 0.057377957}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.671    0.732      0.705      0.702         0.705
precision    0.657    0.745      0.705      0.701         0.706
recall       0.686    0.720      0.705      0.703         0.705
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7645094370322161
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:38:09_2021
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:39:04,191 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:39:04,191 - type = vanillalstm
INFO - 2021-01-17 19:39:04,191 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:39:04,192 - vocab_size = 666
INFO - 2021-01-17 19:39:04,192 - embed_size = 200
INFO - 2021-01-17 19:39:04,192 - hidden_size = 128
INFO - 2021-01-17 19:39:04,192 - pre_embed = None
INFO - 2021-01-17 19:39:04,206 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:39:04,206 - hidden_size = 256
INFO - 2021-01-17 19:39:04,206 - output_size = 1
INFO - 2021-01-17 19:39:04,206 - use_attention = True
INFO - 2021-01-17 19:39:04,206 - regularizer_attention = None
INFO - 2021-01-17 19:39:04,207 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b0efc309790> and extras set()
INFO - 2021-01-17 19:39:04,207 - attention.type = tanh
INFO - 2021-01-17 19:39:04,207 - type = tanh
INFO - 2021-01-17 19:39:04,207 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b0efc309790> and extras set()
INFO - 2021-01-17 19:39:04,207 - attention.hidden_size = 256
INFO - 2021-01-17 19:39:04,207 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7447447447447447, 'roc_auc': 0.823378832645553, 'pr_auc': 0.77308779285972, 'conicity_mean': '0.15937498', 'conicity_std': '0.05581539'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.761    0.727      0.745      0.744         0.746
precision    0.804    0.685      0.745      0.744         0.752
recall       0.722    0.774      0.745      0.748         0.745
support    561.000  438.000    999.000    999.000       999.000
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:39:05,639 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:39:05,639 - type = vanillalstm
INFO - 2021-01-17 19:39:05,640 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:39:05,640 - vocab_size = 666
INFO - 2021-01-17 19:39:05,640 - embed_size = 200
INFO - 2021-01-17 19:39:05,640 - hidden_size = 128
INFO - 2021-01-17 19:39:05,640 - pre_embed = None
INFO - 2021-01-17 19:39:05,654 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:39:05,654 - hidden_size = 256
INFO - 2021-01-17 19:39:05,654 - output_size = 1
INFO - 2021-01-17 19:39:05,654 - use_attention = True
INFO - 2021-01-17 19:39:05,654 - regularizer_attention = None
INFO - 2021-01-17 19:39:05,654 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b0e95f6ad10> and extras set()
INFO - 2021-01-17 19:39:05,654 - attention.type = tanh
INFO - 2021-01-17 19:39:05,654 - type = tanh
INFO - 2021-01-17 19:39:05,654 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b0e95f6ad10> and extras set()
INFO - 2021-01-17 19:39:05,655 - attention.hidden_size = 256
INFO - 2021-01-17 19:39:05,655 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7447447447447447, 'roc_auc': 0.823378832645553, 'pr_auc': 0.77308779285972, 'conicity_mean': '0.15937498', 'conicity_std': '0.05581539'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.761    0.727      0.745      0.744         0.746
precision    0.804    0.685      0.745      0.744         0.752
recall       0.722    0.774      0.745      0.748         0.745
support    561.000  438.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_de vanilla_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:39:16,231 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:39:16,231 - type = vanillalstm
INFO - 2021-01-17 19:39:16,231 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:39:16,250 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:39:16,490 - vocab_size = 648
INFO - 2021-01-17 19:39:16,490 - embed_size = 200
INFO - 2021-01-17 19:39:16,490 - hidden_size = 128
INFO - 2021-01-17 19:39:16,490 - pre_embed = None
INFO - 2021-01-17 19:39:19,567 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:39:19,567 - hidden_size = 256
INFO - 2021-01-17 19:39:19,567 - output_size = 1
INFO - 2021-01-17 19:39:19,567 - use_attention = True
INFO - 2021-01-17 19:39:19,567 - regularizer_attention = None
INFO - 2021-01-17 19:39:19,568 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b609c94b550> and extras set()
INFO - 2021-01-17 19:39:19,568 - attention.type = tanh
INFO - 2021-01-17 19:39:19,568 - type = tanh
INFO - 2021-01-17 19:39:19,568 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b609c94b550> and extras set()
INFO - 2021-01-17 19:39:19,568 - attention.hidden_size = 256
INFO - 2021-01-17 19:39:19,568 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.708, BCE loss: 0.708, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.702, BCE loss: 0.702, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.714, BCE loss: 0.714, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.681, BCE loss: 0.681, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.620                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.712, BCE loss: 0.712, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.642, BCE loss: 0.642, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.710, BCE loss: 0.710, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.727, BCE loss: 0.727, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.711, BCE loss: 0.711, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.706, BCE loss: 0.706, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.613, BCE loss: 0.613, Diversity Loss: 0.694                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.620                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.669, BCE loss: 0.669, Diversity Loss: 0.641                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.673, BCE loss: 0.673, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.664, BCE loss: 0.664, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.719, BCE loss: 0.719, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.666, BCE loss: 0.666, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.651, BCE loss: 0.651, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.673, BCE loss: 0.673, Diversity Loss: 0.612                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.624, BCE loss: 0.624, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.628                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.650, BCE loss: 0.650, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.646, BCE loss: 0.646, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.602, BCE loss: 0.602, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.745                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.582, BCE loss: 0.582, Diversity Loss: 0.692                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.774                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.655                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.597, BCE loss: 0.597, Diversity Loss: 0.741                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.756                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.579, BCE loss: 0.579, Diversity Loss: 0.790                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.709, BCE loss: 0.709, Diversity Loss: 0.873                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.822                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.827, BCE loss: 0.827, Diversity Loss: 0.807                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.648, BCE loss: 0.648, Diversity Loss: 0.741                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.643, BCE loss: 0.643, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.589, BCE loss: 0.589, Diversity Loss: 0.693                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.658, BCE loss: 0.658, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.616, BCE loss: 0.616, Diversity Loss: 0.651                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.624, BCE loss: 0.624, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.739, BCE loss: 0.739, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.582, BCE loss: 0.582, Diversity Loss: 0.579                     (Diversity_weight = 0)
{'accuracy': 0.5707121364092277, 'roc_auc': 0.6663339317195689, 'pr_auc': 0.6197264762795529, 'conicity_mean': 0.5885991, 'conicity_std': 0.12752461}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.681    0.346      0.571      0.513         0.516
precision    0.547    0.693      0.571      0.620         0.619
recall       0.901    0.230      0.571      0.566         0.571
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.6663339317195689
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:39:19_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.724                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.591, BCE loss: 0.591, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.640, BCE loss: 0.640, Diversity Loss: 0.622                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.635, BCE loss: 0.635, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.603, BCE loss: 0.603, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.630                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.591                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.593, BCE loss: 0.593, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.699                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.638                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.640, BCE loss: 0.640, Diversity Loss: 0.645                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.572, BCE loss: 0.572, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.631, BCE loss: 0.631, Diversity Loss: 0.729                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.573, BCE loss: 0.573, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.589, BCE loss: 0.589, Diversity Loss: 0.678                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.516, BCE loss: 0.516, Diversity Loss: 0.629                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.523, BCE loss: 0.523, Diversity Loss: 0.634                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.729, BCE loss: 0.729, Diversity Loss: 0.650                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.639                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.654, BCE loss: 0.654, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.705                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.705                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.558, BCE loss: 0.558, Diversity Loss: 0.770                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.749, BCE loss: 0.749, Diversity Loss: 0.757                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.640, BCE loss: 0.640, Diversity Loss: 0.701                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.562, BCE loss: 0.562, Diversity Loss: 0.829                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.717, BCE loss: 0.717, Diversity Loss: 0.714                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.709, BCE loss: 0.709, Diversity Loss: 0.743                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.593, BCE loss: 0.593, Diversity Loss: 0.781                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.639, BCE loss: 0.639, Diversity Loss: 0.634                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.592, BCE loss: 0.592, Diversity Loss: 0.724                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.570, BCE loss: 0.570, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.602, BCE loss: 0.602, Diversity Loss: 0.716                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.573, BCE loss: 0.573, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.619, BCE loss: 0.619, Diversity Loss: 0.629                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.598, BCE loss: 0.598, Diversity Loss: 0.633                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.575, BCE loss: 0.575, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.692                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.617, BCE loss: 0.617, Diversity Loss: 0.749                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.641, BCE loss: 0.641, Diversity Loss: 0.647                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.627, BCE loss: 0.627, Diversity Loss: 0.677                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.583, BCE loss: 0.583, Diversity Loss: 0.721                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.540, BCE loss: 0.540, Diversity Loss: 0.687                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.630                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.607, BCE loss: 0.607, Diversity Loss: 0.666                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.660                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.620                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.593, BCE loss: 0.593, Diversity Loss: 0.677                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.670                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.545, BCE loss: 0.545, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.672                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.758                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.737                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.650                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.571, BCE loss: 0.571, Diversity Loss: 0.744                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.569, BCE loss: 0.569, Diversity Loss: 0.682                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.667                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.477, BCE loss: 0.477, Diversity Loss: 0.636                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.553, BCE loss: 0.553, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.760                     (Diversity_weight = 0)
{'accuracy': 0.695085255767302, 'roc_auc': 0.757130322082062, 'pr_auc': 0.731123914069567, 'conicity_mean': 0.6623159, 'conicity_std': 0.12430296}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.697    0.693      0.695      0.695         0.695
precision    0.703    0.687      0.695      0.695         0.695
recall       0.692    0.699      0.695      0.695         0.695
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.757130322082062
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:39:19_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.646, BCE loss: 0.646, Diversity Loss: 0.656                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.429, BCE loss: 0.429, Diversity Loss: 0.703                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.825, BCE loss: 0.825, Diversity Loss: 0.756                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.709, BCE loss: 0.709, Diversity Loss: 0.613                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.551, BCE loss: 0.551, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.593, BCE loss: 0.593, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.534, BCE loss: 0.534, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.614, BCE loss: 0.614, Diversity Loss: 0.703                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.425, BCE loss: 0.425, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.589, BCE loss: 0.589, Diversity Loss: 0.633                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.557, BCE loss: 0.557, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.628                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.613, BCE loss: 0.613, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.706                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.565, BCE loss: 0.565, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.685                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.387, BCE loss: 0.387, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.568, BCE loss: 0.568, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.482, BCE loss: 0.482, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.642, BCE loss: 0.642, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.513, BCE loss: 0.513, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.578, BCE loss: 0.578, Diversity Loss: 0.678                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.590, BCE loss: 0.590, Diversity Loss: 0.667                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.348, BCE loss: 0.348, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.437, BCE loss: 0.437, Diversity Loss: 0.643                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.452, BCE loss: 0.452, Diversity Loss: 0.622                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.558, BCE loss: 0.558, Diversity Loss: 0.659                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.671                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.439, BCE loss: 0.439, Diversity Loss: 0.661                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.717, BCE loss: 0.717, Diversity Loss: 0.637                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.519, BCE loss: 0.519, Diversity Loss: 0.653                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.486, BCE loss: 0.486, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.473, BCE loss: 0.473, Diversity Loss: 0.659                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.391, BCE loss: 0.391, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.628                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.424, BCE loss: 0.424, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.518, BCE loss: 0.518, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.440, BCE loss: 0.440, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.523, BCE loss: 0.523, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.398, BCE loss: 0.398, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.529, BCE loss: 0.529, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.622, BCE loss: 0.622, Diversity Loss: 0.649                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.476, BCE loss: 0.476, Diversity Loss: 0.693                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.541, BCE loss: 0.541, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.401, BCE loss: 0.401, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.553, BCE loss: 0.553, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.419, BCE loss: 0.419, Diversity Loss: 0.585                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.478, BCE loss: 0.478, Diversity Loss: 0.613                     (Diversity_weight = 0)
{'accuracy': 0.7161484453360081, 'roc_auc': 0.7874548191558729, 'pr_auc': 0.7637190807536866, 'conicity_mean': 0.6032333, 'conicity_std': 0.113015465}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.701    0.730      0.716      0.715         0.715
precision    0.754    0.686      0.716      0.720         0.721
recall       0.654    0.780      0.716      0.717         0.716
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7874548191558729
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:39:19_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.439, BCE loss: 0.439, Diversity Loss: 0.658                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.356, BCE loss: 0.356, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.344, BCE loss: 0.344, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.278, BCE loss: 0.278, Diversity Loss: 0.599                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.384, BCE loss: 0.384, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.445, BCE loss: 0.445, Diversity Loss: 0.640                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.379, BCE loss: 0.379, Diversity Loss: 0.716                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.292, BCE loss: 0.292, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.531, BCE loss: 0.531, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.454, BCE loss: 0.454, Diversity Loss: 0.750                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.402, BCE loss: 0.402, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.393, BCE loss: 0.393, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.441, BCE loss: 0.441, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.628                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.498, BCE loss: 0.498, Diversity Loss: 0.591                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.520, BCE loss: 0.520, Diversity Loss: 0.653                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.377, BCE loss: 0.377, Diversity Loss: 0.646                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.551, BCE loss: 0.551, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.485, BCE loss: 0.485, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.427, BCE loss: 0.427, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.350, BCE loss: 0.350, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.454, BCE loss: 0.454, Diversity Loss: 0.586                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.628                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.592, BCE loss: 0.592, Diversity Loss: 0.695                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.416, BCE loss: 0.416, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.322, BCE loss: 0.322, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.401, BCE loss: 0.401, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.483, BCE loss: 0.483, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.683                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.238, BCE loss: 0.238, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.373, BCE loss: 0.373, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.463, BCE loss: 0.463, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.306, BCE loss: 0.306, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.224, BCE loss: 0.224, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.434, BCE loss: 0.434, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.505, BCE loss: 0.505, Diversity Loss: 0.627                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.518, BCE loss: 0.518, Diversity Loss: 0.652                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.280, BCE loss: 0.280, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.410, BCE loss: 0.410, Diversity Loss: 0.582                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.426, BCE loss: 0.426, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.375, BCE loss: 0.375, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.471, BCE loss: 0.471, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.339, BCE loss: 0.339, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.356, BCE loss: 0.356, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.584, BCE loss: 0.584, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.333, BCE loss: 0.333, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.317, BCE loss: 0.317, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.420, BCE loss: 0.420, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.392, BCE loss: 0.392, Diversity Loss: 0.553                     (Diversity_weight = 0)
{'accuracy': 0.7462387161484454, 'roc_auc': 0.8224040636597088, 'pr_auc': 0.8119734969956107, 'conicity_mean': 0.56971645, 'conicity_std': 0.111369036}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.748    0.744      0.746      0.746         0.746
precision    0.754    0.739      0.746      0.746         0.746
recall       0.743    0.749      0.746      0.746         0.746
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.8224040636597088
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:39:19_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.291, BCE loss: 0.291, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.254, BCE loss: 0.254, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.241, BCE loss: 0.241, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.241, BCE loss: 0.241, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.350, BCE loss: 0.350, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.306, BCE loss: 0.306, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.609                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.174, BCE loss: 0.174, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.424, BCE loss: 0.424, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.232, BCE loss: 0.232, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.208, BCE loss: 0.208, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.279, BCE loss: 0.279, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.437, BCE loss: 0.437, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.312, BCE loss: 0.312, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.313, BCE loss: 0.313, Diversity Loss: 0.705                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.296, BCE loss: 0.296, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.284, BCE loss: 0.284, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.189, BCE loss: 0.189, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.278, BCE loss: 0.278, Diversity Loss: 0.623                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.275, BCE loss: 0.275, Diversity Loss: 0.609                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.320, BCE loss: 0.320, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.318, BCE loss: 0.318, Diversity Loss: 0.609                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.449, BCE loss: 0.449, Diversity Loss: 0.665                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.151, BCE loss: 0.151, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.285, BCE loss: 0.285, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.338, BCE loss: 0.338, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.628                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.599, BCE loss: 0.599, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.376, BCE loss: 0.376, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.272, BCE loss: 0.272, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.384, BCE loss: 0.384, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.250, BCE loss: 0.250, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.438, BCE loss: 0.438, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.323, BCE loss: 0.323, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.208, BCE loss: 0.208, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.231, BCE loss: 0.231, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.236, BCE loss: 0.236, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.333, BCE loss: 0.333, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.591                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.519, BCE loss: 0.519, Diversity Loss: 0.574                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.441, BCE loss: 0.441, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.400, BCE loss: 0.400, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.285, BCE loss: 0.285, Diversity Loss: 0.631                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.247, BCE loss: 0.247, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.279, BCE loss: 0.279, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.586                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.247, BCE loss: 0.247, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.223, BCE loss: 0.223, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.308, BCE loss: 0.308, Diversity Loss: 0.537                     (Diversity_weight = 0)
{'accuracy': 0.7331995987963892, 'roc_auc': 0.8134483952247168, 'pr_auc': 0.8125831908662631, 'conicity_mean': 0.574252, 'conicity_std': 0.12362032}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.714    0.750      0.733      0.732         0.732
precision    0.783    0.696      0.733      0.740         0.740
recall       0.656    0.813      0.733      0.734         0.733
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.8134483952247168
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:39:19_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.584                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.219, BCE loss: 0.219, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.293, BCE loss: 0.293, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.235, BCE loss: 0.235, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.204, BCE loss: 0.204, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.348, BCE loss: 0.348, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.160, BCE loss: 0.160, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.136, BCE loss: 0.136, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.128, BCE loss: 0.128, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.697                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.127, BCE loss: 0.127, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.251, BCE loss: 0.251, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.255, BCE loss: 0.255, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.595                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.353, BCE loss: 0.353, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.134, BCE loss: 0.134, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.630                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.257, BCE loss: 0.257, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.160, BCE loss: 0.160, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.179, BCE loss: 0.179, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.332, BCE loss: 0.332, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.292, BCE loss: 0.292, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.234, BCE loss: 0.234, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.114, BCE loss: 0.114, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.284, BCE loss: 0.284, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.125, BCE loss: 0.125, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.255, BCE loss: 0.255, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.249, BCE loss: 0.249, Diversity Loss: 0.651                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.412, BCE loss: 0.412, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.151, BCE loss: 0.151, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.281, BCE loss: 0.281, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.335, BCE loss: 0.335, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.373, BCE loss: 0.373, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.188, BCE loss: 0.188, Diversity Loss: 0.618                     (Diversity_weight = 0)
{'accuracy': 0.7231695085255767, 'roc_auc': 0.7939431506242806, 'pr_auc': 0.7852738383041413, 'conicity_mean': 0.5560409, 'conicity_std': 0.11701474}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.721    0.725      0.723      0.723         0.723
precision    0.738    0.710      0.723      0.724         0.724
recall       0.706    0.741      0.723      0.723         0.723
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7939431506242806
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:39:19_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.187, BCE loss: 0.187, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.254, BCE loss: 0.254, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.218, BCE loss: 0.218, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.322, BCE loss: 0.322, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.686                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.617                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.623                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.236, BCE loss: 0.236, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.257, BCE loss: 0.257, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.651, BCE loss: 0.651, Diversity Loss: 0.654                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.319, BCE loss: 0.319, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.297, BCE loss: 0.297, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.235, BCE loss: 0.235, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.118, BCE loss: 0.118, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.224, BCE loss: 0.224, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.098, BCE loss: 0.098, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.134, BCE loss: 0.134, Diversity Loss: 0.607                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.109, BCE loss: 0.109, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.574                     (Diversity_weight = 0)
{'accuracy': 0.753259779338014, 'roc_auc': 0.825116926817095, 'pr_auc': 0.8186976237724848, 'conicity_mean': 0.54529357, 'conicity_std': 0.111246265}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.753    0.754      0.753      0.753         0.753
precision    0.765    0.742      0.753      0.753         0.754
recall       0.741    0.766      0.753      0.753         0.753
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.825116926817095
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:39:19_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.668                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.050, BCE loss: 0.050, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.098, BCE loss: 0.098, Diversity Loss: 0.623                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.098, BCE loss: 0.098, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.561                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.224, BCE loss: 0.224, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.313, BCE loss: 0.313, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.235, BCE loss: 0.235, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.114, BCE loss: 0.114, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.381, BCE loss: 0.381, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.229, BCE loss: 0.229, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.207, BCE loss: 0.207, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.225, BCE loss: 0.225, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.658                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.124, BCE loss: 0.124, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.520                     (Diversity_weight = 0)
{'accuracy': 0.7131394182547643, 'roc_auc': 0.7961247112048493, 'pr_auc': 0.7896220345666334, 'conicity_mean': 0.51773906, 'conicity_std': 0.105294496}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.738    0.683      0.713      0.711         0.711
precision    0.688    0.749      0.713      0.719         0.718
recall       0.796    0.627      0.713      0.712         0.713
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7961247112048493
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:39:19_2021
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:40:18,431 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:40:18,431 - type = vanillalstm
INFO - 2021-01-17 19:40:18,432 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:40:18,432 - vocab_size = 648
INFO - 2021-01-17 19:40:18,432 - embed_size = 200
INFO - 2021-01-17 19:40:18,432 - hidden_size = 128
INFO - 2021-01-17 19:40:18,432 - pre_embed = None
INFO - 2021-01-17 19:40:18,446 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:40:18,446 - hidden_size = 256
INFO - 2021-01-17 19:40:18,446 - output_size = 1
INFO - 2021-01-17 19:40:18,446 - use_attention = True
INFO - 2021-01-17 19:40:18,446 - regularizer_attention = None
INFO - 2021-01-17 19:40:18,447 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b609c974dd0> and extras set()
INFO - 2021-01-17 19:40:18,447 - attention.type = tanh
INFO - 2021-01-17 19:40:18,447 - type = tanh
INFO - 2021-01-17 19:40:18,447 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b609c974dd0> and extras set()
INFO - 2021-01-17 19:40:18,447 - attention.hidden_size = 256
INFO - 2021-01-17 19:40:18,447 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7567567567567568, 'roc_auc': 0.8328322188547328, 'pr_auc': 0.8307337910553036, 'conicity_mean': '0.54344696', 'conicity_std': '0.110146046'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.748    0.765      0.757      0.756         0.757
precision    0.763    0.751      0.757      0.757         0.757
recall       0.734    0.779      0.757      0.756         0.757
support    492.000  507.000    999.000    999.000       999.000
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:40:19,891 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:40:19,891 - type = vanillalstm
INFO - 2021-01-17 19:40:19,892 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:40:19,892 - vocab_size = 648
INFO - 2021-01-17 19:40:19,892 - embed_size = 200
INFO - 2021-01-17 19:40:19,892 - hidden_size = 128
INFO - 2021-01-17 19:40:19,892 - pre_embed = None
INFO - 2021-01-17 19:40:19,906 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:40:19,906 - hidden_size = 256
INFO - 2021-01-17 19:40:19,906 - output_size = 1
INFO - 2021-01-17 19:40:19,906 - use_attention = True
INFO - 2021-01-17 19:40:19,907 - regularizer_attention = None
INFO - 2021-01-17 19:40:19,907 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b60366acc90> and extras set()
INFO - 2021-01-17 19:40:19,907 - attention.type = tanh
INFO - 2021-01-17 19:40:19,907 - type = tanh
INFO - 2021-01-17 19:40:19,907 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b60366acc90> and extras set()
INFO - 2021-01-17 19:40:19,907 - attention.hidden_size = 256
INFO - 2021-01-17 19:40:19,907 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7567567567567568, 'roc_auc': 0.8328322188547328, 'pr_auc': 0.8307337910553036, 'conicity_mean': '0.54344696', 'conicity_std': '0.110146046'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.748    0.765      0.757      0.756         0.757
precision    0.763    0.751      0.757      0.757         0.757
recall       0.734    0.779      0.757      0.756         0.757
support    492.000  507.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_de ortho_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:40:30,349 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:40:30,349 - type = ortholstm
INFO - 2021-01-17 19:40:30,349 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:40:30,368 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:40:30,608 - vocab_size = 648
INFO - 2021-01-17 19:40:30,608 - embed_size = 200
INFO - 2021-01-17 19:40:30,608 - hidden_size = 128
INFO - 2021-01-17 19:40:30,608 - pre_embed = None
INFO - 2021-01-17 19:40:33,704 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:40:33,705 - hidden_size = 256
INFO - 2021-01-17 19:40:33,705 - output_size = 1
INFO - 2021-01-17 19:40:33,705 - use_attention = True
INFO - 2021-01-17 19:40:33,705 - regularizer_attention = None
INFO - 2021-01-17 19:40:33,705 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b9786428110> and extras set()
INFO - 2021-01-17 19:40:33,706 - attention.type = tanh
INFO - 2021-01-17 19:40:33,706 - type = tanh
INFO - 2021-01-17 19:40:33,706 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b9786428110> and extras set()
INFO - 2021-01-17 19:40:33,706 - attention.hidden_size = 256
INFO - 2021-01-17 19:40:33,706 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.706, BCE loss: 0.706, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.091                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.104                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.117                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.703, BCE loss: 0.703, Diversity Loss: 0.113                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.110                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.095                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.273                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.743, BCE loss: 0.743, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.642, BCE loss: 0.642, Diversity Loss: 0.184                     (Diversity_weight = 0)
{'accuracy': 0.6579739217652959, 'roc_auc': 0.6930922614974683, 'pr_auc': 0.6417200377143975, 'conicity_mean': 0.1583341, 'conicity_std': 0.041067522}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.642    0.672      0.658      0.657         0.657
precision    0.685    0.636      0.658      0.660         0.661
recall       0.605    0.713      0.658      0.659         0.658
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.6930922614974683
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:40:33_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.641, BCE loss: 0.641, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.639, BCE loss: 0.639, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.578, BCE loss: 0.578, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.105                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.707, BCE loss: 0.707, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.564, BCE loss: 0.564, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.635, BCE loss: 0.635, Diversity Loss: 0.118                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.630, BCE loss: 0.630, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.092                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.553, BCE loss: 0.553, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.646, BCE loss: 0.646, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.484, BCE loss: 0.484, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.567, BCE loss: 0.567, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.619, BCE loss: 0.619, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.583, BCE loss: 0.583, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.636, BCE loss: 0.636, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.515, BCE loss: 0.515, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.613, BCE loss: 0.613, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.637, BCE loss: 0.637, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.554, BCE loss: 0.554, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.550, BCE loss: 0.550, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.553, BCE loss: 0.553, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.623, BCE loss: 0.623, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.583, BCE loss: 0.583, Diversity Loss: 0.105                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.631, BCE loss: 0.631, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.714, BCE loss: 0.714, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.480, BCE loss: 0.480, Diversity Loss: 0.298                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.510, BCE loss: 0.510, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.627, BCE loss: 0.627, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.615, BCE loss: 0.615, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.573, BCE loss: 0.573, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.643, BCE loss: 0.643, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.643, BCE loss: 0.643, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.832, BCE loss: 0.832, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.607, BCE loss: 0.607, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.500, BCE loss: 0.500, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.539, BCE loss: 0.539, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.520, BCE loss: 0.520, Diversity Loss: 0.238                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.541, BCE loss: 0.541, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.707, BCE loss: 0.707, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.615, BCE loss: 0.615, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.600, BCE loss: 0.600, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.571, BCE loss: 0.571, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.559, BCE loss: 0.559, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.547, BCE loss: 0.547, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.540, BCE loss: 0.540, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.650, BCE loss: 0.650, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.485, BCE loss: 0.485, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.452, BCE loss: 0.452, Diversity Loss: 0.180                     (Diversity_weight = 0)
{'accuracy': 0.6920762286860582, 'roc_auc': 0.7358862690484048, 'pr_auc': 0.6974380481383984, 'conicity_mean': 0.17024525, 'conicity_std': 0.04573552}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.696    0.688      0.692      0.692         0.692
precision    0.697    0.687      0.692      0.692         0.692
recall       0.696    0.688      0.692      0.692         0.692
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7358862690484048
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:40:33_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.621, BCE loss: 0.621, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.447, BCE loss: 0.447, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.569, BCE loss: 0.569, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.417, BCE loss: 0.417, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.478, BCE loss: 0.478, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.463, BCE loss: 0.463, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.474, BCE loss: 0.474, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.565, BCE loss: 0.565, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.658, BCE loss: 0.658, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.424, BCE loss: 0.424, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.624, BCE loss: 0.624, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.471, BCE loss: 0.471, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.318, BCE loss: 0.318, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.407, BCE loss: 0.407, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.424, BCE loss: 0.424, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.250, BCE loss: 0.250, Diversity Loss: 0.245                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.460, BCE loss: 0.460, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.476, BCE loss: 0.476, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.411, BCE loss: 0.411, Diversity Loss: 0.111                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.251, BCE loss: 0.251, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.491, BCE loss: 0.491, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.382, BCE loss: 0.382, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.308, BCE loss: 0.308, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.460, BCE loss: 0.460, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.470, BCE loss: 0.470, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.461, BCE loss: 0.461, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.548, BCE loss: 0.548, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.475, BCE loss: 0.475, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.839, BCE loss: 0.839, Diversity Loss: 0.106                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.361, BCE loss: 0.361, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.541, BCE loss: 0.541, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.465, BCE loss: 0.465, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.480, BCE loss: 0.480, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.789, BCE loss: 0.789, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.610, BCE loss: 0.610, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.579, BCE loss: 0.579, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.551, BCE loss: 0.551, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.328, BCE loss: 0.328, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.400, BCE loss: 0.400, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.443, BCE loss: 0.443, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.513, BCE loss: 0.513, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.464, BCE loss: 0.464, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.607, BCE loss: 0.607, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.387, BCE loss: 0.387, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.283                     (Diversity_weight = 0)
{'accuracy': 0.6840521564694082, 'roc_auc': 0.7499617623145473, 'pr_auc': 0.7146112520376491, 'conicity_mean': 0.16959736, 'conicity_std': 0.04281188}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.709    0.654      0.684      0.682         0.682
precision    0.666    0.710      0.684      0.688         0.687
recall       0.759    0.607      0.684      0.683         0.684
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7499617623145473
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:40:33_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.398, BCE loss: 0.398, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.444, BCE loss: 0.444, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.465, BCE loss: 0.465, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.407, BCE loss: 0.407, Diversity Loss: 0.103                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.412, BCE loss: 0.412, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.320, BCE loss: 0.320, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.477, BCE loss: 0.477, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.590, BCE loss: 0.590, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.389, BCE loss: 0.389, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.286, BCE loss: 0.286, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.331, BCE loss: 0.331, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.525, BCE loss: 0.525, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.298, BCE loss: 0.298, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.389, BCE loss: 0.389, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.369, BCE loss: 0.369, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.293, BCE loss: 0.293, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.544, BCE loss: 0.544, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.272, BCE loss: 0.272, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.499, BCE loss: 0.499, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.368, BCE loss: 0.368, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.317, BCE loss: 0.317, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.241, BCE loss: 0.241, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.359, BCE loss: 0.359, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.167, BCE loss: 0.167, Diversity Loss: 0.241                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.223, BCE loss: 0.223, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.291, BCE loss: 0.291, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.317, BCE loss: 0.317, Diversity Loss: 0.228                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.307, BCE loss: 0.307, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.545, BCE loss: 0.545, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.347, BCE loss: 0.347, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.560, BCE loss: 0.560, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.438, BCE loss: 0.438, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.346, BCE loss: 0.346, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.518, BCE loss: 0.518, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.322, BCE loss: 0.322, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.364, BCE loss: 0.364, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.351, BCE loss: 0.351, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.281                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.211, BCE loss: 0.211, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.368, BCE loss: 0.368, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.515, BCE loss: 0.515, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.353, BCE loss: 0.353, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.400, BCE loss: 0.400, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.339, BCE loss: 0.339, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.434, BCE loss: 0.434, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.417, BCE loss: 0.417, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.406, BCE loss: 0.406, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.411, BCE loss: 0.411, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.290, BCE loss: 0.290, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.375, BCE loss: 0.375, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.201                     (Diversity_weight = 0)
{'accuracy': 0.6740220661985958, 'roc_auc': 0.7308912198224161, 'pr_auc': 0.6869979809650886, 'conicity_mean': 0.17475353, 'conicity_std': 0.04484918}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.642    0.701      0.674      0.671         0.671
precision    0.726    0.639      0.674      0.682         0.683
recall       0.575    0.776      0.674      0.676         0.674
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7308912198224161
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:40:33_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.107                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.206, BCE loss: 0.206, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.238, BCE loss: 0.238, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.287                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.293, BCE loss: 0.293, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.172, BCE loss: 0.172, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.250, BCE loss: 0.250, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.280, BCE loss: 0.280, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.264, BCE loss: 0.264, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.287, BCE loss: 0.287, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.412, BCE loss: 0.412, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.104                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.427, BCE loss: 0.427, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.338, BCE loss: 0.338, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.245, BCE loss: 0.245, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.319, BCE loss: 0.319, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.198, BCE loss: 0.198, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.225, BCE loss: 0.225, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.454, BCE loss: 0.454, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.254, BCE loss: 0.254, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.159, BCE loss: 0.159, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.195, BCE loss: 0.195, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.230, BCE loss: 0.230, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.308, BCE loss: 0.308, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.364, BCE loss: 0.364, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.218, BCE loss: 0.218, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.236, BCE loss: 0.236, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.337, BCE loss: 0.337, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.173, BCE loss: 0.173, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.215, BCE loss: 0.215, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.353, BCE loss: 0.353, Diversity Loss: 0.163                     (Diversity_weight = 0)
{'accuracy': 0.6880641925777332, 'roc_auc': 0.7336886083897506, 'pr_auc': 0.6902991975601586, 'conicity_mean': 0.17559741, 'conicity_std': 0.04424122}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.684    0.692      0.688      0.688         0.688
precision    0.704    0.674      0.688      0.689         0.689
recall       0.666    0.711      0.688      0.688         0.688
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7336886083897506
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:40:33_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.130, BCE loss: 0.130, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.110, BCE loss: 0.110, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.109                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.119, BCE loss: 0.119, Diversity Loss: 0.100                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.136, BCE loss: 0.136, Diversity Loss: 0.278                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.133, BCE loss: 0.133, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.119, BCE loss: 0.119, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.125, BCE loss: 0.125, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.108, BCE loss: 0.108, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.108, BCE loss: 0.108, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.290, BCE loss: 0.290, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.314, BCE loss: 0.314, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.213, BCE loss: 0.213, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.303, BCE loss: 0.303, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.416, BCE loss: 0.416, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.161, BCE loss: 0.161, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.098, BCE loss: 0.098, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.330, BCE loss: 0.330, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.173                     (Diversity_weight = 0)
{'accuracy': 0.6870611835506519, 'roc_auc': 0.7390620094507458, 'pr_auc': 0.6849015849748358, 'conicity_mean': 0.17362112, 'conicity_std': 0.043039422}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.694    0.680      0.687      0.687         0.687
precision    0.689    0.685      0.687      0.687         0.687
recall       0.700    0.674      0.687      0.687         0.687
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7390620094507458
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:40:33_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.232, BCE loss: 0.232, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.125, BCE loss: 0.125, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.155, BCE loss: 0.155, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.279                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.234                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.123                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.237                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.128, BCE loss: 0.128, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.124, BCE loss: 0.124, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.106                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.108, BCE loss: 0.108, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.117                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.011, BCE loss: 0.011, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.242, BCE loss: 0.242, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.127                     (Diversity_weight = 0)
{'accuracy': 0.6740220661985958, 'roc_auc': 0.7257935325986331, 'pr_auc': 0.6826252822791042, 'conicity_mean': 0.17287499, 'conicity_std': 0.043532807}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.682    0.665      0.674      0.674         0.674
precision    0.675    0.673      0.674      0.674         0.674
recall       0.690    0.658      0.674      0.674         0.674
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7257935325986331
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:40:33_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.119                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.014, BCE loss: 0.014, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.011, BCE loss: 0.011, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.119                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.237                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.281                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.108                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.014, BCE loss: 0.014, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.227                     (Diversity_weight = 0)
{'accuracy': 0.683049147442327, 'roc_auc': 0.7338777843072538, 'pr_auc': 0.6931917458343522, 'conicity_mean': 0.17313118, 'conicity_std': 0.042057954}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.700    0.664      0.683      0.682         0.682
precision    0.673    0.695      0.683      0.684         0.684
recall       0.729    0.635      0.683      0.682         0.683
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7338777843072538
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:40:33_2021
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:85: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:86: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:42:11,619 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:42:11,620 - type = ortholstm
INFO - 2021-01-17 19:42:11,620 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:42:11,620 - vocab_size = 648
INFO - 2021-01-17 19:42:11,620 - embed_size = 200
INFO - 2021-01-17 19:42:11,620 - hidden_size = 128
INFO - 2021-01-17 19:42:11,620 - pre_embed = None
INFO - 2021-01-17 19:42:11,635 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:42:11,635 - hidden_size = 256
INFO - 2021-01-17 19:42:11,635 - output_size = 1
INFO - 2021-01-17 19:42:11,635 - use_attention = True
INFO - 2021-01-17 19:42:11,635 - regularizer_attention = None
INFO - 2021-01-17 19:42:11,635 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b9786456510> and extras set()
INFO - 2021-01-17 19:42:11,635 - attention.type = tanh
INFO - 2021-01-17 19:42:11,635 - type = tanh
INFO - 2021-01-17 19:42:11,635 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b9786456510> and extras set()
INFO - 2021-01-17 19:42:11,636 - attention.hidden_size = 256
INFO - 2021-01-17 19:42:11,636 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.6826826826826827, 'roc_auc': 0.7698401244367473, 'pr_auc': 0.7667079214364352, 'conicity_mean': '0.16934237', 'conicity_std': '0.039868966'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.706    0.656      0.683      0.681         0.680
precision    0.650    0.729      0.683      0.690         0.690
recall       0.772    0.596      0.683      0.684         0.683
support    492.000  507.000    999.000    999.000       999.000
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:42:13,862 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:42:13,862 - type = ortholstm
INFO - 2021-01-17 19:42:13,863 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:42:13,863 - vocab_size = 648
INFO - 2021-01-17 19:42:13,863 - embed_size = 200
INFO - 2021-01-17 19:42:13,863 - hidden_size = 128
INFO - 2021-01-17 19:42:13,863 - pre_embed = None
INFO - 2021-01-17 19:42:13,877 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:42:13,877 - hidden_size = 256
INFO - 2021-01-17 19:42:13,877 - output_size = 1
INFO - 2021-01-17 19:42:13,877 - use_attention = True
INFO - 2021-01-17 19:42:13,877 - regularizer_attention = None
INFO - 2021-01-17 19:42:13,877 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b9786443e10> and extras set()
INFO - 2021-01-17 19:42:13,878 - attention.type = tanh
INFO - 2021-01-17 19:42:13,878 - type = tanh
INFO - 2021-01-17 19:42:13,878 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b9786443e10> and extras set()
INFO - 2021-01-17 19:42:13,878 - attention.hidden_size = 256
INFO - 2021-01-17 19:42:13,878 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.6826826826826827, 'roc_auc': 0.7698401244367473, 'pr_auc': 0.7667079214364352, 'conicity_mean': '0.16934237', 'conicity_std': '0.039868966'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.706    0.656      0.683      0.681         0.680
precision    0.650    0.729      0.683      0.690         0.690
recall       0.772    0.596      0.683      0.684         0.683
support    492.000  507.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_de diversity_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:42:27,238 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:42:27,238 - type = vanillalstm
INFO - 2021-01-17 19:42:27,238 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:42:27,258 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:42:27,500 - vocab_size = 648
INFO - 2021-01-17 19:42:27,500 - embed_size = 200
INFO - 2021-01-17 19:42:27,500 - hidden_size = 128
INFO - 2021-01-17 19:42:27,500 - pre_embed = None
INFO - 2021-01-17 19:42:30,590 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:42:30,591 - hidden_size = 256
INFO - 2021-01-17 19:42:30,591 - output_size = 1
INFO - 2021-01-17 19:42:30,591 - use_attention = True
INFO - 2021-01-17 19:42:30,591 - regularizer_attention = None
INFO - 2021-01-17 19:42:30,591 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b90b2a21190> and extras set()
INFO - 2021-01-17 19:42:30,591 - attention.type = tanh
INFO - 2021-01-17 19:42:30,591 - type = tanh
INFO - 2021-01-17 19:42:30,591 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b90b2a21190> and extras set()
INFO - 2021-01-17 19:42:30,592 - attention.hidden_size = 256
INFO - 2021-01-17 19:42:30,592 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.947, BCE loss: 0.671, Diversity Loss: 0.551                     (Diversity_weight = 0.5)
Epoch: 0 Step: 1 Total Loss: 1.053, BCE loss: 0.758, Diversity Loss: 0.590                     (Diversity_weight = 0.5)
Epoch: 0 Step: 2 Total Loss: 0.945, BCE loss: 0.693, Diversity Loss: 0.504                     (Diversity_weight = 0.5)
Epoch: 0 Step: 3 Total Loss: 0.922, BCE loss: 0.694, Diversity Loss: 0.456                     (Diversity_weight = 0.5)
Epoch: 0 Step: 4 Total Loss: 1.032, BCE loss: 0.756, Diversity Loss: 0.551                     (Diversity_weight = 0.5)
Epoch: 0 Step: 5 Total Loss: 0.906, BCE loss: 0.698, Diversity Loss: 0.416                     (Diversity_weight = 0.5)
Epoch: 0 Step: 6 Total Loss: 0.868, BCE loss: 0.691, Diversity Loss: 0.354                     (Diversity_weight = 0.5)
Epoch: 0 Step: 7 Total Loss: 0.869, BCE loss: 0.699, Diversity Loss: 0.339                     (Diversity_weight = 0.5)
Epoch: 0 Step: 8 Total Loss: 0.830, BCE loss: 0.698, Diversity Loss: 0.264                     (Diversity_weight = 0.5)
Epoch: 0 Step: 9 Total Loss: 0.803, BCE loss: 0.682, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 0 Step: 10 Total Loss: 0.819, BCE loss: 0.694, Diversity Loss: 0.250                     (Diversity_weight = 0.5)
Epoch: 0 Step: 11 Total Loss: 0.828, BCE loss: 0.693, Diversity Loss: 0.268                     (Diversity_weight = 0.5)
Epoch: 0 Step: 12 Total Loss: 0.842, BCE loss: 0.688, Diversity Loss: 0.308                     (Diversity_weight = 0.5)
Epoch: 0 Step: 13 Total Loss: 0.815, BCE loss: 0.691, Diversity Loss: 0.247                     (Diversity_weight = 0.5)
Epoch: 0 Step: 14 Total Loss: 0.811, BCE loss: 0.691, Diversity Loss: 0.240                     (Diversity_weight = 0.5)
Epoch: 0 Step: 15 Total Loss: 0.784, BCE loss: 0.690, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 0 Step: 16 Total Loss: 0.782, BCE loss: 0.696, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 0 Step: 17 Total Loss: 0.806, BCE loss: 0.695, Diversity Loss: 0.222                     (Diversity_weight = 0.5)
Epoch: 0 Step: 18 Total Loss: 0.842, BCE loss: 0.694, Diversity Loss: 0.296                     (Diversity_weight = 0.5)
Epoch: 0 Step: 19 Total Loss: 0.812, BCE loss: 0.691, Diversity Loss: 0.240                     (Diversity_weight = 0.5)
Epoch: 0 Step: 20 Total Loss: 0.810, BCE loss: 0.688, Diversity Loss: 0.243                     (Diversity_weight = 0.5)
Epoch: 0 Step: 21 Total Loss: 0.795, BCE loss: 0.693, Diversity Loss: 0.204                     (Diversity_weight = 0.5)
Epoch: 0 Step: 22 Total Loss: 0.800, BCE loss: 0.687, Diversity Loss: 0.227                     (Diversity_weight = 0.5)
Epoch: 0 Step: 23 Total Loss: 0.778, BCE loss: 0.691, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 0 Step: 24 Total Loss: 0.848, BCE loss: 0.694, Diversity Loss: 0.309                     (Diversity_weight = 0.5)
Epoch: 0 Step: 25 Total Loss: 0.782, BCE loss: 0.686, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 0 Step: 26 Total Loss: 0.781, BCE loss: 0.691, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 0 Step: 27 Total Loss: 0.820, BCE loss: 0.685, Diversity Loss: 0.269                     (Diversity_weight = 0.5)
Epoch: 0 Step: 28 Total Loss: 0.775, BCE loss: 0.687, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 0 Step: 29 Total Loss: 0.810, BCE loss: 0.688, Diversity Loss: 0.244                     (Diversity_weight = 0.5)
Epoch: 0 Step: 30 Total Loss: 0.778, BCE loss: 0.688, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 0 Step: 31 Total Loss: 0.816, BCE loss: 0.690, Diversity Loss: 0.252                     (Diversity_weight = 0.5)
Epoch: 0 Step: 32 Total Loss: 0.771, BCE loss: 0.694, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 0 Step: 33 Total Loss: 0.783, BCE loss: 0.687, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 0 Step: 34 Total Loss: 0.778, BCE loss: 0.689, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 0 Step: 35 Total Loss: 0.800, BCE loss: 0.688, Diversity Loss: 0.224                     (Diversity_weight = 0.5)
Epoch: 0 Step: 36 Total Loss: 0.769, BCE loss: 0.689, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 0 Step: 37 Total Loss: 0.794, BCE loss: 0.691, Diversity Loss: 0.206                     (Diversity_weight = 0.5)
Epoch: 0 Step: 38 Total Loss: 0.815, BCE loss: 0.686, Diversity Loss: 0.258                     (Diversity_weight = 0.5)
Epoch: 0 Step: 39 Total Loss: 0.778, BCE loss: 0.694, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 0 Step: 40 Total Loss: 0.776, BCE loss: 0.685, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 0 Step: 41 Total Loss: 0.773, BCE loss: 0.689, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 0 Step: 42 Total Loss: 0.782, BCE loss: 0.695, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 0 Step: 43 Total Loss: 0.753, BCE loss: 0.691, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 0 Step: 44 Total Loss: 0.756, BCE loss: 0.696, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 0 Step: 45 Total Loss: 0.759, BCE loss: 0.685, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 0 Step: 46 Total Loss: 0.762, BCE loss: 0.694, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 0 Step: 47 Total Loss: 0.833, BCE loss: 0.684, Diversity Loss: 0.297                     (Diversity_weight = 0.5)
Epoch: 0 Step: 48 Total Loss: 0.764, BCE loss: 0.685, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 0 Step: 49 Total Loss: 0.765, BCE loss: 0.691, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 0 Step: 50 Total Loss: 0.751, BCE loss: 0.678, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 0 Step: 51 Total Loss: 0.784, BCE loss: 0.684, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 0 Step: 52 Total Loss: 0.744, BCE loss: 0.685, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 0 Step: 53 Total Loss: 0.767, BCE loss: 0.689, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 0 Step: 54 Total Loss: 0.759, BCE loss: 0.693, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 0 Step: 55 Total Loss: 0.796, BCE loss: 0.695, Diversity Loss: 0.202                     (Diversity_weight = 0.5)
Epoch: 0 Step: 56 Total Loss: 0.765, BCE loss: 0.690, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 0 Step: 57 Total Loss: 0.872, BCE loss: 0.702, Diversity Loss: 0.340                     (Diversity_weight = 0.5)
Epoch: 0 Step: 58 Total Loss: 0.754, BCE loss: 0.684, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 0 Step: 59 Total Loss: 0.748, BCE loss: 0.688, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 0 Step: 60 Total Loss: 0.760, BCE loss: 0.690, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 0 Step: 61 Total Loss: 0.752, BCE loss: 0.688, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 0 Step: 62 Total Loss: 0.776, BCE loss: 0.669, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
{'accuracy': 0.5987963891675026, 'roc_auc': 0.7017500784878806, 'pr_auc': 0.668960581158288, 'conicity_mean': 0.15595043, 'conicity_std': 0.056071922}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.688    0.438      0.599      0.563         0.565
precision    0.568    0.706      0.599      0.637         0.636
recall       0.872    0.318      0.599      0.595         0.599
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7017500784878806
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:42:30_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.757, BCE loss: 0.682, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 1 Step: 1 Total Loss: 0.742, BCE loss: 0.691, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 1 Step: 2 Total Loss: 0.745, BCE loss: 0.663, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 1 Step: 3 Total Loss: 0.751, BCE loss: 0.669, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 1 Step: 4 Total Loss: 0.771, BCE loss: 0.680, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 1 Step: 5 Total Loss: 0.753, BCE loss: 0.674, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 1 Step: 6 Total Loss: 0.767, BCE loss: 0.643, Diversity Loss: 0.248                     (Diversity_weight = 0.5)
Epoch: 1 Step: 7 Total Loss: 0.738, BCE loss: 0.680, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 1 Step: 8 Total Loss: 0.829, BCE loss: 0.673, Diversity Loss: 0.312                     (Diversity_weight = 0.5)
Epoch: 1 Step: 9 Total Loss: 0.724, BCE loss: 0.672, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 1 Step: 10 Total Loss: 0.706, BCE loss: 0.628, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 1 Step: 11 Total Loss: 0.737, BCE loss: 0.687, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 1 Step: 12 Total Loss: 0.692, BCE loss: 0.600, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 1 Step: 13 Total Loss: 0.759, BCE loss: 0.695, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 1 Step: 14 Total Loss: 0.733, BCE loss: 0.680, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 1 Step: 15 Total Loss: 0.701, BCE loss: 0.630, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 1 Step: 16 Total Loss: 0.730, BCE loss: 0.654, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 1 Step: 17 Total Loss: 0.790, BCE loss: 0.686, Diversity Loss: 0.208                     (Diversity_weight = 0.5)
Epoch: 1 Step: 18 Total Loss: 0.748, BCE loss: 0.639, Diversity Loss: 0.217                     (Diversity_weight = 0.5)
Epoch: 1 Step: 19 Total Loss: 0.815, BCE loss: 0.742, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 1 Step: 20 Total Loss: 0.799, BCE loss: 0.732, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 1 Step: 21 Total Loss: 0.739, BCE loss: 0.624, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 1 Step: 22 Total Loss: 0.670, BCE loss: 0.602, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 1 Step: 23 Total Loss: 0.670, BCE loss: 0.589, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 1 Step: 24 Total Loss: 0.881, BCE loss: 0.773, Diversity Loss: 0.216                     (Diversity_weight = 0.5)
Epoch: 1 Step: 25 Total Loss: 0.743, BCE loss: 0.619, Diversity Loss: 0.247                     (Diversity_weight = 0.5)
Epoch: 1 Step: 26 Total Loss: 1.146, BCE loss: 1.072, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 1 Step: 27 Total Loss: 0.763, BCE loss: 0.700, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 1 Step: 28 Total Loss: 0.743, BCE loss: 0.639, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 1 Step: 29 Total Loss: 0.727, BCE loss: 0.645, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 1 Step: 30 Total Loss: 0.731, BCE loss: 0.665, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 1 Step: 31 Total Loss: 0.736, BCE loss: 0.686, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 1 Step: 32 Total Loss: 0.752, BCE loss: 0.681, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 1 Step: 33 Total Loss: 0.735, BCE loss: 0.681, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 1 Step: 34 Total Loss: 0.762, BCE loss: 0.685, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 1 Step: 35 Total Loss: 0.758, BCE loss: 0.687, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 1 Step: 36 Total Loss: 0.721, BCE loss: 0.661, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 1 Step: 37 Total Loss: 0.730, BCE loss: 0.671, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 1 Step: 38 Total Loss: 0.721, BCE loss: 0.645, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 1 Step: 39 Total Loss: 0.725, BCE loss: 0.656, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 1 Step: 40 Total Loss: 0.725, BCE loss: 0.668, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 1 Step: 41 Total Loss: 0.738, BCE loss: 0.668, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 1 Step: 42 Total Loss: 0.716, BCE loss: 0.650, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 1 Step: 43 Total Loss: 0.755, BCE loss: 0.663, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 1 Step: 44 Total Loss: 0.744, BCE loss: 0.681, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 1 Step: 45 Total Loss: 0.742, BCE loss: 0.657, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 1 Step: 46 Total Loss: 0.726, BCE loss: 0.675, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 1 Step: 47 Total Loss: 0.726, BCE loss: 0.674, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 1 Step: 48 Total Loss: 0.702, BCE loss: 0.626, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 1 Step: 49 Total Loss: 0.706, BCE loss: 0.653, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 1 Step: 50 Total Loss: 0.698, BCE loss: 0.638, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 1 Step: 51 Total Loss: 0.700, BCE loss: 0.621, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 1 Step: 52 Total Loss: 0.738, BCE loss: 0.643, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 1 Step: 53 Total Loss: 0.734, BCE loss: 0.666, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 1 Step: 54 Total Loss: 0.722, BCE loss: 0.669, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 1 Step: 55 Total Loss: 0.654, BCE loss: 0.589, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 1 Step: 56 Total Loss: 0.725, BCE loss: 0.663, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 57 Total Loss: 0.699, BCE loss: 0.647, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 1 Step: 58 Total Loss: 0.713, BCE loss: 0.664, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 1 Step: 59 Total Loss: 0.759, BCE loss: 0.704, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 1 Step: 60 Total Loss: 0.612, BCE loss: 0.550, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 1 Step: 61 Total Loss: 0.627, BCE loss: 0.566, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 62 Total Loss: 0.705, BCE loss: 0.641, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
{'accuracy': 0.6449348044132397, 'roc_auc': 0.6859357767885175, 'pr_auc': 0.6602748046861715, 'conicity_mean': 0.14047119, 'conicity_std': 0.04839084}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.665    0.622      0.645      0.644         0.644
precision    0.638    0.654      0.645      0.646         0.646
recall       0.696    0.593      0.645      0.644         0.645
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.6859357767885175
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:42:30_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.601, BCE loss: 0.548, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 2 Step: 1 Total Loss: 0.725, BCE loss: 0.681, Diversity Loss: 0.088                     (Diversity_weight = 0.5)
Epoch: 2 Step: 2 Total Loss: 0.647, BCE loss: 0.535, Diversity Loss: 0.225                     (Diversity_weight = 0.5)
Epoch: 2 Step: 3 Total Loss: 0.627, BCE loss: 0.567, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 2 Step: 4 Total Loss: 0.656, BCE loss: 0.604, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 2 Step: 5 Total Loss: 0.767, BCE loss: 0.725, Diversity Loss: 0.084                     (Diversity_weight = 0.5)
Epoch: 2 Step: 6 Total Loss: 0.735, BCE loss: 0.647, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 2 Step: 7 Total Loss: 0.625, BCE loss: 0.578, Diversity Loss: 0.094                     (Diversity_weight = 0.5)
Epoch: 2 Step: 8 Total Loss: 0.694, BCE loss: 0.615, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 2 Step: 9 Total Loss: 0.694, BCE loss: 0.631, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 2 Step: 10 Total Loss: 0.713, BCE loss: 0.652, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 2 Step: 11 Total Loss: 0.638, BCE loss: 0.547, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 2 Step: 12 Total Loss: 0.647, BCE loss: 0.598, Diversity Loss: 0.098                     (Diversity_weight = 0.5)
Epoch: 2 Step: 13 Total Loss: 0.659, BCE loss: 0.582, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 2 Step: 14 Total Loss: 0.696, BCE loss: 0.655, Diversity Loss: 0.081                     (Diversity_weight = 0.5)
Epoch: 2 Step: 15 Total Loss: 0.603, BCE loss: 0.533, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 2 Step: 16 Total Loss: 0.575, BCE loss: 0.493, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 2 Step: 17 Total Loss: 0.745, BCE loss: 0.671, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 2 Step: 18 Total Loss: 0.561, BCE loss: 0.500, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 2 Step: 19 Total Loss: 0.648, BCE loss: 0.589, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 2 Step: 20 Total Loss: 0.669, BCE loss: 0.620, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 2 Step: 21 Total Loss: 0.646, BCE loss: 0.568, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 2 Step: 22 Total Loss: 0.684, BCE loss: 0.633, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 2 Step: 23 Total Loss: 0.694, BCE loss: 0.611, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 2 Step: 24 Total Loss: 0.681, BCE loss: 0.627, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 2 Step: 25 Total Loss: 0.588, BCE loss: 0.517, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 2 Step: 26 Total Loss: 0.674, BCE loss: 0.623, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 2 Step: 27 Total Loss: 0.674, BCE loss: 0.632, Diversity Loss: 0.083                     (Diversity_weight = 0.5)
Epoch: 2 Step: 28 Total Loss: 0.610, BCE loss: 0.530, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 2 Step: 29 Total Loss: 0.639, BCE loss: 0.545, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 2 Step: 30 Total Loss: 0.662, BCE loss: 0.616, Diversity Loss: 0.091                     (Diversity_weight = 0.5)
Epoch: 2 Step: 31 Total Loss: 0.589, BCE loss: 0.522, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 2 Step: 32 Total Loss: 0.602, BCE loss: 0.553, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 2 Step: 33 Total Loss: 0.688, BCE loss: 0.627, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 2 Step: 34 Total Loss: 0.646, BCE loss: 0.582, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 2 Step: 35 Total Loss: 0.658, BCE loss: 0.514, Diversity Loss: 0.288                     (Diversity_weight = 0.5)
Epoch: 2 Step: 36 Total Loss: 0.615, BCE loss: 0.566, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 2 Step: 37 Total Loss: 0.656, BCE loss: 0.603, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 2 Step: 38 Total Loss: 0.535, BCE loss: 0.474, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 2 Step: 39 Total Loss: 0.727, BCE loss: 0.674, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 2 Step: 40 Total Loss: 0.671, BCE loss: 0.603, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 2 Step: 41 Total Loss: 0.517, BCE loss: 0.413, Diversity Loss: 0.210                     (Diversity_weight = 0.5)
Epoch: 2 Step: 42 Total Loss: 0.733, BCE loss: 0.668, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 2 Step: 43 Total Loss: 0.556, BCE loss: 0.445, Diversity Loss: 0.221                     (Diversity_weight = 0.5)
Epoch: 2 Step: 44 Total Loss: 0.572, BCE loss: 0.510, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 2 Step: 45 Total Loss: 0.609, BCE loss: 0.558, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 2 Step: 46 Total Loss: 0.701, BCE loss: 0.644, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 2 Step: 47 Total Loss: 0.771, BCE loss: 0.707, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 2 Step: 48 Total Loss: 0.763, BCE loss: 0.711, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 2 Step: 49 Total Loss: 0.793, BCE loss: 0.730, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 2 Step: 50 Total Loss: 0.622, BCE loss: 0.567, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 2 Step: 51 Total Loss: 0.766, BCE loss: 0.678, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 2 Step: 52 Total Loss: 0.666, BCE loss: 0.594, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 2 Step: 53 Total Loss: 0.585, BCE loss: 0.481, Diversity Loss: 0.208                     (Diversity_weight = 0.5)
Epoch: 2 Step: 54 Total Loss: 0.546, BCE loss: 0.481, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 2 Step: 55 Total Loss: 0.655, BCE loss: 0.594, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 2 Step: 56 Total Loss: 0.699, BCE loss: 0.635, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 2 Step: 57 Total Loss: 0.618, BCE loss: 0.533, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 2 Step: 58 Total Loss: 0.683, BCE loss: 0.615, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 2 Step: 59 Total Loss: 0.761, BCE loss: 0.663, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 2 Step: 60 Total Loss: 0.620, BCE loss: 0.525, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 2 Step: 61 Total Loss: 0.651, BCE loss: 0.580, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 2 Step: 62 Total Loss: 0.669, BCE loss: 0.588, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
{'accuracy': 0.6519558676028084, 'roc_auc': 0.7085080862642184, 'pr_auc': 0.6744318195073308, 'conicity_mean': 0.1463919, 'conicity_std': 0.050646275}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.658    0.646      0.652      0.652         0.652
precision    0.656    0.648      0.652      0.652         0.652
recall       0.660    0.644      0.652      0.652         0.652
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7085080862642184
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:42:30_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.588, BCE loss: 0.513, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 3 Step: 1 Total Loss: 0.587, BCE loss: 0.535, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 3 Step: 2 Total Loss: 0.625, BCE loss: 0.567, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 3 Step: 3 Total Loss: 0.522, BCE loss: 0.403, Diversity Loss: 0.238                     (Diversity_weight = 0.5)
Epoch: 3 Step: 4 Total Loss: 0.585, BCE loss: 0.534, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 3 Step: 5 Total Loss: 0.556, BCE loss: 0.501, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 3 Step: 6 Total Loss: 0.619, BCE loss: 0.564, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 3 Step: 7 Total Loss: 0.545, BCE loss: 0.452, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 3 Step: 8 Total Loss: 0.548, BCE loss: 0.476, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 3 Step: 9 Total Loss: 0.648, BCE loss: 0.575, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 3 Step: 10 Total Loss: 0.611, BCE loss: 0.551, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 3 Step: 11 Total Loss: 0.572, BCE loss: 0.504, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 3 Step: 12 Total Loss: 0.517, BCE loss: 0.417, Diversity Loss: 0.201                     (Diversity_weight = 0.5)
Epoch: 3 Step: 13 Total Loss: 0.540, BCE loss: 0.491, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 3 Step: 14 Total Loss: 0.396, BCE loss: 0.284, Diversity Loss: 0.224                     (Diversity_weight = 0.5)
Epoch: 3 Step: 15 Total Loss: 0.652, BCE loss: 0.588, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 3 Step: 16 Total Loss: 0.539, BCE loss: 0.492, Diversity Loss: 0.094                     (Diversity_weight = 0.5)
Epoch: 3 Step: 17 Total Loss: 0.458, BCE loss: 0.394, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 3 Step: 18 Total Loss: 0.668, BCE loss: 0.626, Diversity Loss: 0.083                     (Diversity_weight = 0.5)
Epoch: 3 Step: 19 Total Loss: 0.424, BCE loss: 0.319, Diversity Loss: 0.210                     (Diversity_weight = 0.5)
Epoch: 3 Step: 20 Total Loss: 0.718, BCE loss: 0.626, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 3 Step: 21 Total Loss: 0.544, BCE loss: 0.487, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 3 Step: 22 Total Loss: 0.673, BCE loss: 0.613, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 3 Step: 23 Total Loss: 0.618, BCE loss: 0.568, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 3 Step: 24 Total Loss: 0.666, BCE loss: 0.611, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 3 Step: 25 Total Loss: 0.549, BCE loss: 0.503, Diversity Loss: 0.091                     (Diversity_weight = 0.5)
Epoch: 3 Step: 26 Total Loss: 0.578, BCE loss: 0.515, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 3 Step: 27 Total Loss: 0.611, BCE loss: 0.536, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 28 Total Loss: 0.608, BCE loss: 0.531, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 3 Step: 29 Total Loss: 0.614, BCE loss: 0.544, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 3 Step: 30 Total Loss: 0.598, BCE loss: 0.523, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 3 Step: 31 Total Loss: 0.644, BCE loss: 0.582, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 3 Step: 32 Total Loss: 0.531, BCE loss: 0.457, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 3 Step: 33 Total Loss: 0.609, BCE loss: 0.530, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 3 Step: 34 Total Loss: 0.511, BCE loss: 0.433, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 3 Step: 35 Total Loss: 0.516, BCE loss: 0.377, Diversity Loss: 0.279                     (Diversity_weight = 0.5)
Epoch: 3 Step: 36 Total Loss: 0.546, BCE loss: 0.494, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 3 Step: 37 Total Loss: 0.579, BCE loss: 0.514, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 3 Step: 38 Total Loss: 0.522, BCE loss: 0.456, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 3 Step: 39 Total Loss: 0.745, BCE loss: 0.687, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 3 Step: 40 Total Loss: 0.655, BCE loss: 0.588, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 3 Step: 41 Total Loss: 0.547, BCE loss: 0.495, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 3 Step: 42 Total Loss: 0.435, BCE loss: 0.367, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 3 Step: 43 Total Loss: 0.650, BCE loss: 0.607, Diversity Loss: 0.086                     (Diversity_weight = 0.5)
Epoch: 3 Step: 44 Total Loss: 0.567, BCE loss: 0.504, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 3 Step: 45 Total Loss: 0.451, BCE loss: 0.374, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 3 Step: 46 Total Loss: 0.629, BCE loss: 0.593, Diversity Loss: 0.072                     (Diversity_weight = 0.5)
Epoch: 3 Step: 47 Total Loss: 0.596, BCE loss: 0.535, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 3 Step: 48 Total Loss: 0.619, BCE loss: 0.551, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 3 Step: 49 Total Loss: 0.550, BCE loss: 0.466, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 3 Step: 50 Total Loss: 0.670, BCE loss: 0.617, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 3 Step: 51 Total Loss: 0.629, BCE loss: 0.564, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 3 Step: 52 Total Loss: 0.624, BCE loss: 0.564, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 3 Step: 53 Total Loss: 0.692, BCE loss: 0.636, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 3 Step: 54 Total Loss: 0.614, BCE loss: 0.521, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 3 Step: 55 Total Loss: 0.474, BCE loss: 0.415, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 3 Step: 56 Total Loss: 0.643, BCE loss: 0.598, Diversity Loss: 0.090                     (Diversity_weight = 0.5)
Epoch: 3 Step: 57 Total Loss: 0.581, BCE loss: 0.491, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 3 Step: 58 Total Loss: 0.710, BCE loss: 0.625, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 3 Step: 59 Total Loss: 0.572, BCE loss: 0.494, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 3 Step: 60 Total Loss: 0.634, BCE loss: 0.563, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 3 Step: 61 Total Loss: 0.505, BCE loss: 0.414, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 3 Step: 62 Total Loss: 0.534, BCE loss: 0.485, Diversity Loss: 0.098                     (Diversity_weight = 0.5)
{'accuracy': 0.6659979939819458, 'roc_auc': 0.7215974497476313, 'pr_auc': 0.6957354754271114, 'conicity_mean': 0.13679217, 'conicity_std': 0.05076162}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.655    0.676      0.666      0.666         0.665
precision    0.688    0.647      0.666      0.668         0.668
recall       0.625    0.709      0.666      0.667         0.666
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7215974497476313
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:42:30_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.469, BCE loss: 0.412, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 4 Step: 1 Total Loss: 0.414, BCE loss: 0.352, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 4 Step: 2 Total Loss: 0.469, BCE loss: 0.407, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 4 Step: 3 Total Loss: 0.492, BCE loss: 0.449, Diversity Loss: 0.086                     (Diversity_weight = 0.5)
Epoch: 4 Step: 4 Total Loss: 0.436, BCE loss: 0.299, Diversity Loss: 0.273                     (Diversity_weight = 0.5)
Epoch: 4 Step: 5 Total Loss: 0.475, BCE loss: 0.380, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 4 Step: 6 Total Loss: 0.406, BCE loss: 0.342, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 4 Step: 7 Total Loss: 0.505, BCE loss: 0.431, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 4 Step: 8 Total Loss: 0.575, BCE loss: 0.513, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 4 Step: 9 Total Loss: 0.623, BCE loss: 0.585, Diversity Loss: 0.077                     (Diversity_weight = 0.5)
Epoch: 4 Step: 10 Total Loss: 0.463, BCE loss: 0.405, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 4 Step: 11 Total Loss: 0.410, BCE loss: 0.329, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 4 Step: 12 Total Loss: 0.413, BCE loss: 0.360, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 4 Step: 13 Total Loss: 0.559, BCE loss: 0.507, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 4 Step: 14 Total Loss: 0.503, BCE loss: 0.447, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 4 Step: 15 Total Loss: 0.452, BCE loss: 0.386, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 4 Step: 16 Total Loss: 0.455, BCE loss: 0.386, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 4 Step: 17 Total Loss: 0.475, BCE loss: 0.425, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 4 Step: 18 Total Loss: 0.430, BCE loss: 0.361, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 4 Step: 19 Total Loss: 0.386, BCE loss: 0.335, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 4 Step: 20 Total Loss: 0.721, BCE loss: 0.661, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 4 Step: 21 Total Loss: 0.264, BCE loss: 0.172, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 4 Step: 22 Total Loss: 0.437, BCE loss: 0.328, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 4 Step: 23 Total Loss: 0.506, BCE loss: 0.443, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 4 Step: 24 Total Loss: 0.602, BCE loss: 0.506, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 4 Step: 25 Total Loss: 0.620, BCE loss: 0.568, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 4 Step: 26 Total Loss: 0.494, BCE loss: 0.398, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 4 Step: 27 Total Loss: 0.467, BCE loss: 0.398, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 4 Step: 28 Total Loss: 0.582, BCE loss: 0.500, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 4 Step: 29 Total Loss: 0.495, BCE loss: 0.433, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 4 Step: 30 Total Loss: 0.561, BCE loss: 0.507, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 4 Step: 31 Total Loss: 0.549, BCE loss: 0.498, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 4 Step: 32 Total Loss: 0.514, BCE loss: 0.455, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 4 Step: 33 Total Loss: 0.546, BCE loss: 0.476, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 4 Step: 34 Total Loss: 0.545, BCE loss: 0.460, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 4 Step: 35 Total Loss: 0.516, BCE loss: 0.455, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 4 Step: 36 Total Loss: 0.442, BCE loss: 0.371, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 4 Step: 37 Total Loss: 0.485, BCE loss: 0.432, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 4 Step: 38 Total Loss: 0.516, BCE loss: 0.450, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 4 Step: 39 Total Loss: 0.712, BCE loss: 0.627, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 4 Step: 40 Total Loss: 0.514, BCE loss: 0.462, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 4 Step: 41 Total Loss: 0.444, BCE loss: 0.362, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 4 Step: 42 Total Loss: 0.536, BCE loss: 0.495, Diversity Loss: 0.082                     (Diversity_weight = 0.5)
Epoch: 4 Step: 43 Total Loss: 0.447, BCE loss: 0.371, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 4 Step: 44 Total Loss: 0.469, BCE loss: 0.397, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 4 Step: 45 Total Loss: 0.371, BCE loss: 0.306, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 4 Step: 46 Total Loss: 0.500, BCE loss: 0.456, Diversity Loss: 0.089                     (Diversity_weight = 0.5)
Epoch: 4 Step: 47 Total Loss: 0.429, BCE loss: 0.370, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 4 Step: 48 Total Loss: 0.517, BCE loss: 0.469, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 4 Step: 49 Total Loss: 0.391, BCE loss: 0.314, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 4 Step: 50 Total Loss: 0.624, BCE loss: 0.555, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 4 Step: 51 Total Loss: 0.581, BCE loss: 0.533, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 4 Step: 52 Total Loss: 0.653, BCE loss: 0.601, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 4 Step: 53 Total Loss: 0.392, BCE loss: 0.315, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 4 Step: 54 Total Loss: 0.392, BCE loss: 0.284, Diversity Loss: 0.217                     (Diversity_weight = 0.5)
Epoch: 4 Step: 55 Total Loss: 0.486, BCE loss: 0.412, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 4 Step: 56 Total Loss: 0.519, BCE loss: 0.473, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 4 Step: 57 Total Loss: 0.328, BCE loss: 0.267, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 4 Step: 58 Total Loss: 0.670, BCE loss: 0.600, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 4 Step: 59 Total Loss: 0.484, BCE loss: 0.419, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 4 Step: 60 Total Loss: 0.301, BCE loss: 0.184, Diversity Loss: 0.234                     (Diversity_weight = 0.5)
Epoch: 4 Step: 61 Total Loss: 0.526, BCE loss: 0.468, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 4 Step: 62 Total Loss: 0.636, BCE loss: 0.537, Diversity Loss: 0.197                     (Diversity_weight = 0.5)
{'accuracy': 0.6700100300902708, 'roc_auc': 0.7271157515113948, 'pr_auc': 0.7064188339792017, 'conicity_mean': 0.1407635, 'conicity_std': 0.05119894}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.706    0.624       0.67      0.665         0.666
precision    0.644    0.711       0.67      0.678         0.677
recall       0.781    0.556       0.67      0.668         0.670
support    506.000  491.000     997.00    997.000       997.000
Model Saved on  roc_auc 0.7271157515113948
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:42:30_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.399, BCE loss: 0.301, Diversity Loss: 0.196                     (Diversity_weight = 0.5)
Epoch: 5 Step: 1 Total Loss: 0.586, BCE loss: 0.545, Diversity Loss: 0.082                     (Diversity_weight = 0.5)
Epoch: 5 Step: 2 Total Loss: 0.389, BCE loss: 0.335, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 5 Step: 3 Total Loss: 0.332, BCE loss: 0.222, Diversity Loss: 0.221                     (Diversity_weight = 0.5)
Epoch: 5 Step: 4 Total Loss: 0.489, BCE loss: 0.438, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 5 Step: 5 Total Loss: 0.558, BCE loss: 0.467, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 5 Step: 6 Total Loss: 0.388, BCE loss: 0.317, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 5 Step: 7 Total Loss: 0.332, BCE loss: 0.275, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 5 Step: 8 Total Loss: 0.489, BCE loss: 0.424, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 5 Step: 9 Total Loss: 0.307, BCE loss: 0.243, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 5 Step: 10 Total Loss: 0.501, BCE loss: 0.409, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 5 Step: 11 Total Loss: 0.434, BCE loss: 0.369, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 5 Step: 12 Total Loss: 0.333, BCE loss: 0.285, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 5 Step: 13 Total Loss: 0.385, BCE loss: 0.330, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 5 Step: 14 Total Loss: 0.448, BCE loss: 0.383, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 5 Step: 15 Total Loss: 0.540, BCE loss: 0.478, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 5 Step: 16 Total Loss: 0.345, BCE loss: 0.291, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 5 Step: 17 Total Loss: 0.498, BCE loss: 0.448, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 5 Step: 18 Total Loss: 0.403, BCE loss: 0.295, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 5 Step: 19 Total Loss: 0.361, BCE loss: 0.309, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 5 Step: 20 Total Loss: 0.320, BCE loss: 0.260, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 5 Step: 21 Total Loss: 0.401, BCE loss: 0.358, Diversity Loss: 0.086                     (Diversity_weight = 0.5)
Epoch: 5 Step: 22 Total Loss: 0.300, BCE loss: 0.229, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 5 Step: 23 Total Loss: 0.308, BCE loss: 0.231, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 5 Step: 24 Total Loss: 0.239, BCE loss: 0.142, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 5 Step: 25 Total Loss: 0.308, BCE loss: 0.246, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 5 Step: 26 Total Loss: 0.249, BCE loss: 0.184, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 5 Step: 27 Total Loss: 0.504, BCE loss: 0.460, Diversity Loss: 0.087                     (Diversity_weight = 0.5)
Epoch: 5 Step: 28 Total Loss: 0.292, BCE loss: 0.194, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 5 Step: 29 Total Loss: 0.307, BCE loss: 0.247, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 5 Step: 30 Total Loss: 0.345, BCE loss: 0.290, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 5 Step: 31 Total Loss: 0.257, BCE loss: 0.208, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 5 Step: 32 Total Loss: 0.342, BCE loss: 0.282, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 5 Step: 33 Total Loss: 0.281, BCE loss: 0.205, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 5 Step: 34 Total Loss: 0.264, BCE loss: 0.148, Diversity Loss: 0.231                     (Diversity_weight = 0.5)
Epoch: 5 Step: 35 Total Loss: 0.426, BCE loss: 0.373, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 5 Step: 36 Total Loss: 0.437, BCE loss: 0.390, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 5 Step: 37 Total Loss: 0.459, BCE loss: 0.397, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 5 Step: 38 Total Loss: 0.410, BCE loss: 0.317, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 5 Step: 39 Total Loss: 0.336, BCE loss: 0.252, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 5 Step: 40 Total Loss: 0.356, BCE loss: 0.288, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 5 Step: 41 Total Loss: 0.450, BCE loss: 0.309, Diversity Loss: 0.281                     (Diversity_weight = 0.5)
Epoch: 5 Step: 42 Total Loss: 0.528, BCE loss: 0.459, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 5 Step: 43 Total Loss: 0.339, BCE loss: 0.271, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 5 Step: 44 Total Loss: 0.210, BCE loss: 0.124, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 5 Step: 45 Total Loss: 0.442, BCE loss: 0.362, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 5 Step: 46 Total Loss: 0.205, BCE loss: 0.137, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 5 Step: 47 Total Loss: 0.525, BCE loss: 0.467, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 5 Step: 48 Total Loss: 0.550, BCE loss: 0.494, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 5 Step: 49 Total Loss: 0.414, BCE loss: 0.344, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 5 Step: 50 Total Loss: 0.347, BCE loss: 0.294, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 5 Step: 51 Total Loss: 0.332, BCE loss: 0.267, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 5 Step: 52 Total Loss: 0.407, BCE loss: 0.319, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 5 Step: 53 Total Loss: 0.323, BCE loss: 0.250, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 5 Step: 54 Total Loss: 0.413, BCE loss: 0.370, Diversity Loss: 0.086                     (Diversity_weight = 0.5)
Epoch: 5 Step: 55 Total Loss: 0.334, BCE loss: 0.278, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 5 Step: 56 Total Loss: 0.276, BCE loss: 0.224, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 5 Step: 57 Total Loss: 0.423, BCE loss: 0.348, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 5 Step: 58 Total Loss: 0.422, BCE loss: 0.374, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 5 Step: 59 Total Loss: 0.411, BCE loss: 0.342, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 5 Step: 60 Total Loss: 0.432, BCE loss: 0.362, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 5 Step: 61 Total Loss: 0.462, BCE loss: 0.396, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 5 Step: 62 Total Loss: 0.291, BCE loss: 0.218, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
{'accuracy': 0.6629889669007021, 'roc_auc': 0.7332659813400095, 'pr_auc': 0.7211604823846791, 'conicity_mean': 0.13589604, 'conicity_std': 0.04837376}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.669    0.657      0.663      0.663         0.663
precision    0.667    0.658      0.663      0.663         0.663
recall       0.670    0.656      0.663      0.663         0.663
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7332659813400095
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:42:30_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.252, BCE loss: 0.203, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 6 Step: 1 Total Loss: 0.291, BCE loss: 0.193, Diversity Loss: 0.197                     (Diversity_weight = 0.5)
Epoch: 6 Step: 2 Total Loss: 0.359, BCE loss: 0.307, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 6 Step: 3 Total Loss: 0.290, BCE loss: 0.228, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 6 Step: 4 Total Loss: 0.226, BCE loss: 0.157, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 6 Step: 5 Total Loss: 0.264, BCE loss: 0.219, Diversity Loss: 0.089                     (Diversity_weight = 0.5)
Epoch: 6 Step: 6 Total Loss: 0.368, BCE loss: 0.276, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 6 Step: 7 Total Loss: 0.353, BCE loss: 0.294, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 6 Step: 8 Total Loss: 0.346, BCE loss: 0.248, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 6 Step: 9 Total Loss: 0.287, BCE loss: 0.240, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 6 Step: 10 Total Loss: 0.282, BCE loss: 0.240, Diversity Loss: 0.085                     (Diversity_weight = 0.5)
Epoch: 6 Step: 11 Total Loss: 0.259, BCE loss: 0.197, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 6 Step: 12 Total Loss: 0.371, BCE loss: 0.293, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 6 Step: 13 Total Loss: 0.315, BCE loss: 0.242, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 6 Step: 14 Total Loss: 0.416, BCE loss: 0.375, Diversity Loss: 0.082                     (Diversity_weight = 0.5)
Epoch: 6 Step: 15 Total Loss: 0.228, BCE loss: 0.159, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 6 Step: 16 Total Loss: 0.211, BCE loss: 0.126, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 6 Step: 17 Total Loss: 0.197, BCE loss: 0.139, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 6 Step: 18 Total Loss: 0.292, BCE loss: 0.187, Diversity Loss: 0.211                     (Diversity_weight = 0.5)
Epoch: 6 Step: 19 Total Loss: 0.271, BCE loss: 0.226, Diversity Loss: 0.091                     (Diversity_weight = 0.5)
Epoch: 6 Step: 20 Total Loss: 0.369, BCE loss: 0.324, Diversity Loss: 0.090                     (Diversity_weight = 0.5)
Epoch: 6 Step: 21 Total Loss: 0.218, BCE loss: 0.115, Diversity Loss: 0.208                     (Diversity_weight = 0.5)
Epoch: 6 Step: 22 Total Loss: 0.164, BCE loss: 0.100, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 6 Step: 23 Total Loss: 0.295, BCE loss: 0.235, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 6 Step: 24 Total Loss: 0.359, BCE loss: 0.295, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 6 Step: 25 Total Loss: 0.213, BCE loss: 0.156, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 6 Step: 26 Total Loss: 0.302, BCE loss: 0.251, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 6 Step: 27 Total Loss: 0.216, BCE loss: 0.136, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 6 Step: 28 Total Loss: 0.252, BCE loss: 0.188, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 6 Step: 29 Total Loss: 0.210, BCE loss: 0.147, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 6 Step: 30 Total Loss: 0.266, BCE loss: 0.180, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 6 Step: 31 Total Loss: 0.319, BCE loss: 0.247, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 6 Step: 32 Total Loss: 0.355, BCE loss: 0.219, Diversity Loss: 0.274                     (Diversity_weight = 0.5)
Epoch: 6 Step: 33 Total Loss: 0.247, BCE loss: 0.170, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 6 Step: 34 Total Loss: 0.228, BCE loss: 0.176, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 6 Step: 35 Total Loss: 0.225, BCE loss: 0.161, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 6 Step: 36 Total Loss: 0.157, BCE loss: 0.073, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 6 Step: 37 Total Loss: 0.435, BCE loss: 0.358, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 6 Step: 38 Total Loss: 0.164, BCE loss: 0.112, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 6 Step: 39 Total Loss: 0.283, BCE loss: 0.224, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 6 Step: 40 Total Loss: 0.271, BCE loss: 0.204, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 6 Step: 41 Total Loss: 0.300, BCE loss: 0.228, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 6 Step: 42 Total Loss: 0.334, BCE loss: 0.254, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 6 Step: 43 Total Loss: 0.303, BCE loss: 0.233, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 6 Step: 44 Total Loss: 0.170, BCE loss: 0.056, Diversity Loss: 0.228                     (Diversity_weight = 0.5)
Epoch: 6 Step: 45 Total Loss: 0.405, BCE loss: 0.342, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 6 Step: 46 Total Loss: 0.536, BCE loss: 0.477, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 6 Step: 47 Total Loss: 0.462, BCE loss: 0.405, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 6 Step: 48 Total Loss: 0.223, BCE loss: 0.150, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 6 Step: 49 Total Loss: 0.338, BCE loss: 0.289, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 6 Step: 50 Total Loss: 0.174, BCE loss: 0.107, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 6 Step: 51 Total Loss: 0.288, BCE loss: 0.241, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 6 Step: 52 Total Loss: 0.274, BCE loss: 0.232, Diversity Loss: 0.083                     (Diversity_weight = 0.5)
Epoch: 6 Step: 53 Total Loss: 0.227, BCE loss: 0.149, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 6 Step: 54 Total Loss: 0.340, BCE loss: 0.286, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 6 Step: 55 Total Loss: 0.279, BCE loss: 0.182, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 6 Step: 56 Total Loss: 0.277, BCE loss: 0.224, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 6 Step: 57 Total Loss: 0.269, BCE loss: 0.210, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 6 Step: 58 Total Loss: 0.357, BCE loss: 0.291, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 6 Step: 59 Total Loss: 0.306, BCE loss: 0.225, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 6 Step: 60 Total Loss: 0.362, BCE loss: 0.307, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 6 Step: 61 Total Loss: 0.543, BCE loss: 0.490, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 6 Step: 62 Total Loss: 0.236, BCE loss: 0.177, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
{'accuracy': 0.6549648946840522, 'roc_auc': 0.7176811057533629, 'pr_auc': 0.7065393986664383, 'conicity_mean': 0.13844661, 'conicity_std': 0.05009654}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.681    0.624      0.655      0.653         0.653
precision    0.642    0.673      0.655      0.657         0.657
recall       0.725    0.582      0.655      0.654         0.655
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7176811057533629
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:42:30_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.284, BCE loss: 0.219, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 7 Step: 1 Total Loss: 0.162, BCE loss: 0.100, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 7 Step: 2 Total Loss: 0.196, BCE loss: 0.155, Diversity Loss: 0.083                     (Diversity_weight = 0.5)
Epoch: 7 Step: 3 Total Loss: 0.236, BCE loss: 0.163, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 7 Step: 4 Total Loss: 0.212, BCE loss: 0.099, Diversity Loss: 0.226                     (Diversity_weight = 0.5)
Epoch: 7 Step: 5 Total Loss: 0.223, BCE loss: 0.175, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 7 Step: 6 Total Loss: 0.175, BCE loss: 0.079, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 7 Step: 7 Total Loss: 0.162, BCE loss: 0.087, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 7 Step: 8 Total Loss: 0.143, BCE loss: 0.089, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 7 Step: 9 Total Loss: 0.218, BCE loss: 0.154, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 7 Step: 10 Total Loss: 0.176, BCE loss: 0.120, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 7 Step: 11 Total Loss: 0.170, BCE loss: 0.112, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 7 Step: 12 Total Loss: 0.233, BCE loss: 0.173, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 7 Step: 13 Total Loss: 0.267, BCE loss: 0.127, Diversity Loss: 0.280                     (Diversity_weight = 0.5)
Epoch: 7 Step: 14 Total Loss: 0.216, BCE loss: 0.146, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 7 Step: 15 Total Loss: 0.160, BCE loss: 0.108, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 7 Step: 16 Total Loss: 0.122, BCE loss: 0.057, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 7 Step: 17 Total Loss: 0.257, BCE loss: 0.162, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 7 Step: 18 Total Loss: 0.211, BCE loss: 0.115, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 7 Step: 19 Total Loss: 0.207, BCE loss: 0.127, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 7 Step: 20 Total Loss: 0.210, BCE loss: 0.158, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 7 Step: 21 Total Loss: 0.257, BCE loss: 0.207, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 7 Step: 22 Total Loss: 0.186, BCE loss: 0.128, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 7 Step: 23 Total Loss: 0.184, BCE loss: 0.131, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 7 Step: 24 Total Loss: 0.200, BCE loss: 0.131, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 7 Step: 25 Total Loss: 0.232, BCE loss: 0.176, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 7 Step: 26 Total Loss: 0.279, BCE loss: 0.182, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 7 Step: 27 Total Loss: 0.277, BCE loss: 0.180, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 7 Step: 28 Total Loss: 0.248, BCE loss: 0.194, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 7 Step: 29 Total Loss: 0.418, BCE loss: 0.361, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 7 Step: 30 Total Loss: 0.188, BCE loss: 0.131, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 7 Step: 31 Total Loss: 0.130, BCE loss: 0.067, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 7 Step: 32 Total Loss: 0.370, BCE loss: 0.306, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 7 Step: 33 Total Loss: 0.215, BCE loss: 0.143, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 7 Step: 34 Total Loss: 0.302, BCE loss: 0.197, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 7 Step: 35 Total Loss: 0.212, BCE loss: 0.160, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 7 Step: 36 Total Loss: 0.258, BCE loss: 0.211, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 7 Step: 37 Total Loss: 0.164, BCE loss: 0.109, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 7 Step: 38 Total Loss: 0.181, BCE loss: 0.117, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 7 Step: 39 Total Loss: 0.208, BCE loss: 0.102, Diversity Loss: 0.212                     (Diversity_weight = 0.5)
Epoch: 7 Step: 40 Total Loss: 0.431, BCE loss: 0.357, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 7 Step: 41 Total Loss: 0.242, BCE loss: 0.184, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 7 Step: 42 Total Loss: 0.318, BCE loss: 0.271, Diversity Loss: 0.095                     (Diversity_weight = 0.5)
Epoch: 7 Step: 43 Total Loss: 0.451, BCE loss: 0.372, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 7 Step: 44 Total Loss: 0.182, BCE loss: 0.106, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 7 Step: 45 Total Loss: 0.248, BCE loss: 0.177, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 7 Step: 46 Total Loss: 0.111, BCE loss: 0.033, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 7 Step: 47 Total Loss: 0.229, BCE loss: 0.152, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 7 Step: 48 Total Loss: 0.189, BCE loss: 0.123, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 7 Step: 49 Total Loss: 0.210, BCE loss: 0.147, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 7 Step: 50 Total Loss: 0.166, BCE loss: 0.113, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 7 Step: 51 Total Loss: 0.274, BCE loss: 0.231, Diversity Loss: 0.086                     (Diversity_weight = 0.5)
Epoch: 7 Step: 52 Total Loss: 0.294, BCE loss: 0.227, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 7 Step: 53 Total Loss: 0.179, BCE loss: 0.114, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 7 Step: 54 Total Loss: 0.289, BCE loss: 0.228, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 7 Step: 55 Total Loss: 0.324, BCE loss: 0.283, Diversity Loss: 0.083                     (Diversity_weight = 0.5)
Epoch: 7 Step: 56 Total Loss: 0.309, BCE loss: 0.213, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 7 Step: 57 Total Loss: 0.207, BCE loss: 0.131, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 7 Step: 58 Total Loss: 0.219, BCE loss: 0.172, Diversity Loss: 0.095                     (Diversity_weight = 0.5)
Epoch: 7 Step: 59 Total Loss: 0.178, BCE loss: 0.092, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 7 Step: 60 Total Loss: 0.209, BCE loss: 0.142, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 7 Step: 61 Total Loss: 0.236, BCE loss: 0.165, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 7 Step: 62 Total Loss: 0.215, BCE loss: 0.136, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
{'accuracy': 0.6569709127382146, 'roc_auc': 0.7167070510291975, 'pr_auc': 0.7027450046457342, 'conicity_mean': 0.13757072, 'conicity_std': 0.048582446}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.667    0.646      0.657      0.657         0.657
precision    0.657    0.657      0.657      0.657         0.657
recall       0.678    0.635      0.657      0.657         0.657
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7167070510291975
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:42:30_2021
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:43:30,237 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:43:30,238 - type = vanillalstm
INFO - 2021-01-17 19:43:30,238 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:43:30,238 - vocab_size = 648
INFO - 2021-01-17 19:43:30,238 - embed_size = 200
INFO - 2021-01-17 19:43:30,238 - hidden_size = 128
INFO - 2021-01-17 19:43:30,238 - pre_embed = None
INFO - 2021-01-17 19:43:30,253 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:43:30,253 - hidden_size = 256
INFO - 2021-01-17 19:43:30,254 - output_size = 1
INFO - 2021-01-17 19:43:30,254 - use_attention = True
INFO - 2021-01-17 19:43:30,254 - regularizer_attention = None
INFO - 2021-01-17 19:43:30,254 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b90b2a4fd50> and extras set()
INFO - 2021-01-17 19:43:30,254 - attention.type = tanh
INFO - 2021-01-17 19:43:30,254 - type = tanh
INFO - 2021-01-17 19:43:30,254 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b90b2a4fd50> and extras set()
INFO - 2021-01-17 19:43:30,254 - attention.hidden_size = 256
INFO - 2021-01-17 19:43:30,254 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.6776776776776777, 'roc_auc': 0.750934077388111, 'pr_auc': 0.7474288538705021, 'conicity_mean': '0.1348591', 'conicity_std': '0.04472883'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.681    0.674      0.678      0.678         0.678
precision    0.664    0.692      0.678      0.678         0.678
recall       0.699    0.657      0.678      0.678         0.678
support    492.000  507.000    999.000    999.000       999.000
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:43:31,699 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:43:31,699 - type = vanillalstm
INFO - 2021-01-17 19:43:31,700 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:43:31,700 - vocab_size = 648
INFO - 2021-01-17 19:43:31,700 - embed_size = 200
INFO - 2021-01-17 19:43:31,700 - hidden_size = 128
INFO - 2021-01-17 19:43:31,700 - pre_embed = None
INFO - 2021-01-17 19:43:31,714 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:43:31,714 - hidden_size = 256
INFO - 2021-01-17 19:43:31,714 - output_size = 1
INFO - 2021-01-17 19:43:31,714 - use_attention = True
INFO - 2021-01-17 19:43:31,714 - regularizer_attention = None
INFO - 2021-01-17 19:43:31,714 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b90968256d0> and extras set()
INFO - 2021-01-17 19:43:31,714 - attention.type = tanh
INFO - 2021-01-17 19:43:31,714 - type = tanh
INFO - 2021-01-17 19:43:31,714 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b90968256d0> and extras set()
INFO - 2021-01-17 19:43:31,715 - attention.hidden_size = 256
INFO - 2021-01-17 19:43:31,715 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.6776776776776777, 'roc_auc': 0.750934077388111, 'pr_auc': 0.7474288538705021, 'conicity_mean': '0.1348591', 'conicity_std': '0.04472883'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.681    0.674      0.678      0.678         0.678
precision    0.664    0.692      0.678      0.678         0.678
recall       0.699    0.657      0.678      0.678         0.678
support    492.000  507.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_fr vanilla_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:43:42,353 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:43:42,353 - type = vanillalstm
INFO - 2021-01-17 19:43:42,354 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:43:42,372 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:43:42,612 - vocab_size = 604
INFO - 2021-01-17 19:43:42,613 - embed_size = 200
INFO - 2021-01-17 19:43:42,613 - hidden_size = 128
INFO - 2021-01-17 19:43:42,613 - pre_embed = None
INFO - 2021-01-17 19:43:45,691 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:43:45,691 - hidden_size = 256
INFO - 2021-01-17 19:43:45,692 - output_size = 1
INFO - 2021-01-17 19:43:45,692 - use_attention = True
INFO - 2021-01-17 19:43:45,692 - regularizer_attention = None
INFO - 2021-01-17 19:43:45,692 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b18857b70d0> and extras set()
INFO - 2021-01-17 19:43:45,692 - attention.type = tanh
INFO - 2021-01-17 19:43:45,692 - type = tanh
INFO - 2021-01-17 19:43:45,692 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b18857b70d0> and extras set()
INFO - 2021-01-17 19:43:45,692 - attention.hidden_size = 256
INFO - 2021-01-17 19:43:45,692 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.714, BCE loss: 0.714, Diversity Loss: 0.666                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.665, BCE loss: 0.665, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.717, BCE loss: 0.717, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.742, BCE loss: 0.742, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.629                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.731, BCE loss: 0.731, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.707, BCE loss: 0.707, Diversity Loss: 0.712                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.707, BCE loss: 0.707, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.708, BCE loss: 0.708, Diversity Loss: 0.627                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.722, BCE loss: 0.722, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.710, BCE loss: 0.710, Diversity Loss: 0.631                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.681                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.712, BCE loss: 0.712, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.638                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.659                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.664, BCE loss: 0.664, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.713, BCE loss: 0.713, Diversity Loss: 0.801                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.681, BCE loss: 0.681, Diversity Loss: 0.585                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.646                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.681, BCE loss: 0.681, Diversity Loss: 0.711                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.593                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.640, BCE loss: 0.640, Diversity Loss: 0.631                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.624, BCE loss: 0.624, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.652, BCE loss: 0.652, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.645                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.682                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.650, BCE loss: 0.650, Diversity Loss: 0.637                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.684                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.753                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.769, BCE loss: 0.769, Diversity Loss: 0.680                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.738, BCE loss: 0.738, Diversity Loss: 0.726                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.719, BCE loss: 0.719, Diversity Loss: 0.723                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.728                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.665, BCE loss: 0.665, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.646, BCE loss: 0.646, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.605, BCE loss: 0.605, Diversity Loss: 0.581                     (Diversity_weight = 0)
{'accuracy': 0.6312625250501002, 'roc_auc': 0.7082856396971706, 'pr_auc': 0.7025749610071839, 'conicity_mean': 0.63349164, 'conicity_std': 0.12052095}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.693    0.539      0.631      0.616         0.616
precision    0.597    0.710      0.631      0.653         0.653
recall       0.825    0.434      0.631      0.630         0.631
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7082856396971706
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:43:45_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.727, BCE loss: 0.727, Diversity Loss: 0.739                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.641, BCE loss: 0.641, Diversity Loss: 0.641                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.644, BCE loss: 0.644, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.550, BCE loss: 0.550, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.593, BCE loss: 0.593, Diversity Loss: 0.666                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.743                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.650, BCE loss: 0.650, Diversity Loss: 0.642                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.651, BCE loss: 0.651, Diversity Loss: 0.629                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.778                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.562, BCE loss: 0.562, Diversity Loss: 0.642                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.604, BCE loss: 0.604, Diversity Loss: 0.582                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.553, BCE loss: 0.553, Diversity Loss: 0.641                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.717, BCE loss: 0.717, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.651                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.728                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.726                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.651, BCE loss: 0.651, Diversity Loss: 0.816                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.580, BCE loss: 0.580, Diversity Loss: 0.781                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.650, BCE loss: 0.650, Diversity Loss: 0.719                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.705, BCE loss: 0.705, Diversity Loss: 0.714                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.771                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.619, BCE loss: 0.619, Diversity Loss: 0.682                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.530, BCE loss: 0.530, Diversity Loss: 0.734                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.755                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.668                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.592, BCE loss: 0.592, Diversity Loss: 0.731                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.567, BCE loss: 0.567, Diversity Loss: 0.752                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.711, BCE loss: 0.711, Diversity Loss: 0.714                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.643, BCE loss: 0.643, Diversity Loss: 0.686                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.568, BCE loss: 0.568, Diversity Loss: 0.779                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.710                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.596, BCE loss: 0.596, Diversity Loss: 0.733                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.609, BCE loss: 0.609, Diversity Loss: 0.742                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.593, BCE loss: 0.593, Diversity Loss: 0.741                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.508, BCE loss: 0.508, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.644, BCE loss: 0.644, Diversity Loss: 0.678                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.545, BCE loss: 0.545, Diversity Loss: 0.637                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.721                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.701                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.562, BCE loss: 0.562, Diversity Loss: 0.623                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.647                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.790, BCE loss: 0.790, Diversity Loss: 0.693                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.627                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.454, BCE loss: 0.454, Diversity Loss: 0.613                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.657                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.641, BCE loss: 0.641, Diversity Loss: 0.708                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.560, BCE loss: 0.560, Diversity Loss: 0.793                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.594                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.635, BCE loss: 0.635, Diversity Loss: 0.663                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.648, BCE loss: 0.648, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.562, BCE loss: 0.562, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.568, BCE loss: 0.568, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.473, BCE loss: 0.473, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.599                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.606, BCE loss: 0.606, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.512, BCE loss: 0.512, Diversity Loss: 0.653                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.631                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.635, BCE loss: 0.635, Diversity Loss: 0.662                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.517, BCE loss: 0.517, Diversity Loss: 0.562                     (Diversity_weight = 0)
{'accuracy': 0.687374749498998, 'roc_auc': 0.7610257646042935, 'pr_auc': 0.7422367516164191, 'conicity_mean': 0.62028795, 'conicity_std': 0.10270545}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.695    0.679      0.687      0.687         0.687
precision    0.683    0.692      0.687      0.688         0.688
recall       0.708    0.667      0.687      0.687         0.687
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7610257646042935
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:43:45_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.530, BCE loss: 0.530, Diversity Loss: 0.640                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.777                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.473, BCE loss: 0.473, Diversity Loss: 0.663                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.442, BCE loss: 0.442, Diversity Loss: 0.634                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.673                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.675                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.458, BCE loss: 0.458, Diversity Loss: 0.651                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.651                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.504, BCE loss: 0.504, Diversity Loss: 0.669                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.408, BCE loss: 0.408, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.479, BCE loss: 0.479, Diversity Loss: 0.671                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.536, BCE loss: 0.536, Diversity Loss: 0.634                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.548, BCE loss: 0.548, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.428, BCE loss: 0.428, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.511, BCE loss: 0.511, Diversity Loss: 0.643                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.359, BCE loss: 0.359, Diversity Loss: 0.732                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.646, BCE loss: 0.646, Diversity Loss: 0.699                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.719                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.522, BCE loss: 0.522, Diversity Loss: 0.770                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.439, BCE loss: 0.439, Diversity Loss: 0.735                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.764                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.657                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.521, BCE loss: 0.521, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.475, BCE loss: 0.475, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.545, BCE loss: 0.545, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.550, BCE loss: 0.550, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.627                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.404, BCE loss: 0.404, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.426, BCE loss: 0.426, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.347, BCE loss: 0.347, Diversity Loss: 0.686                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.594, BCE loss: 0.594, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.843, BCE loss: 0.843, Diversity Loss: 0.657                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.618, BCE loss: 0.618, Diversity Loss: 0.643                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.435, BCE loss: 0.435, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.406, BCE loss: 0.406, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.687                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.542, BCE loss: 0.542, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.522, BCE loss: 0.522, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.371, BCE loss: 0.371, Diversity Loss: 0.652                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.531, BCE loss: 0.531, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.600, BCE loss: 0.600, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.665, BCE loss: 0.665, Diversity Loss: 0.708                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.486, BCE loss: 0.486, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.557, BCE loss: 0.557, Diversity Loss: 0.727                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.424, BCE loss: 0.424, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.636                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.519, BCE loss: 0.519, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.518, BCE loss: 0.518, Diversity Loss: 0.704                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.564, BCE loss: 0.564, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.591, BCE loss: 0.591, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.363, BCE loss: 0.363, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.594, BCE loss: 0.594, Diversity Loss: 0.700                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.428, BCE loss: 0.428, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.669, BCE loss: 0.669, Diversity Loss: 0.690                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.548, BCE loss: 0.548, Diversity Loss: 0.663                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.404, BCE loss: 0.404, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.694                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.407, BCE loss: 0.407, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.627                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.475, BCE loss: 0.475, Diversity Loss: 0.622                     (Diversity_weight = 0)
{'accuracy': 0.7274549098196392, 'roc_auc': 0.7954254272345724, 'pr_auc': 0.7645020436369675, 'conicity_mean': 0.6113943, 'conicity_std': 0.10755253}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.714    0.740      0.727      0.727         0.727
precision    0.758    0.702      0.727      0.730         0.731
recall       0.674    0.782      0.727      0.728         0.727
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7954254272345724
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:43:45_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.465, BCE loss: 0.465, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.393, BCE loss: 0.393, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.437, BCE loss: 0.437, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.394, BCE loss: 0.394, Diversity Loss: 0.593                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.386, BCE loss: 0.386, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.359, BCE loss: 0.359, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.440, BCE loss: 0.440, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.494, BCE loss: 0.494, Diversity Loss: 0.574                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.344, BCE loss: 0.344, Diversity Loss: 0.651                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.307, BCE loss: 0.307, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.593                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.738, BCE loss: 0.738, Diversity Loss: 0.693                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.609                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.522, BCE loss: 0.522, Diversity Loss: 0.664                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.477, BCE loss: 0.477, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.252, BCE loss: 0.252, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.213, BCE loss: 0.213, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.534, BCE loss: 0.534, Diversity Loss: 0.654                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.495, BCE loss: 0.495, Diversity Loss: 0.675                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.568, BCE loss: 0.568, Diversity Loss: 0.636                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.339, BCE loss: 0.339, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.398, BCE loss: 0.398, Diversity Loss: 0.628                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.442, BCE loss: 0.442, Diversity Loss: 0.700                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.656                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.714                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.704                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.345, BCE loss: 0.345, Diversity Loss: 0.681                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.470, BCE loss: 0.470, Diversity Loss: 0.667                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.714                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.667                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.488, BCE loss: 0.488, Diversity Loss: 0.624                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.393, BCE loss: 0.393, Diversity Loss: 0.686                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.544, BCE loss: 0.544, Diversity Loss: 0.703                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.721                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.463, BCE loss: 0.463, Diversity Loss: 0.763                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.492, BCE loss: 0.492, Diversity Loss: 0.641                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.382, BCE loss: 0.382, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.656                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.438, BCE loss: 0.438, Diversity Loss: 0.630                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.299, BCE loss: 0.299, Diversity Loss: 0.624                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.340, BCE loss: 0.340, Diversity Loss: 0.642                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.329, BCE loss: 0.329, Diversity Loss: 0.680                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.691                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.347, BCE loss: 0.347, Diversity Loss: 0.775                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.283, BCE loss: 0.283, Diversity Loss: 0.653                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.444, BCE loss: 0.444, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.655                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.389, BCE loss: 0.389, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.652                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.393, BCE loss: 0.393, Diversity Loss: 0.677                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.337, BCE loss: 0.337, Diversity Loss: 0.664                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.307, BCE loss: 0.307, Diversity Loss: 0.622                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.710                     (Diversity_weight = 0)
{'accuracy': 0.7424849699398798, 'roc_auc': 0.7980038958170171, 'pr_auc': 0.7521940332849851, 'conicity_mean': 0.6381903, 'conicity_std': 0.08613303}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.740    0.745      0.742      0.742         0.742
precision    0.753    0.732      0.742      0.743         0.743
recall       0.728    0.758      0.742      0.743         0.742
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7980038958170171
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:43:45_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.220, BCE loss: 0.220, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.260, BCE loss: 0.260, Diversity Loss: 0.650                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.227, BCE loss: 0.227, Diversity Loss: 0.613                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.388, BCE loss: 0.388, Diversity Loss: 0.613                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.396, BCE loss: 0.396, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.281, BCE loss: 0.281, Diversity Loss: 0.640                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.689                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.328, BCE loss: 0.328, Diversity Loss: 0.623                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.381, BCE loss: 0.381, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.404, BCE loss: 0.404, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.415, BCE loss: 0.415, Diversity Loss: 0.631                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.364, BCE loss: 0.364, Diversity Loss: 0.686                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.211, BCE loss: 0.211, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.609                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.430, BCE loss: 0.430, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.423, BCE loss: 0.423, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.220, BCE loss: 0.220, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.159, BCE loss: 0.159, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.279, BCE loss: 0.279, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.629                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.119, BCE loss: 0.119, Diversity Loss: 0.741                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.278, BCE loss: 0.278, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.651                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.267, BCE loss: 0.267, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.278, BCE loss: 0.278, Diversity Loss: 0.613                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.627                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.273, BCE loss: 0.273, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.459, BCE loss: 0.459, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.352, BCE loss: 0.352, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.302, BCE loss: 0.302, Diversity Loss: 0.661                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.268, BCE loss: 0.268, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.289, BCE loss: 0.289, Diversity Loss: 0.609                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.630                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.486, BCE loss: 0.486, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.407, BCE loss: 0.407, Diversity Loss: 0.640                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.410, BCE loss: 0.410, Diversity Loss: 0.655                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.217, BCE loss: 0.217, Diversity Loss: 0.667                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.233, BCE loss: 0.233, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.461, BCE loss: 0.461, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.457, BCE loss: 0.457, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.241, BCE loss: 0.241, Diversity Loss: 0.640                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.498, BCE loss: 0.498, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.317, BCE loss: 0.317, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.348, BCE loss: 0.348, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.336, BCE loss: 0.336, Diversity Loss: 0.574                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.596, BCE loss: 0.596, Diversity Loss: 0.689                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.608                     (Diversity_weight = 0)
{'accuracy': 0.7214428857715431, 'roc_auc': 0.7927184368536258, 'pr_auc': 0.749099494109873, 'conicity_mean': 0.5936606, 'conicity_std': 0.090053245}
                0        1  micro avg  macro avg  weighted avg
f1-score     0.72    0.723      0.721      0.721         0.721
precision    0.73    0.713      0.721      0.722         0.722
recall       0.71    0.733      0.721      0.722         0.721
support    503.00  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7927184368536258
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:43:45_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.172, BCE loss: 0.172, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.172, BCE loss: 0.172, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.280, BCE loss: 0.280, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.296, BCE loss: 0.296, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.303, BCE loss: 0.303, Diversity Loss: 0.599                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.238, BCE loss: 0.238, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.684                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.253, BCE loss: 0.253, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.638                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.265, BCE loss: 0.265, Diversity Loss: 0.599                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.289, BCE loss: 0.289, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.303, BCE loss: 0.303, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.293, BCE loss: 0.293, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.245, BCE loss: 0.245, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.327, BCE loss: 0.327, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.370, BCE loss: 0.370, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.612                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.591                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.638                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.235, BCE loss: 0.235, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.358, BCE loss: 0.358, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.155, BCE loss: 0.155, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.299, BCE loss: 0.299, Diversity Loss: 0.585                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.352, BCE loss: 0.352, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.351, BCE loss: 0.351, Diversity Loss: 0.659                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.634                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.609                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.126, BCE loss: 0.126, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.225, BCE loss: 0.225, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.354, BCE loss: 0.354, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.716                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.280, BCE loss: 0.280, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.108, BCE loss: 0.108, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.344, BCE loss: 0.344, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.416, BCE loss: 0.416, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.213, BCE loss: 0.213, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.329, BCE loss: 0.329, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.298, BCE loss: 0.298, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.620                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.218, BCE loss: 0.218, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.388, BCE loss: 0.388, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.363, BCE loss: 0.363, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.126, BCE loss: 0.126, Diversity Loss: 0.555                     (Diversity_weight = 0)
{'accuracy': 0.718436873747495, 'roc_auc': 0.7879470650842421, 'pr_auc': 0.7588524907559016, 'conicity_mean': 0.5748413, 'conicity_std': 0.09028686}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.719    0.718      0.718      0.718         0.718
precision    0.723    0.714      0.718      0.718         0.718
recall       0.716    0.721      0.718      0.718         0.718
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7879470650842421
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:43:45_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.161, BCE loss: 0.161, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.134, BCE loss: 0.134, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.247, BCE loss: 0.247, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.628                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.114, BCE loss: 0.114, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.622                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.217, BCE loss: 0.217, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.191, BCE loss: 0.191, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.224, BCE loss: 0.224, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.624                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.126, BCE loss: 0.126, Diversity Loss: 0.584                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.160, BCE loss: 0.160, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.150, BCE loss: 0.150, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.736                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.219, BCE loss: 0.219, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.636                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.172, BCE loss: 0.172, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.173, BCE loss: 0.173, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.128, BCE loss: 0.128, Diversity Loss: 0.628                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.638                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.320, BCE loss: 0.320, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.591                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.585                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.127, BCE loss: 0.127, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.638                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.126, BCE loss: 0.126, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.586                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.204, BCE loss: 0.204, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.375, BCE loss: 0.375, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.230, BCE loss: 0.230, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.231, BCE loss: 0.231, Diversity Loss: 0.656                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.249, BCE loss: 0.249, Diversity Loss: 0.536                     (Diversity_weight = 0)
{'accuracy': 0.7144288577154309, 'roc_auc': 0.7983854449063198, 'pr_auc': 0.7667500343328473, 'conicity_mean': 0.5539383, 'conicity_std': 0.09300002}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.727    0.700      0.714      0.714         0.714
precision    0.701    0.730      0.714      0.716         0.716
recall       0.755    0.673      0.714      0.714         0.714
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7983854449063198
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:43:45_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.591                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.150, BCE loss: 0.150, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.270, BCE loss: 0.270, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.179, BCE loss: 0.179, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.153, BCE loss: 0.153, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.108, BCE loss: 0.108, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.236, BCE loss: 0.236, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.561                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.161, BCE loss: 0.161, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.235, BCE loss: 0.235, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.263, BCE loss: 0.263, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.200, BCE loss: 0.200, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.127, BCE loss: 0.127, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.198, BCE loss: 0.198, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.102, BCE loss: 0.102, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.153, BCE loss: 0.153, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.267, BCE loss: 0.267, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.493                     (Diversity_weight = 0)
{'accuracy': 0.7334669338677354, 'roc_auc': 0.8011888266361427, 'pr_auc': 0.7727870013803728, 'conicity_mean': 0.53388596, 'conicity_std': 0.089536965}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.723    0.743      0.733      0.733         0.733
precision    0.758    0.712      0.733      0.735         0.735
recall       0.692    0.776      0.733      0.734         0.733
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.8011888266361427
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:43:45_2021
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:44:41,126 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:44:41,126 - type = vanillalstm
INFO - 2021-01-17 19:44:41,127 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:44:41,127 - vocab_size = 604
INFO - 2021-01-17 19:44:41,127 - embed_size = 200
INFO - 2021-01-17 19:44:41,127 - hidden_size = 128
INFO - 2021-01-17 19:44:41,127 - pre_embed = None
INFO - 2021-01-17 19:44:41,143 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:44:41,143 - hidden_size = 256
INFO - 2021-01-17 19:44:41,143 - output_size = 1
INFO - 2021-01-17 19:44:41,143 - use_attention = True
INFO - 2021-01-17 19:44:41,143 - regularizer_attention = None
INFO - 2021-01-17 19:44:41,144 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b18857f3890> and extras set()
INFO - 2021-01-17 19:44:41,144 - attention.type = tanh
INFO - 2021-01-17 19:44:41,144 - type = tanh
INFO - 2021-01-17 19:44:41,144 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b18857f3890> and extras set()
INFO - 2021-01-17 19:44:41,144 - attention.hidden_size = 256
INFO - 2021-01-17 19:44:41,144 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7384769539078156, 'roc_auc': 0.81118541277587, 'pr_auc': 0.793943270201168, 'conicity_mean': '0.52903193', 'conicity_std': '0.08851954'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.731    0.745      0.738      0.738         0.738
precision    0.746    0.732      0.738      0.739         0.739
recall       0.717    0.759      0.738      0.738         0.738
support    495.000  503.000    998.000    998.000       998.000
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:44:42,491 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:44:42,491 - type = vanillalstm
INFO - 2021-01-17 19:44:42,491 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:44:42,492 - vocab_size = 604
INFO - 2021-01-17 19:44:42,492 - embed_size = 200
INFO - 2021-01-17 19:44:42,492 - hidden_size = 128
INFO - 2021-01-17 19:44:42,492 - pre_embed = None
INFO - 2021-01-17 19:44:42,506 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:44:42,506 - hidden_size = 256
INFO - 2021-01-17 19:44:42,506 - output_size = 1
INFO - 2021-01-17 19:44:42,506 - use_attention = True
INFO - 2021-01-17 19:44:42,506 - regularizer_attention = None
INFO - 2021-01-17 19:44:42,506 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b187858b190> and extras set()
INFO - 2021-01-17 19:44:42,507 - attention.type = tanh
INFO - 2021-01-17 19:44:42,507 - type = tanh
INFO - 2021-01-17 19:44:42,507 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b187858b190> and extras set()
INFO - 2021-01-17 19:44:42,507 - attention.hidden_size = 256
INFO - 2021-01-17 19:44:42,507 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7384769539078156, 'roc_auc': 0.81118541277587, 'pr_auc': 0.793943270201168, 'conicity_mean': '0.52903193', 'conicity_std': '0.08851954'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.731    0.745      0.738      0.738         0.738
precision    0.746    0.732      0.738      0.739         0.739
recall       0.717    0.759      0.738      0.738         0.738
support    495.000  503.000    998.000    998.000       998.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_fr ortho_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:44:52,892 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:44:52,892 - type = ortholstm
INFO - 2021-01-17 19:44:52,892 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:44:52,911 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:44:53,150 - vocab_size = 604
INFO - 2021-01-17 19:44:53,150 - embed_size = 200
INFO - 2021-01-17 19:44:53,150 - hidden_size = 128
INFO - 2021-01-17 19:44:53,150 - pre_embed = None
INFO - 2021-01-17 19:44:56,211 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:44:56,212 - hidden_size = 256
INFO - 2021-01-17 19:44:56,212 - output_size = 1
INFO - 2021-01-17 19:44:56,212 - use_attention = True
INFO - 2021-01-17 19:44:56,212 - regularizer_attention = None
INFO - 2021-01-17 19:44:56,213 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ab34d7af050> and extras set()
INFO - 2021-01-17 19:44:56,213 - attention.type = tanh
INFO - 2021-01-17 19:44:56,213 - type = tanh
INFO - 2021-01-17 19:44:56,213 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ab34d7af050> and extras set()
INFO - 2021-01-17 19:44:56,213 - attention.hidden_size = 256
INFO - 2021-01-17 19:44:56,213 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.118                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.119                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.117                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.714, BCE loss: 0.714, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.654, BCE loss: 0.654, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.744, BCE loss: 0.744, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.105                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.703, BCE loss: 0.703, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.706, BCE loss: 0.706, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.116                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.260                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.673, BCE loss: 0.673, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.669, BCE loss: 0.669, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.681, BCE loss: 0.681, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.105                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.703, BCE loss: 0.703, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.648, BCE loss: 0.648, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.104                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.644, BCE loss: 0.644, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.150                     (Diversity_weight = 0)
{'accuracy': 0.6282565130260521, 'roc_auc': 0.6705986304395848, 'pr_auc': 0.6433129888007127, 'conicity_mean': 0.15900096, 'conicity_std': 0.03928325}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.644    0.612      0.628      0.628         0.628
precision    0.623    0.635      0.628      0.629         0.629
recall       0.666    0.590      0.628      0.628         0.628
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.6705986304395848
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:44:56_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.605, BCE loss: 0.605, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.103                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.665, BCE loss: 0.665, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.594, BCE loss: 0.594, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.587, BCE loss: 0.587, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.619, BCE loss: 0.619, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.658, BCE loss: 0.658, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.539, BCE loss: 0.539, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.104                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.542, BCE loss: 0.542, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.584, BCE loss: 0.584, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.596, BCE loss: 0.596, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.730, BCE loss: 0.730, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.496, BCE loss: 0.496, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.627, BCE loss: 0.627, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.807, BCE loss: 0.807, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.627, BCE loss: 0.627, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.533, BCE loss: 0.533, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.486, BCE loss: 0.486, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.570, BCE loss: 0.570, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.711, BCE loss: 0.711, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.604, BCE loss: 0.604, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.591, BCE loss: 0.591, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.516, BCE loss: 0.516, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.548, BCE loss: 0.548, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.521, BCE loss: 0.521, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.473, BCE loss: 0.473, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.479, BCE loss: 0.479, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.645, BCE loss: 0.645, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.105                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.596, BCE loss: 0.596, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.578, BCE loss: 0.578, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.536, BCE loss: 0.536, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.605, BCE loss: 0.605, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.521, BCE loss: 0.521, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.637, BCE loss: 0.637, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.609, BCE loss: 0.609, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.619, BCE loss: 0.619, Diversity Loss: 0.240                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.268                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.466, BCE loss: 0.466, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.551, BCE loss: 0.551, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.580, BCE loss: 0.580, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.583, BCE loss: 0.583, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.599, BCE loss: 0.599, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.495, BCE loss: 0.495, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.557, BCE loss: 0.557, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.604, BCE loss: 0.604, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.119                     (Diversity_weight = 0)
{'accuracy': 0.6783567134268537, 'roc_auc': 0.7659658212342109, 'pr_auc': 0.7104664877522247, 'conicity_mean': 0.16135496, 'conicity_std': 0.040788103}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.590    0.735      0.678      0.663         0.662
precision    0.825    0.621      0.678      0.723         0.724
recall       0.459    0.901      0.678      0.680         0.678
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7659658212342109
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:44:56_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.550, BCE loss: 0.550, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.520, BCE loss: 0.520, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.537, BCE loss: 0.537, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.530, BCE loss: 0.530, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.460, BCE loss: 0.460, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.474, BCE loss: 0.474, Diversity Loss: 0.090                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.509, BCE loss: 0.509, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.547, BCE loss: 0.547, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.477, BCE loss: 0.477, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.537, BCE loss: 0.537, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.355, BCE loss: 0.355, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.447, BCE loss: 0.447, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.381, BCE loss: 0.381, Diversity Loss: 0.270                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.361, BCE loss: 0.361, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.442, BCE loss: 0.442, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.550, BCE loss: 0.550, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.336, BCE loss: 0.336, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.357, BCE loss: 0.357, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.398, BCE loss: 0.398, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.450, BCE loss: 0.450, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.488, BCE loss: 0.488, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.481, BCE loss: 0.481, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.582, BCE loss: 0.582, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.234                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.666, BCE loss: 0.666, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.123                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.523, BCE loss: 0.523, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.413, BCE loss: 0.413, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.274, BCE loss: 0.274, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.544, BCE loss: 0.544, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.496, BCE loss: 0.496, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.520, BCE loss: 0.520, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.453, BCE loss: 0.453, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.242                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.604, BCE loss: 0.604, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.404, BCE loss: 0.404, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.411, BCE loss: 0.411, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.457, BCE loss: 0.457, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.510, BCE loss: 0.510, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.382, BCE loss: 0.382, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.416, BCE loss: 0.416, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.412, BCE loss: 0.412, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.494, BCE loss: 0.494, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.426, BCE loss: 0.426, Diversity Loss: 0.146                     (Diversity_weight = 0)
{'accuracy': 0.7244488977955912, 'roc_auc': 0.7909271642870054, 'pr_auc': 0.7421528413452474, 'conicity_mean': 0.16673419, 'conicity_std': 0.04239344}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.717    0.732      0.724      0.724         0.724
precision    0.744    0.708      0.724      0.726         0.726
recall       0.692    0.758      0.724      0.725         0.724
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7909271642870054
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:44:56_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.394, BCE loss: 0.394, Diversity Loss: 0.103                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.377, BCE loss: 0.377, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.422, BCE loss: 0.422, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.435, BCE loss: 0.435, Diversity Loss: 0.272                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.345, BCE loss: 0.345, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.353, BCE loss: 0.353, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.401, BCE loss: 0.401, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.426, BCE loss: 0.426, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.295, BCE loss: 0.295, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.313, BCE loss: 0.313, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.452, BCE loss: 0.452, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.110                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.370, BCE loss: 0.370, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.470, BCE loss: 0.470, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.402, BCE loss: 0.402, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.369, BCE loss: 0.369, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.341, BCE loss: 0.341, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.280, BCE loss: 0.280, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.317, BCE loss: 0.317, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.409, BCE loss: 0.409, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.347, BCE loss: 0.347, Diversity Loss: 0.243                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.242, BCE loss: 0.242, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.232, BCE loss: 0.232, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.263, BCE loss: 0.263, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.369, BCE loss: 0.369, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.359, BCE loss: 0.359, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.224, BCE loss: 0.224, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.345, BCE loss: 0.345, Diversity Loss: 0.092                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.340, BCE loss: 0.340, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.255, BCE loss: 0.255, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.314, BCE loss: 0.314, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.165, BCE loss: 0.165, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.234, BCE loss: 0.234, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.268, BCE loss: 0.268, Diversity Loss: 0.120                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.278, BCE loss: 0.278, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.220, BCE loss: 0.220, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.240, BCE loss: 0.240, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.285, BCE loss: 0.285, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.298, BCE loss: 0.298, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.575, BCE loss: 0.575, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.241, BCE loss: 0.241, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.357, BCE loss: 0.357, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.404, BCE loss: 0.404, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.329, BCE loss: 0.329, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.284, BCE loss: 0.284, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.397, BCE loss: 0.397, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.198, BCE loss: 0.198, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.325, BCE loss: 0.325, Diversity Loss: 0.159                     (Diversity_weight = 0)
{'accuracy': 0.718436873747495, 'roc_auc': 0.7803923931160512, 'pr_auc': 0.7368733408141983, 'conicity_mean': 0.16589762, 'conicity_std': 0.04104885}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.722    0.715      0.718      0.718         0.718
precision    0.719    0.717      0.718      0.718         0.718
recall       0.724    0.713      0.718      0.718         0.718
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7803923931160512
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:44:56_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.325, BCE loss: 0.325, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.219, BCE loss: 0.219, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.195, BCE loss: 0.195, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.370, BCE loss: 0.370, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.285, BCE loss: 0.285, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.133, BCE loss: 0.133, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.164, BCE loss: 0.164, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.371, BCE loss: 0.371, Diversity Loss: 0.090                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.339, BCE loss: 0.339, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.285, BCE loss: 0.285, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.188, BCE loss: 0.188, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.235, BCE loss: 0.235, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.266                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.250, BCE loss: 0.250, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.198, BCE loss: 0.198, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.281, BCE loss: 0.281, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.182, BCE loss: 0.182, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.208, BCE loss: 0.208, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.358, BCE loss: 0.358, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.332, BCE loss: 0.332, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.381, BCE loss: 0.381, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.120                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.348, BCE loss: 0.348, Diversity Loss: 0.104                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.213, BCE loss: 0.213, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.238, BCE loss: 0.238, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.353, BCE loss: 0.353, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.294, BCE loss: 0.294, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.415, BCE loss: 0.415, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.283, BCE loss: 0.283, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.426, BCE loss: 0.426, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.155, BCE loss: 0.155, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.318, BCE loss: 0.318, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.248, BCE loss: 0.248, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.371, BCE loss: 0.371, Diversity Loss: 0.246                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.128                     (Diversity_weight = 0)
{'accuracy': 0.7114228456913828, 'roc_auc': 0.7926702411791874, 'pr_auc': 0.7488338356989553, 'conicity_mean': 0.1687535, 'conicity_std': 0.041637298}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.708    0.715      0.711      0.711         0.711
precision    0.723    0.701      0.711      0.712         0.712
recall       0.694    0.729      0.711      0.712         0.711
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7926702411791874
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:44:56_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.167, BCE loss: 0.167, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.108, BCE loss: 0.108, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.120                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.271                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.223, BCE loss: 0.223, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.275, BCE loss: 0.275, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.238                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.173, BCE loss: 0.173, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.220, BCE loss: 0.220, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.138, BCE loss: 0.138, Diversity Loss: 0.239                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.273, BCE loss: 0.273, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.216, BCE loss: 0.216, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.209, BCE loss: 0.209, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.093                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.285, BCE loss: 0.285, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.109, BCE loss: 0.109, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.113                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.109                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.167, BCE loss: 0.167, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.151                     (Diversity_weight = 0)
{'accuracy': 0.7224448897795591, 'roc_auc': 0.7983854449063197, 'pr_auc': 0.7507019927682526, 'conicity_mean': 0.17105317, 'conicity_std': 0.041206766}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.725    0.720      0.722      0.722         0.722
precision    0.724    0.721      0.722      0.722         0.722
recall       0.726    0.719      0.722      0.722         0.722
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7983854449063197
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:44:56_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.155, BCE loss: 0.155, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.213, BCE loss: 0.213, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.123                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.130, BCE loss: 0.130, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.118, BCE loss: 0.118, Diversity Loss: 0.266                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.138, BCE loss: 0.138, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.248, BCE loss: 0.248, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.206, BCE loss: 0.206, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.109                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.090                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.126, BCE loss: 0.126, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.094, BCE loss: 0.094, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.236                     (Diversity_weight = 0)
{'accuracy': 0.7164328657314629, 'roc_auc': 0.7832640520513284, 'pr_auc': 0.7271958270238801, 'conicity_mean': 0.16714276, 'conicity_std': 0.04060323}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.724    0.709      0.716      0.716         0.716
precision    0.711    0.723      0.716      0.717         0.717
recall       0.738    0.695      0.716      0.716         0.716
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7832640520513284
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:44:56_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.119                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.263                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.107                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.006, BCE loss: 0.006, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.164, BCE loss: 0.164, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.208, BCE loss: 0.208, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.150, BCE loss: 0.150, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.098, BCE loss: 0.098, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.094                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.351, BCE loss: 0.351, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.050, BCE loss: 0.050, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.162                     (Diversity_weight = 0)
{'accuracy': 0.7294589178356713, 'roc_auc': 0.7740084744060888, 'pr_auc': 0.7153376688901298, 'conicity_mean': 0.16979977, 'conicity_std': 0.04071747}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.690    0.760      0.729      0.725         0.725
precision    0.816    0.679      0.729      0.747         0.748
recall       0.598    0.863      0.729      0.731         0.729
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7740084744060888
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:44:56_2021
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:85: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:86: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:46:28,498 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:46:28,499 - type = ortholstm
INFO - 2021-01-17 19:46:28,499 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:46:28,499 - vocab_size = 604
INFO - 2021-01-17 19:46:28,499 - embed_size = 200
INFO - 2021-01-17 19:46:28,499 - hidden_size = 128
INFO - 2021-01-17 19:46:28,499 - pre_embed = None
INFO - 2021-01-17 19:46:28,514 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:46:28,514 - hidden_size = 256
INFO - 2021-01-17 19:46:28,514 - output_size = 1
INFO - 2021-01-17 19:46:28,514 - use_attention = True
INFO - 2021-01-17 19:46:28,514 - regularizer_attention = None
INFO - 2021-01-17 19:46:28,514 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ab34d7f0850> and extras set()
INFO - 2021-01-17 19:46:28,514 - attention.type = tanh
INFO - 2021-01-17 19:46:28,514 - type = tanh
INFO - 2021-01-17 19:46:28,515 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ab34d7f0850> and extras set()
INFO - 2021-01-17 19:46:28,515 - attention.hidden_size = 256
INFO - 2021-01-17 19:46:28,515 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7214428857715431, 'roc_auc': 0.7849910637186979, 'pr_auc': 0.747050131731303, 'conicity_mean': '0.17274506', 'conicity_std': '0.039313763'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.721    0.721      0.721      0.721         0.721
precision    0.716    0.727      0.721      0.721         0.722
recall       0.727    0.716      0.721      0.721         0.721
support    495.000  503.000    998.000    998.000       998.000
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:46:30,606 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:46:30,606 - type = ortholstm
INFO - 2021-01-17 19:46:30,606 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:46:30,606 - vocab_size = 604
INFO - 2021-01-17 19:46:30,606 - embed_size = 200
INFO - 2021-01-17 19:46:30,606 - hidden_size = 128
INFO - 2021-01-17 19:46:30,606 - pre_embed = None
INFO - 2021-01-17 19:46:30,620 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:46:30,620 - hidden_size = 256
INFO - 2021-01-17 19:46:30,620 - output_size = 1
INFO - 2021-01-17 19:46:30,620 - use_attention = True
INFO - 2021-01-17 19:46:30,620 - regularizer_attention = None
INFO - 2021-01-17 19:46:30,620 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ab34b93a350> and extras set()
INFO - 2021-01-17 19:46:30,621 - attention.type = tanh
INFO - 2021-01-17 19:46:30,621 - type = tanh
INFO - 2021-01-17 19:46:30,621 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ab34b93a350> and extras set()
INFO - 2021-01-17 19:46:30,621 - attention.hidden_size = 256
INFO - 2021-01-17 19:46:30,621 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7214428857715431, 'roc_auc': 0.7849910637186979, 'pr_auc': 0.747050131731303, 'conicity_mean': '0.17274506', 'conicity_std': '0.039313763'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.721    0.721      0.721      0.721         0.721
precision    0.716    0.727      0.721      0.721         0.722
recall       0.727    0.716      0.721      0.721         0.721
support    495.000  503.000    998.000    998.000       998.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_fr diversity_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:46:43,684 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:46:43,684 - type = vanillalstm
INFO - 2021-01-17 19:46:43,684 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:46:43,703 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:46:43,944 - vocab_size = 604
INFO - 2021-01-17 19:46:43,944 - embed_size = 200
INFO - 2021-01-17 19:46:43,944 - hidden_size = 128
INFO - 2021-01-17 19:46:43,944 - pre_embed = None
INFO - 2021-01-17 19:46:47,012 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:46:47,013 - hidden_size = 256
INFO - 2021-01-17 19:46:47,013 - output_size = 1
INFO - 2021-01-17 19:46:47,013 - use_attention = True
INFO - 2021-01-17 19:46:47,013 - regularizer_attention = None
INFO - 2021-01-17 19:46:47,013 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ba15b6eb290> and extras set()
INFO - 2021-01-17 19:46:47,014 - attention.type = tanh
INFO - 2021-01-17 19:46:47,014 - type = tanh
INFO - 2021-01-17 19:46:47,014 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ba15b6eb290> and extras set()
INFO - 2021-01-17 19:46:47,014 - attention.hidden_size = 256
INFO - 2021-01-17 19:46:47,014 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.953, BCE loss: 0.693, Diversity Loss: 0.521                     (Diversity_weight = 0.5)
Epoch: 0 Step: 1 Total Loss: 0.930, BCE loss: 0.691, Diversity Loss: 0.478                     (Diversity_weight = 0.5)
Epoch: 0 Step: 2 Total Loss: 0.888, BCE loss: 0.689, Diversity Loss: 0.399                     (Diversity_weight = 0.5)
Epoch: 0 Step: 3 Total Loss: 0.884, BCE loss: 0.690, Diversity Loss: 0.388                     (Diversity_weight = 0.5)
Epoch: 0 Step: 4 Total Loss: 0.838, BCE loss: 0.691, Diversity Loss: 0.293                     (Diversity_weight = 0.5)
Epoch: 0 Step: 5 Total Loss: 0.831, BCE loss: 0.692, Diversity Loss: 0.277                     (Diversity_weight = 0.5)
Epoch: 0 Step: 6 Total Loss: 0.816, BCE loss: 0.694, Diversity Loss: 0.243                     (Diversity_weight = 0.5)
Epoch: 0 Step: 7 Total Loss: 0.840, BCE loss: 0.694, Diversity Loss: 0.292                     (Diversity_weight = 0.5)
Epoch: 0 Step: 8 Total Loss: 0.814, BCE loss: 0.691, Diversity Loss: 0.245                     (Diversity_weight = 0.5)
Epoch: 0 Step: 9 Total Loss: 0.801, BCE loss: 0.691, Diversity Loss: 0.222                     (Diversity_weight = 0.5)
Epoch: 0 Step: 10 Total Loss: 0.804, BCE loss: 0.687, Diversity Loss: 0.235                     (Diversity_weight = 0.5)
Epoch: 0 Step: 11 Total Loss: 0.870, BCE loss: 0.686, Diversity Loss: 0.370                     (Diversity_weight = 0.5)
Epoch: 0 Step: 12 Total Loss: 0.832, BCE loss: 0.686, Diversity Loss: 0.291                     (Diversity_weight = 0.5)
Epoch: 0 Step: 13 Total Loss: 0.816, BCE loss: 0.693, Diversity Loss: 0.247                     (Diversity_weight = 0.5)
Epoch: 0 Step: 14 Total Loss: 0.826, BCE loss: 0.689, Diversity Loss: 0.275                     (Diversity_weight = 0.5)
Epoch: 0 Step: 15 Total Loss: 0.810, BCE loss: 0.691, Diversity Loss: 0.238                     (Diversity_weight = 0.5)
Epoch: 0 Step: 16 Total Loss: 0.903, BCE loss: 0.685, Diversity Loss: 0.436                     (Diversity_weight = 0.5)
Epoch: 0 Step: 17 Total Loss: 0.792, BCE loss: 0.685, Diversity Loss: 0.213                     (Diversity_weight = 0.5)
Epoch: 0 Step: 18 Total Loss: 0.783, BCE loss: 0.691, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 0 Step: 19 Total Loss: 0.784, BCE loss: 0.691, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 0 Step: 20 Total Loss: 0.797, BCE loss: 0.687, Diversity Loss: 0.219                     (Diversity_weight = 0.5)
Epoch: 0 Step: 21 Total Loss: 0.790, BCE loss: 0.690, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 0 Step: 22 Total Loss: 0.796, BCE loss: 0.687, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 0 Step: 23 Total Loss: 0.778, BCE loss: 0.695, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 0 Step: 24 Total Loss: 0.800, BCE loss: 0.686, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 0 Step: 25 Total Loss: 0.769, BCE loss: 0.686, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 0 Step: 26 Total Loss: 0.790, BCE loss: 0.689, Diversity Loss: 0.202                     (Diversity_weight = 0.5)
Epoch: 0 Step: 27 Total Loss: 0.780, BCE loss: 0.690, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 0 Step: 28 Total Loss: 0.812, BCE loss: 0.686, Diversity Loss: 0.252                     (Diversity_weight = 0.5)
Epoch: 0 Step: 29 Total Loss: 0.775, BCE loss: 0.686, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 0 Step: 30 Total Loss: 0.818, BCE loss: 0.688, Diversity Loss: 0.260                     (Diversity_weight = 0.5)
Epoch: 0 Step: 31 Total Loss: 0.797, BCE loss: 0.689, Diversity Loss: 0.216                     (Diversity_weight = 0.5)
Epoch: 0 Step: 32 Total Loss: 0.778, BCE loss: 0.687, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 0 Step: 33 Total Loss: 0.751, BCE loss: 0.678, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 0 Step: 34 Total Loss: 0.773, BCE loss: 0.681, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 0 Step: 35 Total Loss: 0.773, BCE loss: 0.683, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 0 Step: 36 Total Loss: 0.752, BCE loss: 0.685, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 0 Step: 37 Total Loss: 0.816, BCE loss: 0.695, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 0 Step: 38 Total Loss: 0.756, BCE loss: 0.692, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 0 Step: 39 Total Loss: 0.763, BCE loss: 0.683, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 0 Step: 40 Total Loss: 0.783, BCE loss: 0.680, Diversity Loss: 0.206                     (Diversity_weight = 0.5)
Epoch: 0 Step: 41 Total Loss: 0.754, BCE loss: 0.685, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 0 Step: 42 Total Loss: 0.759, BCE loss: 0.688, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 0 Step: 43 Total Loss: 0.789, BCE loss: 0.668, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 0 Step: 44 Total Loss: 0.797, BCE loss: 0.662, Diversity Loss: 0.269                     (Diversity_weight = 0.5)
Epoch: 0 Step: 45 Total Loss: 0.759, BCE loss: 0.690, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 0 Step: 46 Total Loss: 0.786, BCE loss: 0.690, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 0 Step: 47 Total Loss: 0.756, BCE loss: 0.663, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 0 Step: 48 Total Loss: 0.764, BCE loss: 0.676, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 0 Step: 49 Total Loss: 0.748, BCE loss: 0.680, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 0 Step: 50 Total Loss: 0.765, BCE loss: 0.692, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 0 Step: 51 Total Loss: 0.783, BCE loss: 0.679, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 0 Step: 52 Total Loss: 0.731, BCE loss: 0.640, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 0 Step: 53 Total Loss: 0.762, BCE loss: 0.688, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 0 Step: 54 Total Loss: 0.753, BCE loss: 0.684, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 0 Step: 55 Total Loss: 0.771, BCE loss: 0.643, Diversity Loss: 0.256                     (Diversity_weight = 0.5)
Epoch: 0 Step: 56 Total Loss: 0.765, BCE loss: 0.668, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 0 Step: 57 Total Loss: 0.734, BCE loss: 0.673, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 0 Step: 58 Total Loss: 0.780, BCE loss: 0.666, Diversity Loss: 0.229                     (Diversity_weight = 0.5)
Epoch: 0 Step: 59 Total Loss: 0.774, BCE loss: 0.623, Diversity Loss: 0.301                     (Diversity_weight = 0.5)
Epoch: 0 Step: 60 Total Loss: 0.725, BCE loss: 0.647, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 0 Step: 61 Total Loss: 0.774, BCE loss: 0.689, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 0 Step: 62 Total Loss: 0.753, BCE loss: 0.675, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
{'accuracy': 0.6482965931863728, 'roc_auc': 0.7010341988473201, 'pr_auc': 0.6701627605367281, 'conicity_mean': 0.16712941, 'conicity_std': 0.05008565}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.636    0.660      0.648      0.648         0.648
precision    0.665    0.634      0.648      0.650         0.650
recall       0.608    0.689      0.648      0.649         0.648
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7010341988473201
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:46:47_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.694, BCE loss: 0.623, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 1 Step: 1 Total Loss: 0.710, BCE loss: 0.655, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 1 Step: 2 Total Loss: 0.673, BCE loss: 0.592, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 1 Step: 3 Total Loss: 0.767, BCE loss: 0.663, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 1 Step: 4 Total Loss: 0.714, BCE loss: 0.632, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 1 Step: 5 Total Loss: 0.729, BCE loss: 0.603, Diversity Loss: 0.252                     (Diversity_weight = 0.5)
Epoch: 1 Step: 6 Total Loss: 0.693, BCE loss: 0.632, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 1 Step: 7 Total Loss: 0.724, BCE loss: 0.633, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 1 Step: 8 Total Loss: 0.732, BCE loss: 0.657, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 1 Step: 9 Total Loss: 0.717, BCE loss: 0.636, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 1 Step: 10 Total Loss: 0.642, BCE loss: 0.569, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 1 Step: 11 Total Loss: 0.877, BCE loss: 0.771, Diversity Loss: 0.211                     (Diversity_weight = 0.5)
Epoch: 1 Step: 12 Total Loss: 0.728, BCE loss: 0.644, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 1 Step: 13 Total Loss: 0.722, BCE loss: 0.630, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 1 Step: 14 Total Loss: 0.686, BCE loss: 0.568, Diversity Loss: 0.235                     (Diversity_weight = 0.5)
Epoch: 1 Step: 15 Total Loss: 0.789, BCE loss: 0.650, Diversity Loss: 0.279                     (Diversity_weight = 0.5)
Epoch: 1 Step: 16 Total Loss: 0.768, BCE loss: 0.695, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 1 Step: 17 Total Loss: 0.718, BCE loss: 0.647, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 1 Step: 18 Total Loss: 0.660, BCE loss: 0.591, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 1 Step: 19 Total Loss: 0.755, BCE loss: 0.659, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 1 Step: 20 Total Loss: 0.691, BCE loss: 0.617, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 1 Step: 21 Total Loss: 0.767, BCE loss: 0.705, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 1 Step: 22 Total Loss: 0.677, BCE loss: 0.599, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 1 Step: 23 Total Loss: 0.709, BCE loss: 0.648, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 1 Step: 24 Total Loss: 0.735, BCE loss: 0.638, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 1 Step: 25 Total Loss: 0.690, BCE loss: 0.630, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 1 Step: 26 Total Loss: 0.777, BCE loss: 0.635, Diversity Loss: 0.284                     (Diversity_weight = 0.5)
Epoch: 1 Step: 27 Total Loss: 0.722, BCE loss: 0.661, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 28 Total Loss: 0.653, BCE loss: 0.593, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 1 Step: 29 Total Loss: 0.678, BCE loss: 0.624, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 1 Step: 30 Total Loss: 0.717, BCE loss: 0.651, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 1 Step: 31 Total Loss: 0.785, BCE loss: 0.684, Diversity Loss: 0.202                     (Diversity_weight = 0.5)
Epoch: 1 Step: 32 Total Loss: 0.736, BCE loss: 0.683, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 1 Step: 33 Total Loss: 0.732, BCE loss: 0.637, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 1 Step: 34 Total Loss: 0.759, BCE loss: 0.704, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 1 Step: 35 Total Loss: 0.729, BCE loss: 0.668, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 1 Step: 36 Total Loss: 0.706, BCE loss: 0.637, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 1 Step: 37 Total Loss: 0.719, BCE loss: 0.622, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 1 Step: 38 Total Loss: 0.678, BCE loss: 0.567, Diversity Loss: 0.222                     (Diversity_weight = 0.5)
Epoch: 1 Step: 39 Total Loss: 0.633, BCE loss: 0.555, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 1 Step: 40 Total Loss: 0.688, BCE loss: 0.614, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 1 Step: 41 Total Loss: 0.693, BCE loss: 0.621, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 1 Step: 42 Total Loss: 0.659, BCE loss: 0.594, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 1 Step: 43 Total Loss: 0.737, BCE loss: 0.646, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 1 Step: 44 Total Loss: 0.644, BCE loss: 0.586, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 1 Step: 45 Total Loss: 0.653, BCE loss: 0.584, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 1 Step: 46 Total Loss: 0.735, BCE loss: 0.669, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 1 Step: 47 Total Loss: 0.765, BCE loss: 0.681, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 1 Step: 48 Total Loss: 0.648, BCE loss: 0.586, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 49 Total Loss: 0.623, BCE loss: 0.561, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 50 Total Loss: 0.740, BCE loss: 0.654, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 1 Step: 51 Total Loss: 0.756, BCE loss: 0.665, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 1 Step: 52 Total Loss: 0.696, BCE loss: 0.572, Diversity Loss: 0.249                     (Diversity_weight = 0.5)
Epoch: 1 Step: 53 Total Loss: 0.816, BCE loss: 0.727, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 1 Step: 54 Total Loss: 0.607, BCE loss: 0.529, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 1 Step: 55 Total Loss: 0.620, BCE loss: 0.565, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 1 Step: 56 Total Loss: 0.549, BCE loss: 0.485, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 1 Step: 57 Total Loss: 0.686, BCE loss: 0.605, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 1 Step: 58 Total Loss: 0.726, BCE loss: 0.642, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 1 Step: 59 Total Loss: 0.741, BCE loss: 0.662, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 1 Step: 60 Total Loss: 0.760, BCE loss: 0.705, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 1 Step: 61 Total Loss: 0.601, BCE loss: 0.516, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 1 Step: 62 Total Loss: 0.656, BCE loss: 0.593, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
{'accuracy': 0.6903807615230461, 'roc_auc': 0.7252806393959476, 'pr_auc': 0.6711805023620286, 'conicity_mean': 0.15265101, 'conicity_std': 0.048848845}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.664    0.713       0.69      0.688         0.688
precision    0.733    0.660       0.69      0.696         0.697
recall       0.606    0.776       0.69      0.691         0.690
support    503.000  495.000     998.00    998.000       998.000
Model Saved on  roc_auc 0.7252806393959476
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:46:47_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.615, BCE loss: 0.542, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 2 Step: 1 Total Loss: 0.604, BCE loss: 0.525, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 2 Step: 2 Total Loss: 0.635, BCE loss: 0.579, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 2 Step: 3 Total Loss: 0.671, BCE loss: 0.563, Diversity Loss: 0.217                     (Diversity_weight = 0.5)
Epoch: 2 Step: 4 Total Loss: 0.616, BCE loss: 0.505, Diversity Loss: 0.222                     (Diversity_weight = 0.5)
Epoch: 2 Step: 5 Total Loss: 0.481, BCE loss: 0.402, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 2 Step: 6 Total Loss: 0.637, BCE loss: 0.553, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 2 Step: 7 Total Loss: 0.522, BCE loss: 0.459, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 2 Step: 8 Total Loss: 0.633, BCE loss: 0.561, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 2 Step: 9 Total Loss: 0.625, BCE loss: 0.569, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 2 Step: 10 Total Loss: 0.682, BCE loss: 0.626, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 2 Step: 11 Total Loss: 0.519, BCE loss: 0.436, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 2 Step: 12 Total Loss: 0.593, BCE loss: 0.507, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 2 Step: 13 Total Loss: 0.557, BCE loss: 0.497, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 2 Step: 14 Total Loss: 0.648, BCE loss: 0.508, Diversity Loss: 0.280                     (Diversity_weight = 0.5)
Epoch: 2 Step: 15 Total Loss: 0.557, BCE loss: 0.493, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 2 Step: 16 Total Loss: 0.666, BCE loss: 0.596, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 2 Step: 17 Total Loss: 0.508, BCE loss: 0.443, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 2 Step: 18 Total Loss: 0.769, BCE loss: 0.672, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 2 Step: 19 Total Loss: 0.575, BCE loss: 0.488, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 2 Step: 20 Total Loss: 0.514, BCE loss: 0.448, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 2 Step: 21 Total Loss: 0.637, BCE loss: 0.580, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 2 Step: 22 Total Loss: 0.739, BCE loss: 0.615, Diversity Loss: 0.247                     (Diversity_weight = 0.5)
Epoch: 2 Step: 23 Total Loss: 0.604, BCE loss: 0.511, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 2 Step: 24 Total Loss: 0.534, BCE loss: 0.442, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 2 Step: 25 Total Loss: 0.687, BCE loss: 0.594, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 2 Step: 26 Total Loss: 0.516, BCE loss: 0.452, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 2 Step: 27 Total Loss: 0.544, BCE loss: 0.475, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 2 Step: 28 Total Loss: 0.536, BCE loss: 0.475, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 2 Step: 29 Total Loss: 0.754, BCE loss: 0.662, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 2 Step: 30 Total Loss: 0.408, BCE loss: 0.324, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 2 Step: 31 Total Loss: 0.511, BCE loss: 0.457, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 2 Step: 32 Total Loss: 0.579, BCE loss: 0.504, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 2 Step: 33 Total Loss: 0.606, BCE loss: 0.551, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 2 Step: 34 Total Loss: 0.757, BCE loss: 0.665, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 2 Step: 35 Total Loss: 0.717, BCE loss: 0.651, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 2 Step: 36 Total Loss: 0.500, BCE loss: 0.427, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 2 Step: 37 Total Loss: 0.663, BCE loss: 0.576, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 2 Step: 38 Total Loss: 0.493, BCE loss: 0.429, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 2 Step: 39 Total Loss: 0.535, BCE loss: 0.476, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 2 Step: 40 Total Loss: 0.568, BCE loss: 0.502, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 2 Step: 41 Total Loss: 0.565, BCE loss: 0.510, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 2 Step: 42 Total Loss: 0.667, BCE loss: 0.539, Diversity Loss: 0.256                     (Diversity_weight = 0.5)
Epoch: 2 Step: 43 Total Loss: 0.695, BCE loss: 0.589, Diversity Loss: 0.212                     (Diversity_weight = 0.5)
Epoch: 2 Step: 44 Total Loss: 0.544, BCE loss: 0.481, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 2 Step: 45 Total Loss: 0.746, BCE loss: 0.668, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 2 Step: 46 Total Loss: 0.630, BCE loss: 0.559, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 2 Step: 47 Total Loss: 0.475, BCE loss: 0.406, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 2 Step: 48 Total Loss: 0.485, BCE loss: 0.407, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 2 Step: 49 Total Loss: 0.459, BCE loss: 0.383, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 2 Step: 50 Total Loss: 0.809, BCE loss: 0.716, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 2 Step: 51 Total Loss: 0.916, BCE loss: 0.839, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 2 Step: 52 Total Loss: 0.506, BCE loss: 0.445, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 2 Step: 53 Total Loss: 0.647, BCE loss: 0.574, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 2 Step: 54 Total Loss: 0.608, BCE loss: 0.544, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 2 Step: 55 Total Loss: 0.533, BCE loss: 0.480, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 2 Step: 56 Total Loss: 0.453, BCE loss: 0.365, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 2 Step: 57 Total Loss: 0.542, BCE loss: 0.423, Diversity Loss: 0.238                     (Diversity_weight = 0.5)
Epoch: 2 Step: 58 Total Loss: 0.585, BCE loss: 0.520, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 2 Step: 59 Total Loss: 0.668, BCE loss: 0.617, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 2 Step: 60 Total Loss: 0.609, BCE loss: 0.558, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 2 Step: 61 Total Loss: 0.616, BCE loss: 0.544, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 2 Step: 62 Total Loss: 0.516, BCE loss: 0.450, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
{'accuracy': 0.6913827655310621, 'roc_auc': 0.7654276362029842, 'pr_auc': 0.7117173095209977, 'conicity_mean': 0.14593813, 'conicity_std': 0.04739034}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.707    0.674      0.691      0.690         0.691
precision    0.678    0.708      0.691      0.693         0.693
recall       0.740    0.642      0.691      0.691         0.691
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7654276362029842
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:46:47_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.523, BCE loss: 0.471, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 3 Step: 1 Total Loss: 0.509, BCE loss: 0.396, Diversity Loss: 0.226                     (Diversity_weight = 0.5)
Epoch: 3 Step: 2 Total Loss: 0.577, BCE loss: 0.465, Diversity Loss: 0.225                     (Diversity_weight = 0.5)
Epoch: 3 Step: 3 Total Loss: 0.472, BCE loss: 0.411, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 3 Step: 4 Total Loss: 0.518, BCE loss: 0.442, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 3 Step: 5 Total Loss: 0.499, BCE loss: 0.440, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 3 Step: 6 Total Loss: 0.393, BCE loss: 0.326, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 3 Step: 7 Total Loss: 0.467, BCE loss: 0.388, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 3 Step: 8 Total Loss: 0.592, BCE loss: 0.529, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 3 Step: 9 Total Loss: 0.364, BCE loss: 0.286, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 3 Step: 10 Total Loss: 0.466, BCE loss: 0.403, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 3 Step: 11 Total Loss: 0.508, BCE loss: 0.406, Diversity Loss: 0.203                     (Diversity_weight = 0.5)
Epoch: 3 Step: 12 Total Loss: 0.546, BCE loss: 0.462, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 3 Step: 13 Total Loss: 0.519, BCE loss: 0.457, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 3 Step: 14 Total Loss: 0.532, BCE loss: 0.438, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 3 Step: 15 Total Loss: 0.409, BCE loss: 0.324, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 3 Step: 16 Total Loss: 0.492, BCE loss: 0.433, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 3 Step: 17 Total Loss: 0.250, BCE loss: 0.189, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 3 Step: 18 Total Loss: 0.634, BCE loss: 0.515, Diversity Loss: 0.237                     (Diversity_weight = 0.5)
Epoch: 3 Step: 19 Total Loss: 0.451, BCE loss: 0.399, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 3 Step: 20 Total Loss: 0.464, BCE loss: 0.414, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 3 Step: 21 Total Loss: 0.359, BCE loss: 0.275, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 3 Step: 22 Total Loss: 0.687, BCE loss: 0.619, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 3 Step: 23 Total Loss: 0.381, BCE loss: 0.325, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 3 Step: 24 Total Loss: 0.520, BCE loss: 0.432, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 3 Step: 25 Total Loss: 0.401, BCE loss: 0.281, Diversity Loss: 0.239                     (Diversity_weight = 0.5)
Epoch: 3 Step: 26 Total Loss: 0.526, BCE loss: 0.477, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 3 Step: 27 Total Loss: 0.530, BCE loss: 0.460, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 3 Step: 28 Total Loss: 0.501, BCE loss: 0.427, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 3 Step: 29 Total Loss: 0.444, BCE loss: 0.370, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 3 Step: 30 Total Loss: 0.556, BCE loss: 0.504, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 3 Step: 31 Total Loss: 0.313, BCE loss: 0.234, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 3 Step: 32 Total Loss: 0.593, BCE loss: 0.527, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 3 Step: 33 Total Loss: 0.597, BCE loss: 0.499, Diversity Loss: 0.196                     (Diversity_weight = 0.5)
Epoch: 3 Step: 34 Total Loss: 0.397, BCE loss: 0.318, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 3 Step: 35 Total Loss: 0.371, BCE loss: 0.307, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 3 Step: 36 Total Loss: 0.592, BCE loss: 0.509, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 3 Step: 37 Total Loss: 0.568, BCE loss: 0.469, Diversity Loss: 0.197                     (Diversity_weight = 0.5)
Epoch: 3 Step: 38 Total Loss: 0.316, BCE loss: 0.259, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 3 Step: 39 Total Loss: 0.380, BCE loss: 0.328, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 3 Step: 40 Total Loss: 0.445, BCE loss: 0.382, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 3 Step: 41 Total Loss: 0.578, BCE loss: 0.510, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 3 Step: 42 Total Loss: 0.502, BCE loss: 0.443, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 3 Step: 43 Total Loss: 0.469, BCE loss: 0.418, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 3 Step: 44 Total Loss: 0.306, BCE loss: 0.236, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 3 Step: 45 Total Loss: 0.402, BCE loss: 0.322, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 3 Step: 46 Total Loss: 0.376, BCE loss: 0.320, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 3 Step: 47 Total Loss: 0.354, BCE loss: 0.294, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 3 Step: 48 Total Loss: 0.578, BCE loss: 0.439, Diversity Loss: 0.279                     (Diversity_weight = 0.5)
Epoch: 3 Step: 49 Total Loss: 0.662, BCE loss: 0.607, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 3 Step: 50 Total Loss: 0.505, BCE loss: 0.425, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 3 Step: 51 Total Loss: 0.459, BCE loss: 0.385, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 52 Total Loss: 0.450, BCE loss: 0.383, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 3 Step: 53 Total Loss: 0.582, BCE loss: 0.528, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 3 Step: 54 Total Loss: 0.465, BCE loss: 0.395, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 3 Step: 55 Total Loss: 0.644, BCE loss: 0.592, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 3 Step: 56 Total Loss: 0.455, BCE loss: 0.396, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 3 Step: 57 Total Loss: 0.397, BCE loss: 0.322, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 58 Total Loss: 0.715, BCE loss: 0.622, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 3 Step: 59 Total Loss: 0.563, BCE loss: 0.491, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 3 Step: 60 Total Loss: 0.464, BCE loss: 0.376, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 3 Step: 61 Total Loss: 0.592, BCE loss: 0.503, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 3 Step: 62 Total Loss: 0.513, BCE loss: 0.426, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
{'accuracy': 0.7014028056112225, 'roc_auc': 0.7924172138883869, 'pr_auc': 0.7490534823758221, 'conicity_mean': 0.15126379, 'conicity_std': 0.050165657}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.730    0.667      0.701      0.698         0.698
precision    0.671    0.747      0.701      0.709         0.709
recall       0.799    0.602      0.701      0.701         0.701
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7924172138883869
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:46:47_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.342, BCE loss: 0.231, Diversity Loss: 0.222                     (Diversity_weight = 0.5)
Epoch: 4 Step: 1 Total Loss: 0.325, BCE loss: 0.247, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 4 Step: 2 Total Loss: 0.326, BCE loss: 0.271, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 4 Step: 3 Total Loss: 0.462, BCE loss: 0.391, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 4 Step: 4 Total Loss: 0.371, BCE loss: 0.296, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 4 Step: 5 Total Loss: 0.310, BCE loss: 0.239, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 4 Step: 6 Total Loss: 0.421, BCE loss: 0.326, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 4 Step: 7 Total Loss: 0.369, BCE loss: 0.280, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 4 Step: 8 Total Loss: 0.383, BCE loss: 0.305, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 4 Step: 9 Total Loss: 0.464, BCE loss: 0.374, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 4 Step: 10 Total Loss: 0.299, BCE loss: 0.217, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 4 Step: 11 Total Loss: 0.408, BCE loss: 0.332, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 4 Step: 12 Total Loss: 0.250, BCE loss: 0.191, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 4 Step: 13 Total Loss: 0.339, BCE loss: 0.283, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 4 Step: 14 Total Loss: 0.473, BCE loss: 0.413, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 4 Step: 15 Total Loss: 0.314, BCE loss: 0.259, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 4 Step: 16 Total Loss: 0.449, BCE loss: 0.397, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 4 Step: 17 Total Loss: 0.392, BCE loss: 0.329, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 4 Step: 18 Total Loss: 0.315, BCE loss: 0.261, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 4 Step: 19 Total Loss: 0.322, BCE loss: 0.273, Diversity Loss: 0.098                     (Diversity_weight = 0.5)
Epoch: 4 Step: 20 Total Loss: 0.252, BCE loss: 0.188, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 4 Step: 21 Total Loss: 0.536, BCE loss: 0.401, Diversity Loss: 0.269                     (Diversity_weight = 0.5)
Epoch: 4 Step: 22 Total Loss: 0.296, BCE loss: 0.237, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 4 Step: 23 Total Loss: 0.270, BCE loss: 0.203, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 4 Step: 24 Total Loss: 0.211, BCE loss: 0.145, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 4 Step: 25 Total Loss: 0.396, BCE loss: 0.314, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 4 Step: 26 Total Loss: 0.354, BCE loss: 0.278, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 4 Step: 27 Total Loss: 0.458, BCE loss: 0.367, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 4 Step: 28 Total Loss: 0.545, BCE loss: 0.472, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 4 Step: 29 Total Loss: 0.307, BCE loss: 0.233, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 4 Step: 30 Total Loss: 0.468, BCE loss: 0.410, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 4 Step: 31 Total Loss: 0.494, BCE loss: 0.444, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 4 Step: 32 Total Loss: 0.393, BCE loss: 0.315, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 4 Step: 33 Total Loss: 0.433, BCE loss: 0.349, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 4 Step: 34 Total Loss: 0.409, BCE loss: 0.319, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 4 Step: 35 Total Loss: 0.397, BCE loss: 0.300, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 4 Step: 36 Total Loss: 0.493, BCE loss: 0.372, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 4 Step: 37 Total Loss: 0.331, BCE loss: 0.258, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 4 Step: 38 Total Loss: 0.254, BCE loss: 0.206, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 4 Step: 39 Total Loss: 0.253, BCE loss: 0.181, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 4 Step: 40 Total Loss: 0.387, BCE loss: 0.268, Diversity Loss: 0.239                     (Diversity_weight = 0.5)
Epoch: 4 Step: 41 Total Loss: 0.466, BCE loss: 0.394, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 4 Step: 42 Total Loss: 0.312, BCE loss: 0.236, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 4 Step: 43 Total Loss: 0.532, BCE loss: 0.477, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 4 Step: 44 Total Loss: 0.376, BCE loss: 0.317, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 4 Step: 45 Total Loss: 0.513, BCE loss: 0.419, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 4 Step: 46 Total Loss: 0.527, BCE loss: 0.430, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 4 Step: 47 Total Loss: 0.562, BCE loss: 0.495, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 4 Step: 48 Total Loss: 0.364, BCE loss: 0.313, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 4 Step: 49 Total Loss: 0.448, BCE loss: 0.379, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 4 Step: 50 Total Loss: 0.419, BCE loss: 0.368, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 4 Step: 51 Total Loss: 0.263, BCE loss: 0.211, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 4 Step: 52 Total Loss: 0.456, BCE loss: 0.374, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 4 Step: 53 Total Loss: 0.370, BCE loss: 0.295, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 4 Step: 54 Total Loss: 0.395, BCE loss: 0.312, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 4 Step: 55 Total Loss: 0.345, BCE loss: 0.289, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 4 Step: 56 Total Loss: 0.258, BCE loss: 0.196, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 4 Step: 57 Total Loss: 0.576, BCE loss: 0.483, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 4 Step: 58 Total Loss: 0.502, BCE loss: 0.442, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 4 Step: 59 Total Loss: 0.384, BCE loss: 0.325, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 4 Step: 60 Total Loss: 0.391, BCE loss: 0.339, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 4 Step: 61 Total Loss: 0.304, BCE loss: 0.244, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 4 Step: 62 Total Loss: 0.515, BCE loss: 0.395, Diversity Loss: 0.240                     (Diversity_weight = 0.5)
{'accuracy': 0.7314629258517034, 'roc_auc': 0.8148683655641906, 'pr_auc': 0.7974314787736073, 'conicity_mean': 0.14483175, 'conicity_std': 0.044971377}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.745    0.717      0.731      0.731         0.731
precision    0.715    0.752      0.731      0.733         0.733
recall       0.777    0.685      0.731      0.731         0.731
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.8148683655641906
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:46:47_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.245, BCE loss: 0.152, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 5 Step: 1 Total Loss: 0.252, BCE loss: 0.194, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 5 Step: 2 Total Loss: 0.334, BCE loss: 0.248, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 5 Step: 3 Total Loss: 0.185, BCE loss: 0.124, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 5 Step: 4 Total Loss: 0.300, BCE loss: 0.221, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 5 Step: 5 Total Loss: 0.429, BCE loss: 0.337, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 5 Step: 6 Total Loss: 0.219, BCE loss: 0.162, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 5 Step: 7 Total Loss: 0.272, BCE loss: 0.210, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 5 Step: 8 Total Loss: 0.237, BCE loss: 0.162, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 5 Step: 9 Total Loss: 0.176, BCE loss: 0.105, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 5 Step: 10 Total Loss: 0.337, BCE loss: 0.278, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 5 Step: 11 Total Loss: 0.188, BCE loss: 0.132, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 5 Step: 12 Total Loss: 0.284, BCE loss: 0.207, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 5 Step: 13 Total Loss: 0.198, BCE loss: 0.111, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 5 Step: 14 Total Loss: 0.166, BCE loss: 0.085, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 5 Step: 15 Total Loss: 0.164, BCE loss: 0.109, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 5 Step: 16 Total Loss: 0.178, BCE loss: 0.102, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 5 Step: 17 Total Loss: 0.190, BCE loss: 0.137, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 5 Step: 18 Total Loss: 0.175, BCE loss: 0.097, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 5 Step: 19 Total Loss: 0.188, BCE loss: 0.135, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 5 Step: 20 Total Loss: 0.379, BCE loss: 0.307, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 5 Step: 21 Total Loss: 0.285, BCE loss: 0.224, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 5 Step: 22 Total Loss: 0.359, BCE loss: 0.226, Diversity Loss: 0.266                     (Diversity_weight = 0.5)
Epoch: 5 Step: 23 Total Loss: 0.342, BCE loss: 0.281, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 5 Step: 24 Total Loss: 0.208, BCE loss: 0.138, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 5 Step: 25 Total Loss: 0.197, BCE loss: 0.112, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 5 Step: 26 Total Loss: 0.172, BCE loss: 0.096, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 5 Step: 27 Total Loss: 0.309, BCE loss: 0.216, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 5 Step: 28 Total Loss: 0.262, BCE loss: 0.189, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 5 Step: 29 Total Loss: 0.293, BCE loss: 0.240, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 5 Step: 30 Total Loss: 0.411, BCE loss: 0.304, Diversity Loss: 0.214                     (Diversity_weight = 0.5)
Epoch: 5 Step: 31 Total Loss: 0.257, BCE loss: 0.181, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 5 Step: 32 Total Loss: 0.468, BCE loss: 0.404, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 5 Step: 33 Total Loss: 0.392, BCE loss: 0.297, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 5 Step: 34 Total Loss: 0.344, BCE loss: 0.281, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 5 Step: 35 Total Loss: 0.196, BCE loss: 0.115, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 5 Step: 36 Total Loss: 0.221, BCE loss: 0.170, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 5 Step: 37 Total Loss: 0.447, BCE loss: 0.328, Diversity Loss: 0.238                     (Diversity_weight = 0.5)
Epoch: 5 Step: 38 Total Loss: 0.184, BCE loss: 0.110, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 5 Step: 39 Total Loss: 0.231, BCE loss: 0.112, Diversity Loss: 0.239                     (Diversity_weight = 0.5)
Epoch: 5 Step: 40 Total Loss: 0.245, BCE loss: 0.162, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 5 Step: 41 Total Loss: 0.186, BCE loss: 0.132, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 5 Step: 42 Total Loss: 0.301, BCE loss: 0.237, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 5 Step: 43 Total Loss: 0.304, BCE loss: 0.240, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 5 Step: 44 Total Loss: 0.350, BCE loss: 0.265, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 5 Step: 45 Total Loss: 0.226, BCE loss: 0.134, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 5 Step: 46 Total Loss: 0.420, BCE loss: 0.375, Diversity Loss: 0.089                     (Diversity_weight = 0.5)
Epoch: 5 Step: 47 Total Loss: 0.339, BCE loss: 0.280, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 5 Step: 48 Total Loss: 0.277, BCE loss: 0.203, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 5 Step: 49 Total Loss: 0.269, BCE loss: 0.217, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 5 Step: 50 Total Loss: 0.308, BCE loss: 0.236, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 5 Step: 51 Total Loss: 0.515, BCE loss: 0.444, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 5 Step: 52 Total Loss: 0.231, BCE loss: 0.179, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 5 Step: 53 Total Loss: 0.350, BCE loss: 0.243, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 5 Step: 54 Total Loss: 0.278, BCE loss: 0.222, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 5 Step: 55 Total Loss: 0.235, BCE loss: 0.185, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 5 Step: 56 Total Loss: 0.212, BCE loss: 0.149, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 5 Step: 57 Total Loss: 0.429, BCE loss: 0.367, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 5 Step: 58 Total Loss: 0.262, BCE loss: 0.212, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 5 Step: 59 Total Loss: 0.368, BCE loss: 0.303, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 5 Step: 60 Total Loss: 0.366, BCE loss: 0.317, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 5 Step: 61 Total Loss: 0.353, BCE loss: 0.234, Diversity Loss: 0.238                     (Diversity_weight = 0.5)
Epoch: 5 Step: 62 Total Loss: 0.161, BCE loss: 0.086, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
{'accuracy': 0.7294589178356713, 'roc_auc': 0.8061088017350443, 'pr_auc': 0.7717194418829992, 'conicity_mean': 0.14437224, 'conicity_std': 0.046695985}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.737    0.722      0.729      0.729         0.729
precision    0.723    0.737      0.729      0.730         0.730
recall       0.751    0.707      0.729      0.729         0.729
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.8061088017350443
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:46:47_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.248, BCE loss: 0.179, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 6 Step: 1 Total Loss: 0.259, BCE loss: 0.196, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 6 Step: 2 Total Loss: 0.211, BCE loss: 0.162, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 6 Step: 3 Total Loss: 0.088, BCE loss: 0.043, Diversity Loss: 0.090                     (Diversity_weight = 0.5)
Epoch: 6 Step: 4 Total Loss: 0.340, BCE loss: 0.281, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 6 Step: 5 Total Loss: 0.229, BCE loss: 0.150, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 6 Step: 6 Total Loss: 0.193, BCE loss: 0.145, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 6 Step: 7 Total Loss: 0.240, BCE loss: 0.184, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 6 Step: 8 Total Loss: 0.177, BCE loss: 0.119, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 6 Step: 9 Total Loss: 0.258, BCE loss: 0.178, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 6 Step: 10 Total Loss: 0.145, BCE loss: 0.090, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 6 Step: 11 Total Loss: 0.249, BCE loss: 0.174, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 6 Step: 12 Total Loss: 0.232, BCE loss: 0.132, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 6 Step: 13 Total Loss: 0.162, BCE loss: 0.086, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 6 Step: 14 Total Loss: 0.187, BCE loss: 0.122, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 6 Step: 15 Total Loss: 0.156, BCE loss: 0.081, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 6 Step: 16 Total Loss: 0.121, BCE loss: 0.057, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 6 Step: 17 Total Loss: 0.196, BCE loss: 0.141, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 6 Step: 18 Total Loss: 0.208, BCE loss: 0.123, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 6 Step: 19 Total Loss: 0.328, BCE loss: 0.235, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 6 Step: 20 Total Loss: 0.207, BCE loss: 0.110, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 6 Step: 21 Total Loss: 0.143, BCE loss: 0.093, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 6 Step: 22 Total Loss: 0.295, BCE loss: 0.201, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 6 Step: 23 Total Loss: 0.149, BCE loss: 0.095, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 6 Step: 24 Total Loss: 0.316, BCE loss: 0.255, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 6 Step: 25 Total Loss: 0.164, BCE loss: 0.077, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 6 Step: 26 Total Loss: 0.256, BCE loss: 0.198, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 6 Step: 27 Total Loss: 0.390, BCE loss: 0.267, Diversity Loss: 0.245                     (Diversity_weight = 0.5)
Epoch: 6 Step: 28 Total Loss: 0.116, BCE loss: 0.056, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 6 Step: 29 Total Loss: 0.106, BCE loss: 0.050, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 6 Step: 30 Total Loss: 0.229, BCE loss: 0.177, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 6 Step: 31 Total Loss: 0.187, BCE loss: 0.115, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 6 Step: 32 Total Loss: 0.105, BCE loss: 0.037, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 6 Step: 33 Total Loss: 0.172, BCE loss: 0.096, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 6 Step: 34 Total Loss: 0.153, BCE loss: 0.096, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 6 Step: 35 Total Loss: 0.148, BCE loss: 0.081, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 6 Step: 36 Total Loss: 0.202, BCE loss: 0.121, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 6 Step: 37 Total Loss: 0.134, BCE loss: 0.058, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 6 Step: 38 Total Loss: 0.161, BCE loss: 0.043, Diversity Loss: 0.236                     (Diversity_weight = 0.5)
Epoch: 6 Step: 39 Total Loss: 0.296, BCE loss: 0.235, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 6 Step: 40 Total Loss: 0.351, BCE loss: 0.234, Diversity Loss: 0.235                     (Diversity_weight = 0.5)
Epoch: 6 Step: 41 Total Loss: 0.119, BCE loss: 0.047, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 6 Step: 42 Total Loss: 0.147, BCE loss: 0.092, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 6 Step: 43 Total Loss: 0.116, BCE loss: 0.067, Diversity Loss: 0.098                     (Diversity_weight = 0.5)
Epoch: 6 Step: 44 Total Loss: 0.159, BCE loss: 0.084, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 6 Step: 45 Total Loss: 0.132, BCE loss: 0.069, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 6 Step: 46 Total Loss: 0.399, BCE loss: 0.335, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 6 Step: 47 Total Loss: 0.361, BCE loss: 0.298, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 6 Step: 48 Total Loss: 0.203, BCE loss: 0.136, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 6 Step: 49 Total Loss: 0.227, BCE loss: 0.177, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 6 Step: 50 Total Loss: 0.131, BCE loss: 0.054, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 6 Step: 51 Total Loss: 0.204, BCE loss: 0.132, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 6 Step: 52 Total Loss: 0.169, BCE loss: 0.118, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 6 Step: 53 Total Loss: 0.272, BCE loss: 0.221, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 6 Step: 54 Total Loss: 0.296, BCE loss: 0.204, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 6 Step: 55 Total Loss: 0.213, BCE loss: 0.108, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 6 Step: 56 Total Loss: 0.161, BCE loss: 0.079, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 6 Step: 57 Total Loss: 0.128, BCE loss: 0.049, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 6 Step: 58 Total Loss: 0.250, BCE loss: 0.180, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 6 Step: 59 Total Loss: 0.188, BCE loss: 0.089, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 6 Step: 60 Total Loss: 0.200, BCE loss: 0.112, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 6 Step: 61 Total Loss: 0.323, BCE loss: 0.192, Diversity Loss: 0.263                     (Diversity_weight = 0.5)
Epoch: 6 Step: 62 Total Loss: 0.204, BCE loss: 0.125, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
{'accuracy': 0.717434869739479, 'roc_auc': 0.8020804466132498, 'pr_auc': 0.7740773567348055, 'conicity_mean': 0.14592238, 'conicity_std': 0.047350947}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.732    0.701      0.717      0.717         0.717
precision    0.701    0.738      0.717      0.719         0.719
recall       0.767    0.667      0.717      0.717         0.717
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.8020804466132498
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:46:47_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.285, BCE loss: 0.154, Diversity Loss: 0.263                     (Diversity_weight = 0.5)
Epoch: 7 Step: 1 Total Loss: 0.138, BCE loss: 0.089, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 7 Step: 2 Total Loss: 0.112, BCE loss: 0.042, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 7 Step: 3 Total Loss: 0.089, BCE loss: 0.025, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 7 Step: 4 Total Loss: 0.100, BCE loss: 0.041, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 7 Step: 5 Total Loss: 0.088, BCE loss: 0.023, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 7 Step: 6 Total Loss: 0.117, BCE loss: 0.042, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 7 Step: 7 Total Loss: 0.080, BCE loss: 0.039, Diversity Loss: 0.082                     (Diversity_weight = 0.5)
Epoch: 7 Step: 8 Total Loss: 0.135, BCE loss: 0.084, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 7 Step: 9 Total Loss: 0.193, BCE loss: 0.111, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 7 Step: 10 Total Loss: 0.091, BCE loss: 0.014, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 7 Step: 11 Total Loss: 0.173, BCE loss: 0.078, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 7 Step: 12 Total Loss: 0.235, BCE loss: 0.183, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 7 Step: 13 Total Loss: 0.125, BCE loss: 0.068, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 7 Step: 14 Total Loss: 0.072, BCE loss: 0.022, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 7 Step: 15 Total Loss: 0.120, BCE loss: 0.035, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 7 Step: 16 Total Loss: 0.191, BCE loss: 0.090, Diversity Loss: 0.201                     (Diversity_weight = 0.5)
Epoch: 7 Step: 17 Total Loss: 0.294, BCE loss: 0.179, Diversity Loss: 0.231                     (Diversity_weight = 0.5)
Epoch: 7 Step: 18 Total Loss: 0.136, BCE loss: 0.062, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 7 Step: 19 Total Loss: 0.128, BCE loss: 0.054, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 7 Step: 20 Total Loss: 0.275, BCE loss: 0.194, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 7 Step: 21 Total Loss: 0.224, BCE loss: 0.131, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 7 Step: 22 Total Loss: 0.199, BCE loss: 0.112, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 7 Step: 23 Total Loss: 0.084, BCE loss: 0.029, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 7 Step: 24 Total Loss: 0.160, BCE loss: 0.042, Diversity Loss: 0.234                     (Diversity_weight = 0.5)
Epoch: 7 Step: 25 Total Loss: 0.106, BCE loss: 0.052, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 7 Step: 26 Total Loss: 0.112, BCE loss: 0.047, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 7 Step: 27 Total Loss: 0.114, BCE loss: 0.040, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 7 Step: 28 Total Loss: 0.133, BCE loss: 0.082, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 7 Step: 29 Total Loss: 0.124, BCE loss: 0.052, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 7 Step: 30 Total Loss: 0.189, BCE loss: 0.130, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 7 Step: 31 Total Loss: 0.304, BCE loss: 0.218, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 7 Step: 32 Total Loss: 0.172, BCE loss: 0.094, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 7 Step: 33 Total Loss: 0.164, BCE loss: 0.076, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 7 Step: 34 Total Loss: 0.147, BCE loss: 0.088, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 7 Step: 35 Total Loss: 0.246, BCE loss: 0.187, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 7 Step: 36 Total Loss: 0.138, BCE loss: 0.079, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 7 Step: 37 Total Loss: 0.206, BCE loss: 0.155, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 7 Step: 38 Total Loss: 0.168, BCE loss: 0.104, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 7 Step: 39 Total Loss: 0.088, BCE loss: 0.034, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 7 Step: 40 Total Loss: 0.264, BCE loss: 0.208, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 7 Step: 41 Total Loss: 0.213, BCE loss: 0.104, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 7 Step: 42 Total Loss: 0.245, BCE loss: 0.154, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 7 Step: 43 Total Loss: 0.228, BCE loss: 0.164, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 7 Step: 44 Total Loss: 0.225, BCE loss: 0.157, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 7 Step: 45 Total Loss: 0.195, BCE loss: 0.109, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 7 Step: 46 Total Loss: 0.267, BCE loss: 0.171, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 7 Step: 47 Total Loss: 0.112, BCE loss: 0.063, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 7 Step: 48 Total Loss: 0.143, BCE loss: 0.091, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 7 Step: 49 Total Loss: 0.126, BCE loss: 0.052, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 7 Step: 50 Total Loss: 0.134, BCE loss: 0.059, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 7 Step: 51 Total Loss: 0.100, BCE loss: 0.038, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 7 Step: 52 Total Loss: 0.196, BCE loss: 0.125, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 7 Step: 53 Total Loss: 0.217, BCE loss: 0.140, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 7 Step: 54 Total Loss: 0.128, BCE loss: 0.057, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 7 Step: 55 Total Loss: 0.196, BCE loss: 0.131, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 7 Step: 56 Total Loss: 0.284, BCE loss: 0.162, Diversity Loss: 0.245                     (Diversity_weight = 0.5)
Epoch: 7 Step: 57 Total Loss: 0.134, BCE loss: 0.058, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 7 Step: 58 Total Loss: 0.097, BCE loss: 0.033, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 7 Step: 59 Total Loss: 0.180, BCE loss: 0.127, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 7 Step: 60 Total Loss: 0.089, BCE loss: 0.030, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 7 Step: 61 Total Loss: 0.146, BCE loss: 0.066, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 7 Step: 62 Total Loss: 0.215, BCE loss: 0.164, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
{'accuracy': 0.7525050100200401, 'roc_auc': 0.8024800690804665, 'pr_auc': 0.7747244213217281, 'conicity_mean': 0.14660114, 'conicity_std': 0.047710735}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.736    0.767      0.753      0.752         0.751
precision    0.795    0.720      0.753      0.757         0.758
recall       0.686    0.820      0.753      0.753         0.753
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.8024800690804665
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:46:47_2021
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:47:42,811 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:47:42,811 - type = vanillalstm
INFO - 2021-01-17 19:47:42,812 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:47:42,812 - vocab_size = 604
INFO - 2021-01-17 19:47:42,812 - embed_size = 200
INFO - 2021-01-17 19:47:42,812 - hidden_size = 128
INFO - 2021-01-17 19:47:42,812 - pre_embed = None
INFO - 2021-01-17 19:47:42,826 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:47:42,826 - hidden_size = 256
INFO - 2021-01-17 19:47:42,826 - output_size = 1
INFO - 2021-01-17 19:47:42,827 - use_attention = True
INFO - 2021-01-17 19:47:42,827 - regularizer_attention = None
INFO - 2021-01-17 19:47:42,827 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ba15b728ad0> and extras set()
INFO - 2021-01-17 19:47:42,827 - attention.type = tanh
INFO - 2021-01-17 19:47:42,827 - type = tanh
INFO - 2021-01-17 19:47:42,827 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ba15b728ad0> and extras set()
INFO - 2021-01-17 19:47:42,827 - attention.hidden_size = 256
INFO - 2021-01-17 19:47:42,827 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7344689378757515, 'roc_auc': 0.8095025804767355, 'pr_auc': 0.783851673667748, 'conicity_mean': '0.14465074', 'conicity_std': '0.046262644'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.744    0.724      0.734      0.734         0.734
precision    0.712    0.761      0.734      0.737         0.737
recall       0.780    0.690      0.734      0.735         0.734
support    495.000  503.000    998.000    998.000       998.000
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:47:44,181 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:47:44,181 - type = vanillalstm
INFO - 2021-01-17 19:47:44,181 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:47:44,181 - vocab_size = 604
INFO - 2021-01-17 19:47:44,181 - embed_size = 200
INFO - 2021-01-17 19:47:44,181 - hidden_size = 128
INFO - 2021-01-17 19:47:44,181 - pre_embed = None
INFO - 2021-01-17 19:47:44,195 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:47:44,195 - hidden_size = 256
INFO - 2021-01-17 19:47:44,196 - output_size = 1
INFO - 2021-01-17 19:47:44,196 - use_attention = True
INFO - 2021-01-17 19:47:44,196 - regularizer_attention = None
INFO - 2021-01-17 19:47:44,196 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ba15b6d86d0> and extras set()
INFO - 2021-01-17 19:47:44,196 - attention.type = tanh
INFO - 2021-01-17 19:47:44,196 - type = tanh
INFO - 2021-01-17 19:47:44,196 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ba15b6d86d0> and extras set()
INFO - 2021-01-17 19:47:44,196 - attention.hidden_size = 256
INFO - 2021-01-17 19:47:44,196 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7344689378757515, 'roc_auc': 0.8095025804767355, 'pr_auc': 0.783851673667748, 'conicity_mean': '0.14465074', 'conicity_std': '0.046262644'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.744    0.724      0.734      0.734         0.734
precision    0.712    0.761      0.734      0.737         0.737
recall       0.780    0.690      0.734      0.735         0.734
support    495.000  503.000    998.000    998.000       998.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_jp vanilla_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:47:54,559 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:47:54,559 - type = vanillalstm
INFO - 2021-01-17 19:47:54,560 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:47:54,578 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:47:54,818 - vocab_size = 548
INFO - 2021-01-17 19:47:54,818 - embed_size = 200
INFO - 2021-01-17 19:47:54,818 - hidden_size = 128
INFO - 2021-01-17 19:47:54,818 - pre_embed = None
INFO - 2021-01-17 19:47:57,889 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:47:57,890 - hidden_size = 256
INFO - 2021-01-17 19:47:57,890 - output_size = 1
INFO - 2021-01-17 19:47:57,890 - use_attention = True
INFO - 2021-01-17 19:47:57,890 - regularizer_attention = None
INFO - 2021-01-17 19:47:57,890 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b107c4011d0> and extras set()
INFO - 2021-01-17 19:47:57,890 - attention.type = tanh
INFO - 2021-01-17 19:47:57,890 - type = tanh
INFO - 2021-01-17 19:47:57,891 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b107c4011d0> and extras set()
INFO - 2021-01-17 19:47:57,891 - attention.hidden_size = 256
INFO - 2021-01-17 19:47:57,891 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.654, BCE loss: 0.654, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.812, BCE loss: 0.812, Diversity Loss: 0.599                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.635, BCE loss: 0.635, Diversity Loss: 0.607                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.709, BCE loss: 0.709, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.722, BCE loss: 0.722, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.701, BCE loss: 0.701, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.458                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.421                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.681, BCE loss: 0.681, Diversity Loss: 0.456                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.681, BCE loss: 0.681, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.438                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.660, BCE loss: 0.660, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.460                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.723, BCE loss: 0.723, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.744, BCE loss: 0.744, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.732, BCE loss: 0.732, Diversity Loss: 0.450                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.452                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.422                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.669, BCE loss: 0.669, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.666, BCE loss: 0.666, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.616, BCE loss: 0.616, Diversity Loss: 0.681                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.681, BCE loss: 0.681, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.666, BCE loss: 0.666, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.615, BCE loss: 0.615, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.634, BCE loss: 0.634, Diversity Loss: 0.656                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.702, BCE loss: 0.702, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.547, BCE loss: 0.547, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.604, BCE loss: 0.604, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.560, BCE loss: 0.560, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.565, BCE loss: 0.565, Diversity Loss: 0.710                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.622, BCE loss: 0.622, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.733, BCE loss: 0.733, Diversity Loss: 0.742                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.600, BCE loss: 0.600, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.599, BCE loss: 0.599, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.594, BCE loss: 0.594, Diversity Loss: 0.687                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.627, BCE loss: 0.627, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.772, BCE loss: 0.772, Diversity Loss: 0.678                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.531, BCE loss: 0.531, Diversity Loss: 0.717                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.752, BCE loss: 0.752, Diversity Loss: 0.598                     (Diversity_weight = 0)
{'accuracy': 0.6152304609218436, 'roc_auc': 0.7729536077286907, 'pr_auc': 0.7232896432349099, 'conicity_mean': 0.59994614, 'conicity_std': 0.12296994}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.478    0.695      0.615      0.587         0.582
precision    0.826    0.558      0.615      0.692         0.699
recall       0.337    0.922      0.615      0.629         0.615
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7729536077286907
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:47:58_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.603, BCE loss: 0.603, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.676                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.608, BCE loss: 0.608, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.606, BCE loss: 0.606, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.552, BCE loss: 0.552, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.596, BCE loss: 0.596, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.520, BCE loss: 0.520, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.542, BCE loss: 0.542, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.533, BCE loss: 0.533, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.580, BCE loss: 0.580, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.583, BCE loss: 0.583, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.626, BCE loss: 0.626, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.598, BCE loss: 0.598, Diversity Loss: 0.585                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.623, BCE loss: 0.623, Diversity Loss: 0.654                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.645, BCE loss: 0.645, Diversity Loss: 0.623                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.592, BCE loss: 0.592, Diversity Loss: 0.741                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.779                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.547, BCE loss: 0.547, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.690                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.456, BCE loss: 0.456, Diversity Loss: 0.638                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.716, BCE loss: 0.716, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.696                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.582                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.619, BCE loss: 0.619, Diversity Loss: 0.658                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.466, BCE loss: 0.466, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.541, BCE loss: 0.541, Diversity Loss: 0.612                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.651                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.490, BCE loss: 0.490, Diversity Loss: 0.649                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.390, BCE loss: 0.390, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.582, BCE loss: 0.582, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.512, BCE loss: 0.512, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.423, BCE loss: 0.423, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.660, BCE loss: 0.660, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.870, BCE loss: 0.870, Diversity Loss: 0.704                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.691                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.586                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.707, BCE loss: 0.707, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.594, BCE loss: 0.594, Diversity Loss: 0.662                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.614, BCE loss: 0.614, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.609, BCE loss: 0.609, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.592, BCE loss: 0.592, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.673, BCE loss: 0.673, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.614, BCE loss: 0.614, Diversity Loss: 0.623                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.578, BCE loss: 0.578, Diversity Loss: 0.642                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.579, BCE loss: 0.579, Diversity Loss: 0.487                     (Diversity_weight = 0)
{'accuracy': 0.7374749498997996, 'roc_auc': 0.8027774982389051, 'pr_auc': 0.7639423649702205, 'conicity_mean': 0.56438744, 'conicity_std': 0.1403756}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.743    0.732      0.737      0.737         0.738
precision    0.763    0.713      0.737      0.738         0.739
recall       0.725    0.752      0.737      0.738         0.737
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.8027774982389051
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:47:58_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.521, BCE loss: 0.521, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.517, BCE loss: 0.517, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.593, BCE loss: 0.593, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.487, BCE loss: 0.487, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.487, BCE loss: 0.487, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.461, BCE loss: 0.461, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.551, BCE loss: 0.551, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.469, BCE loss: 0.469, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.429, BCE loss: 0.429, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.570, BCE loss: 0.570, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.752                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.492, BCE loss: 0.492, Diversity Loss: 0.622                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.596, BCE loss: 0.596, Diversity Loss: 0.561                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.483, BCE loss: 0.483, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.323, BCE loss: 0.323, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.425, BCE loss: 0.425, Diversity Loss: 0.731                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.504, BCE loss: 0.504, Diversity Loss: 0.781                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.318, BCE loss: 0.318, Diversity Loss: 0.713                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.463, BCE loss: 0.463, Diversity Loss: 0.645                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.518, BCE loss: 0.518, Diversity Loss: 0.714                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.306, BCE loss: 0.306, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.480, BCE loss: 0.480, Diversity Loss: 0.661                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.440, BCE loss: 0.440, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.462, BCE loss: 0.462, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.713, BCE loss: 0.713, Diversity Loss: 0.640                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.561                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.602, BCE loss: 0.602, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.590, BCE loss: 0.590, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.494, BCE loss: 0.494, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.587, BCE loss: 0.587, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.421                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.408, BCE loss: 0.408, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.425, BCE loss: 0.425, Diversity Loss: 0.433                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.512, BCE loss: 0.512, Diversity Loss: 0.445                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.474, BCE loss: 0.474, Diversity Loss: 0.415                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.427                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.442                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.442, BCE loss: 0.442, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.666, BCE loss: 0.666, Diversity Loss: 0.434                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.451                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.462, BCE loss: 0.462, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.355, BCE loss: 0.355, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.618, BCE loss: 0.618, Diversity Loss: 0.449                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.456                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.534, BCE loss: 0.534, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.513, BCE loss: 0.513, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.512, BCE loss: 0.512, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.511, BCE loss: 0.511, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.338, BCE loss: 0.338, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.637, BCE loss: 0.637, Diversity Loss: 0.627                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.544, BCE loss: 0.544, Diversity Loss: 0.510                     (Diversity_weight = 0)
{'accuracy': 0.7635270541082164, 'roc_auc': 0.8472013686223205, 'pr_auc': 0.8311913732474665, 'conicity_mean': 0.5091535, 'conicity_std': 0.13667986}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.779    0.746      0.764      0.762         0.763
precision    0.764    0.763      0.764      0.763         0.763
recall       0.793    0.731      0.764      0.762         0.764
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.8472013686223205
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:47:58_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.295, BCE loss: 0.295, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.394, BCE loss: 0.394, Diversity Loss: 0.629                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.398, BCE loss: 0.398, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.364, BCE loss: 0.364, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.336, BCE loss: 0.336, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.452, BCE loss: 0.452, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.420, BCE loss: 0.420, Diversity Loss: 0.622                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.293, BCE loss: 0.293, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.489, BCE loss: 0.489, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.531, BCE loss: 0.531, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.325, BCE loss: 0.325, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.274, BCE loss: 0.274, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.391, BCE loss: 0.391, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.540, BCE loss: 0.540, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.397, BCE loss: 0.397, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.351, BCE loss: 0.351, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.286, BCE loss: 0.286, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.401, BCE loss: 0.401, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.346, BCE loss: 0.346, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.419, BCE loss: 0.419, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.335, BCE loss: 0.335, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.408                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.251, BCE loss: 0.251, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.569, BCE loss: 0.569, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.370, BCE loss: 0.370, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.295, BCE loss: 0.295, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.268, BCE loss: 0.268, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.560, BCE loss: 0.560, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.399, BCE loss: 0.399, Diversity Loss: 0.685                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.281, BCE loss: 0.281, Diversity Loss: 0.623                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.427, BCE loss: 0.427, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.361, BCE loss: 0.361, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.313, BCE loss: 0.313, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.515, BCE loss: 0.515, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.457, BCE loss: 0.457, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.585, BCE loss: 0.585, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.343, BCE loss: 0.343, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.379, BCE loss: 0.379, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.392, BCE loss: 0.392, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.482, BCE loss: 0.482, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.317, BCE loss: 0.317, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.420, BCE loss: 0.420, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.422, BCE loss: 0.422, Diversity Loss: 0.654                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.510, BCE loss: 0.510, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.310, BCE loss: 0.310, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.432                     (Diversity_weight = 0)
{'accuracy': 0.7755511022044088, 'roc_auc': 0.85336016906511, 'pr_auc': 0.8396945390206695, 'conicity_mean': 0.48733726, 'conicity_std': 0.1378997}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.787    0.763      0.776      0.775         0.775
precision    0.783    0.768      0.776      0.775         0.775
recall       0.792    0.758      0.776      0.775         0.776
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.85336016906511
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:47:58_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.417, BCE loss: 0.417, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.368, BCE loss: 0.368, Diversity Loss: 0.417                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.319, BCE loss: 0.319, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.354, BCE loss: 0.354, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.159, BCE loss: 0.159, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.292, BCE loss: 0.292, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.471                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.267, BCE loss: 0.267, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.411, BCE loss: 0.411, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.220, BCE loss: 0.220, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.375, BCE loss: 0.375, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.234, BCE loss: 0.234, Diversity Loss: 0.586                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 1.288, BCE loss: 1.288, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.465, BCE loss: 0.465, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.332, BCE loss: 0.332, Diversity Loss: 0.674                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.302, BCE loss: 0.302, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.301, BCE loss: 0.301, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.649                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.292, BCE loss: 0.292, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.213, BCE loss: 0.213, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.240, BCE loss: 0.240, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.151, BCE loss: 0.151, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.454, BCE loss: 0.454, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.253, BCE loss: 0.253, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.336, BCE loss: 0.336, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.272, BCE loss: 0.272, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.297, BCE loss: 0.297, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.345, BCE loss: 0.345, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.366, BCE loss: 0.366, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.308, BCE loss: 0.308, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.591                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.319, BCE loss: 0.319, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.445, BCE loss: 0.445, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.514, BCE loss: 0.514, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.456                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.336, BCE loss: 0.336, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.310, BCE loss: 0.310, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.263, BCE loss: 0.263, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.366, BCE loss: 0.366, Diversity Loss: 0.434                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.322, BCE loss: 0.322, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.482, BCE loss: 0.482, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.265, BCE loss: 0.265, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.422, BCE loss: 0.422, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.726                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.398, BCE loss: 0.398, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.394, BCE loss: 0.394, Diversity Loss: 0.631                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.330, BCE loss: 0.330, Diversity Loss: 0.532                     (Diversity_weight = 0)
{'accuracy': 0.7755511022044088, 'roc_auc': 0.849668914159203, 'pr_auc': 0.8378390792398216, 'conicity_mean': 0.5341738, 'conicity_std': 0.15201208}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.776    0.775      0.776      0.776         0.776
precision    0.813    0.741      0.776      0.777         0.779
recall       0.742    0.813      0.776      0.777         0.776
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.849668914159203
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:47:58_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.182, BCE loss: 0.182, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.316, BCE loss: 0.316, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.316, BCE loss: 0.316, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.471                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.233, BCE loss: 0.233, Diversity Loss: 0.445                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.437                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.285, BCE loss: 0.285, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.130, BCE loss: 0.130, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.341, BCE loss: 0.341, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.322, BCE loss: 0.322, Diversity Loss: 0.438                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.438                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.484, BCE loss: 0.484, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.445                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.318, BCE loss: 0.318, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.695                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.297, BCE loss: 0.297, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.266, BCE loss: 0.266, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.133, BCE loss: 0.133, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.262, BCE loss: 0.262, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.125, BCE loss: 0.125, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.429                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.284, BCE loss: 0.284, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.361, BCE loss: 0.361, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.189, BCE loss: 0.189, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.226, BCE loss: 0.226, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.109, BCE loss: 0.109, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.470                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.247, BCE loss: 0.247, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.263, BCE loss: 0.263, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.182, BCE loss: 0.182, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.303, BCE loss: 0.303, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.211, BCE loss: 0.211, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.422                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.345, BCE loss: 0.345, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.451                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.150, BCE loss: 0.150, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.340, BCE loss: 0.340, Diversity Loss: 0.453                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.542                     (Diversity_weight = 0)
{'accuracy': 0.749498997995992, 'roc_auc': 0.834964274932072, 'pr_auc': 0.8207283960650834, 'conicity_mean': 0.4928034, 'conicity_std': 0.12920861}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.781    0.707      0.749      0.744         0.746
precision    0.720    0.798      0.749      0.759         0.757
recall       0.855    0.634      0.749      0.744         0.749
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.834964274932072
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:47:58_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.242, BCE loss: 0.242, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.108, BCE loss: 0.108, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.701                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.313, BCE loss: 0.313, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.457                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.130, BCE loss: 0.130, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.164, BCE loss: 0.164, Diversity Loss: 0.458                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.312, BCE loss: 0.312, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.297, BCE loss: 0.297, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.198, BCE loss: 0.198, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.167, BCE loss: 0.167, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.693                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.436                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.251, BCE loss: 0.251, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.153, BCE loss: 0.153, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.438                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.429, BCE loss: 0.429, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.307, BCE loss: 0.307, Diversity Loss: 0.451                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.331, BCE loss: 0.331, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.453, BCE loss: 0.453, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.335, BCE loss: 0.335, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.320, BCE loss: 0.320, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.127, BCE loss: 0.127, Diversity Loss: 0.392                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.242, BCE loss: 0.242, Diversity Loss: 0.449                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.373                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.390                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.402                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.394                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.234, BCE loss: 0.234, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.424                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.227, BCE loss: 0.227, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.406                     (Diversity_weight = 0)
{'accuracy': 0.7464929859719439, 'roc_auc': 0.8315749220086546, 'pr_auc': 0.8160778906240441, 'conicity_mean': 0.47590408, 'conicity_std': 0.1350166}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.737    0.756      0.746      0.746         0.746
precision    0.808    0.698      0.746      0.753         0.756
recall       0.677    0.823      0.746      0.750         0.746
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.8315749220086546
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:47:58_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.108, BCE loss: 0.108, Diversity Loss: 0.574                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.452                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.423                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.291, BCE loss: 0.291, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.102, BCE loss: 0.102, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.630                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.331, BCE loss: 0.331, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.234, BCE loss: 0.234, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.094, BCE loss: 0.094, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.584                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.429                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.050, BCE loss: 0.050, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.125, BCE loss: 0.125, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.453                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.458                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.249, BCE loss: 0.249, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.172, BCE loss: 0.172, Diversity Loss: 0.433                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.433                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.118, BCE loss: 0.118, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.428                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.433                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.310, BCE loss: 0.310, Diversity Loss: 0.430                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.499                     (Diversity_weight = 0)
{'accuracy': 0.7394789579158316, 'roc_auc': 0.8176552279359968, 'pr_auc': 0.7986997802265831, 'conicity_mean': 0.490009, 'conicity_std': 0.12762775}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.722    0.755      0.739      0.738         0.738
precision    0.818    0.684      0.739      0.751         0.754
recall       0.646    0.842      0.739      0.744         0.739
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.8176552279359968
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:47:58_2021
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:48:45,845 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:48:45,845 - type = vanillalstm
INFO - 2021-01-17 19:48:45,845 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:48:45,845 - vocab_size = 548
INFO - 2021-01-17 19:48:45,845 - embed_size = 200
INFO - 2021-01-17 19:48:45,846 - hidden_size = 128
INFO - 2021-01-17 19:48:45,846 - pre_embed = None
INFO - 2021-01-17 19:48:45,860 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:48:45,860 - hidden_size = 256
INFO - 2021-01-17 19:48:45,860 - output_size = 1
INFO - 2021-01-17 19:48:45,860 - use_attention = True
INFO - 2021-01-17 19:48:45,860 - regularizer_attention = None
INFO - 2021-01-17 19:48:45,860 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b107c4409d0> and extras set()
INFO - 2021-01-17 19:48:45,860 - attention.type = tanh
INFO - 2021-01-17 19:48:45,860 - type = tanh
INFO - 2021-01-17 19:48:45,860 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b107c4409d0> and extras set()
INFO - 2021-01-17 19:48:45,861 - attention.hidden_size = 256
INFO - 2021-01-17 19:48:45,861 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7677677677677678, 'roc_auc': 0.8557251908396947, 'pr_auc': 0.8702743742911447, 'conicity_mean': '0.48544788', 'conicity_std': '0.13002373'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.768    0.768      0.768      0.768         0.768
precision    0.731    0.808      0.768      0.770         0.772
recall       0.808    0.731      0.768      0.770         0.768
support    475.000  524.000    999.000    999.000       999.000
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:48:47,145 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:48:47,146 - type = vanillalstm
INFO - 2021-01-17 19:48:47,146 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:48:47,146 - vocab_size = 548
INFO - 2021-01-17 19:48:47,146 - embed_size = 200
INFO - 2021-01-17 19:48:47,146 - hidden_size = 128
INFO - 2021-01-17 19:48:47,146 - pre_embed = None
INFO - 2021-01-17 19:48:47,160 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:48:47,160 - hidden_size = 256
INFO - 2021-01-17 19:48:47,160 - output_size = 1
INFO - 2021-01-17 19:48:47,160 - use_attention = True
INFO - 2021-01-17 19:48:47,160 - regularizer_attention = None
INFO - 2021-01-17 19:48:47,160 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b107c401090> and extras set()
INFO - 2021-01-17 19:48:47,160 - attention.type = tanh
INFO - 2021-01-17 19:48:47,160 - type = tanh
INFO - 2021-01-17 19:48:47,161 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b107c401090> and extras set()
INFO - 2021-01-17 19:48:47,161 - attention.hidden_size = 256
INFO - 2021-01-17 19:48:47,161 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7677677677677678, 'roc_auc': 0.8557251908396947, 'pr_auc': 0.8702743742911447, 'conicity_mean': '0.48544788', 'conicity_std': '0.13002373'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.768    0.768      0.768      0.768         0.768
precision    0.731    0.808      0.768      0.770         0.772
recall       0.808    0.731      0.768      0.770         0.768
support    475.000  524.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_jp ortho_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:48:56,839 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:48:56,840 - type = ortholstm
INFO - 2021-01-17 19:48:56,840 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:48:56,858 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:48:57,097 - vocab_size = 548
INFO - 2021-01-17 19:48:57,097 - embed_size = 200
INFO - 2021-01-17 19:48:57,097 - hidden_size = 128
INFO - 2021-01-17 19:48:57,097 - pre_embed = None
INFO - 2021-01-17 19:49:00,170 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:49:00,171 - hidden_size = 256
INFO - 2021-01-17 19:49:00,171 - output_size = 1
INFO - 2021-01-17 19:49:00,171 - use_attention = True
INFO - 2021-01-17 19:49:00,171 - regularizer_attention = None
INFO - 2021-01-17 19:49:00,171 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2add98b2d250> and extras set()
INFO - 2021-01-17 19:49:00,172 - attention.type = tanh
INFO - 2021-01-17 19:49:00,172 - type = tanh
INFO - 2021-01-17 19:49:00,172 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2add98b2d250> and extras set()
INFO - 2021-01-17 19:49:00,172 - attention.hidden_size = 256
INFO - 2021-01-17 19:49:00,172 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.669, BCE loss: 0.669, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.603, BCE loss: 0.603, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.701, BCE loss: 0.701, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.568, BCE loss: 0.568, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.713, BCE loss: 0.713, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.598, BCE loss: 0.598, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.640, BCE loss: 0.640, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.642, BCE loss: 0.642, Diversity Loss: 0.304                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.607, BCE loss: 0.607, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.234                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.516, BCE loss: 0.516, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.645, BCE loss: 0.645, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.582, BCE loss: 0.582, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.781, BCE loss: 0.781, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.705, BCE loss: 0.705, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.658, BCE loss: 0.658, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.627, BCE loss: 0.627, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.708, BCE loss: 0.708, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.636, BCE loss: 0.636, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.642, BCE loss: 0.642, Diversity Loss: 0.259                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.232                     (Diversity_weight = 0)
{'accuracy': 0.654308617234469, 'roc_auc': 0.723522189795713, 'pr_auc': 0.6803729498859811, 'conicity_mean': 0.19256754, 'conicity_std': 0.04121998}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.606    0.692      0.654      0.649         0.647
precision    0.753    0.601      0.654      0.677         0.680
recall       0.507    0.817      0.654      0.662         0.654
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.723522189795713
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:49:00_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.560, BCE loss: 0.560, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.513, BCE loss: 0.513, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.610, BCE loss: 0.610, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.631, BCE loss: 0.631, Diversity Loss: 0.252                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.616, BCE loss: 0.616, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.508, BCE loss: 0.508, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.520, BCE loss: 0.520, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.573, BCE loss: 0.573, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.597, BCE loss: 0.597, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.239                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.512, BCE loss: 0.512, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.600, BCE loss: 0.600, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.614, BCE loss: 0.614, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.533, BCE loss: 0.533, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.754, BCE loss: 0.754, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.640, BCE loss: 0.640, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.477, BCE loss: 0.477, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.564, BCE loss: 0.564, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.658, BCE loss: 0.658, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.635, BCE loss: 0.635, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.415, BCE loss: 0.415, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.606, BCE loss: 0.606, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.480, BCE loss: 0.480, Diversity Loss: 0.247                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.508, BCE loss: 0.508, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.597, BCE loss: 0.597, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.500, BCE loss: 0.500, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.484, BCE loss: 0.484, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.666, BCE loss: 0.666, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.371, BCE loss: 0.371, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.547, BCE loss: 0.547, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.487, BCE loss: 0.487, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.617, BCE loss: 0.617, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.508, BCE loss: 0.508, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.373, BCE loss: 0.373, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.427, BCE loss: 0.427, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.554, BCE loss: 0.554, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.564, BCE loss: 0.564, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.596, BCE loss: 0.596, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.459, BCE loss: 0.459, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.548, BCE loss: 0.548, Diversity Loss: 0.316                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.479, BCE loss: 0.479, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.575, BCE loss: 0.575, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.445, BCE loss: 0.445, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.525, BCE loss: 0.525, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.533, BCE loss: 0.533, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.434, BCE loss: 0.434, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.497, BCE loss: 0.497, Diversity Loss: 0.213                     (Diversity_weight = 0)
{'accuracy': 0.7014028056112225, 'roc_auc': 0.7778041662473584, 'pr_auc': 0.7309342182665663, 'conicity_mean': 0.20023254, 'conicity_std': 0.04038021}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.692    0.710      0.701      0.701         0.701
precision    0.753    0.660      0.701      0.706         0.709
recall       0.641    0.768      0.701      0.704         0.701
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7778041662473584
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:49:00_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.356, BCE loss: 0.356, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.250, BCE loss: 0.250, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.444, BCE loss: 0.444, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.444, BCE loss: 0.444, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.439, BCE loss: 0.439, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.333, BCE loss: 0.333, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.556, BCE loss: 0.556, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.396, BCE loss: 0.396, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.434, BCE loss: 0.434, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.399, BCE loss: 0.399, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.398, BCE loss: 0.398, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.308, BCE loss: 0.308, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.387, BCE loss: 0.387, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.430, BCE loss: 0.430, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.352, BCE loss: 0.352, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.248, BCE loss: 0.248, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.557, BCE loss: 0.557, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.443, BCE loss: 0.443, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.517, BCE loss: 0.517, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.343, BCE loss: 0.343, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.355, BCE loss: 0.355, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.616, BCE loss: 0.616, Diversity Loss: 0.247                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.298, BCE loss: 0.298, Diversity Loss: 0.237                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.391, BCE loss: 0.391, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.473, BCE loss: 0.473, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.533, BCE loss: 0.533, Diversity Loss: 0.317                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.480, BCE loss: 0.480, Diversity Loss: 0.248                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.422, BCE loss: 0.422, Diversity Loss: 0.240                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.487, BCE loss: 0.487, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.363, BCE loss: 0.363, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.268                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.626, BCE loss: 0.626, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.474, BCE loss: 0.474, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.420, BCE loss: 0.420, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.439, BCE loss: 0.439, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.567, BCE loss: 0.567, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.475, BCE loss: 0.475, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.489, BCE loss: 0.489, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.357, BCE loss: 0.357, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.477, BCE loss: 0.477, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.540, BCE loss: 0.540, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.590, BCE loss: 0.590, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.512, BCE loss: 0.512, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.398, BCE loss: 0.398, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.484, BCE loss: 0.484, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.369, BCE loss: 0.369, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.293, BCE loss: 0.293, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.400, BCE loss: 0.400, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.316, BCE loss: 0.316, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.349, BCE loss: 0.349, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.434, BCE loss: 0.434, Diversity Loss: 0.181                     (Diversity_weight = 0)
{'accuracy': 0.7244488977955912, 'roc_auc': 0.7786816946764616, 'pr_auc': 0.7500907810481648, 'conicity_mean': 0.19743156, 'conicity_std': 0.040044412}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.739    0.708      0.724      0.724         0.724
precision    0.734    0.714      0.724      0.724         0.724
recall       0.744    0.703      0.724      0.723         0.724
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7786816946764616
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:49:00_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.266, BCE loss: 0.266, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.409, BCE loss: 0.409, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.344, BCE loss: 0.344, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.354, BCE loss: 0.354, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.306, BCE loss: 0.306, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.560, BCE loss: 0.560, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.305, BCE loss: 0.305, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.218, BCE loss: 0.218, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.307, BCE loss: 0.307, Diversity Loss: 0.243                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.290, BCE loss: 0.290, Diversity Loss: 0.251                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.392, BCE loss: 0.392, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.263, BCE loss: 0.263, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.208, BCE loss: 0.208, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.248, BCE loss: 0.248, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.242, BCE loss: 0.242, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.280, BCE loss: 0.280, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.275, BCE loss: 0.275, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.237                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.390, BCE loss: 0.390, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.159, BCE loss: 0.159, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.306, BCE loss: 0.306, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.238, BCE loss: 0.238, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.248, BCE loss: 0.248, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.327, BCE loss: 0.327, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.444, BCE loss: 0.444, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.354, BCE loss: 0.354, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.267, BCE loss: 0.267, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.379, BCE loss: 0.379, Diversity Loss: 0.304                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.250, BCE loss: 0.250, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.377, BCE loss: 0.377, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.379, BCE loss: 0.379, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.525, BCE loss: 0.525, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.228                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.255, BCE loss: 0.255, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.307, BCE loss: 0.307, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.192, BCE loss: 0.192, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.357, BCE loss: 0.357, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.161, BCE loss: 0.161, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.326, BCE loss: 0.326, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.262, BCE loss: 0.262, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.329, BCE loss: 0.329, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.310, BCE loss: 0.310, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.589, BCE loss: 0.589, Diversity Loss: 0.265                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.348, BCE loss: 0.348, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.479, BCE loss: 0.479, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.232, BCE loss: 0.232, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.454, BCE loss: 0.454, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.462, BCE loss: 0.462, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.359, BCE loss: 0.359, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.228                     (Diversity_weight = 0)
{'accuracy': 0.7124248496993988, 'roc_auc': 0.7681674549662877, 'pr_auc': 0.7444155667001042, 'conicity_mean': 0.19430332, 'conicity_std': 0.03811285}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.721    0.703      0.712      0.712         0.713
precision    0.733    0.691      0.712      0.712         0.713
recall       0.709    0.716      0.712      0.713         0.712
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7681674549662877
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:49:00_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.274, BCE loss: 0.274, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.263, BCE loss: 0.263, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.294, BCE loss: 0.294, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.294, BCE loss: 0.294, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.353, BCE loss: 0.353, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.290, BCE loss: 0.290, Diversity Loss: 0.241                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.232, BCE loss: 0.232, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.153, BCE loss: 0.153, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.247, BCE loss: 0.247, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.218, BCE loss: 0.218, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.241, BCE loss: 0.241, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.240, BCE loss: 0.240, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.234, BCE loss: 0.234, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.348, BCE loss: 0.348, Diversity Loss: 0.301                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.119, BCE loss: 0.119, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.301, BCE loss: 0.301, Diversity Loss: 0.261                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.266, BCE loss: 0.266, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.292, BCE loss: 0.292, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.485, BCE loss: 0.485, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.218, BCE loss: 0.218, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.191, BCE loss: 0.191, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.213, BCE loss: 0.213, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.242, BCE loss: 0.242, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.349, BCE loss: 0.349, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.244                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.264, BCE loss: 0.264, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.384, BCE loss: 0.384, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.317, BCE loss: 0.317, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.251, BCE loss: 0.251, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.314, BCE loss: 0.314, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.204, BCE loss: 0.204, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.210                     (Diversity_weight = 0)
{'accuracy': 0.687374749498998, 'roc_auc': 0.7558015497635102, 'pr_auc': 0.7285491376030483, 'conicity_mean': 0.19258559, 'conicity_std': 0.03877108}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.726    0.636      0.687      0.681         0.683
precision    0.671    0.714      0.687      0.692         0.691
recall       0.792    0.573      0.687      0.682         0.687
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7558015497635102
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:49:00_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.134, BCE loss: 0.134, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.244                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.244, BCE loss: 0.244, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.297                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.449, BCE loss: 0.449, Diversity Loss: 0.260                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.220, BCE loss: 0.220, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.232, BCE loss: 0.232, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.220, BCE loss: 0.220, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.108, BCE loss: 0.108, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.151, BCE loss: 0.151, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.305, BCE loss: 0.305, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.351, BCE loss: 0.351, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.248, BCE loss: 0.248, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.262, BCE loss: 0.262, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.260, BCE loss: 0.260, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.124, BCE loss: 0.124, Diversity Loss: 0.212                     (Diversity_weight = 0)
{'accuracy': 0.6923847695390781, 'roc_auc': 0.7478715910234476, 'pr_auc': 0.7079891597441952, 'conicity_mean': 0.19055063, 'conicity_std': 0.03797234}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.716    0.664      0.692      0.690         0.691
precision    0.694    0.691      0.692      0.692         0.692
recall       0.740    0.640      0.692      0.690         0.692
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7478715910234476
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:49:00_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.117                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.128, BCE loss: 0.128, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.151, BCE loss: 0.151, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.241                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.118, BCE loss: 0.118, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.102, BCE loss: 0.102, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.244, BCE loss: 0.244, Diversity Loss: 0.265                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.172, BCE loss: 0.172, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.174, BCE loss: 0.174, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.109, BCE loss: 0.109, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.050, BCE loss: 0.050, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.125, BCE loss: 0.125, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.173, BCE loss: 0.173, Diversity Loss: 0.294                     (Diversity_weight = 0)
{'accuracy': 0.6923847695390781, 'roc_auc': 0.7350065412096206, 'pr_auc': 0.6816773515189113, 'conicity_mean': 0.1915535, 'conicity_std': 0.037421938}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.711    0.672      0.692      0.691         0.692
precision    0.701    0.683      0.692      0.692         0.692
recall       0.721    0.661      0.692      0.691         0.692
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7350065412096206
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:49:00_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.241                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.173, BCE loss: 0.173, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.191, BCE loss: 0.191, Diversity Loss: 0.299                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.229                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.127, BCE loss: 0.127, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.133, BCE loss: 0.133, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.243                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.262                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.157                     (Diversity_weight = 0)
{'accuracy': 0.7004008016032064, 'roc_auc': 0.7429163731508504, 'pr_auc': 0.6789565006122498, 'conicity_mean': 0.19242786, 'conicity_std': 0.037264876}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.711    0.689        0.7        0.7         0.701
precision    0.719    0.681        0.7        0.7         0.701
recall       0.704    0.697        0.7        0.7         0.700
support    523.000  475.000      998.0      998.0       998.000
Model not saved on  roc_auc 0.7429163731508504
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:49:00_2021
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:85: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:86: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:50:19,686 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:50:19,686 - type = ortholstm
INFO - 2021-01-17 19:50:19,687 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:50:19,687 - vocab_size = 548
INFO - 2021-01-17 19:50:19,687 - embed_size = 200
INFO - 2021-01-17 19:50:19,687 - hidden_size = 128
INFO - 2021-01-17 19:50:19,687 - pre_embed = None
INFO - 2021-01-17 19:50:19,701 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:50:19,702 - hidden_size = 256
INFO - 2021-01-17 19:50:19,702 - output_size = 1
INFO - 2021-01-17 19:50:19,702 - use_attention = True
INFO - 2021-01-17 19:50:19,702 - regularizer_attention = None
INFO - 2021-01-17 19:50:19,702 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2add98b6c790> and extras set()
INFO - 2021-01-17 19:50:19,702 - attention.type = tanh
INFO - 2021-01-17 19:50:19,702 - type = tanh
INFO - 2021-01-17 19:50:19,702 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2add98b6c790> and extras set()
INFO - 2021-01-17 19:50:19,702 - attention.hidden_size = 256
INFO - 2021-01-17 19:50:19,702 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7377377377377378, 'roc_auc': 0.804849337083166, 'pr_auc': 0.8061891524835373, 'conicity_mean': '0.19720025', 'conicity_std': '0.03798319'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.734    0.742      0.738      0.738         0.738
precision    0.709    0.767      0.738      0.738         0.740
recall       0.760    0.718      0.738      0.739         0.738
support    475.000  524.000    999.000    999.000       999.000
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:50:21,618 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:50:21,619 - type = ortholstm
INFO - 2021-01-17 19:50:21,619 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:50:21,619 - vocab_size = 548
INFO - 2021-01-17 19:50:21,619 - embed_size = 200
INFO - 2021-01-17 19:50:21,619 - hidden_size = 128
INFO - 2021-01-17 19:50:21,619 - pre_embed = None
INFO - 2021-01-17 19:50:21,633 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:50:21,633 - hidden_size = 256
INFO - 2021-01-17 19:50:21,633 - output_size = 1
INFO - 2021-01-17 19:50:21,633 - use_attention = True
INFO - 2021-01-17 19:50:21,633 - regularizer_attention = None
INFO - 2021-01-17 19:50:21,633 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2add940134d0> and extras set()
INFO - 2021-01-17 19:50:21,633 - attention.type = tanh
INFO - 2021-01-17 19:50:21,633 - type = tanh
INFO - 2021-01-17 19:50:21,634 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2add940134d0> and extras set()
INFO - 2021-01-17 19:50:21,634 - attention.hidden_size = 256
INFO - 2021-01-17 19:50:21,634 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7377377377377378, 'roc_auc': 0.804849337083166, 'pr_auc': 0.8061891524835373, 'conicity_mean': '0.19720025', 'conicity_std': '0.03798319'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.734    0.742      0.738      0.738         0.738
precision    0.709    0.767      0.738      0.738         0.740
recall       0.760    0.718      0.738      0.739         0.738
support    475.000  524.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_jp diversity_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error
[nltk_data]     [Errno 101] Network is unreachable>
INFO - 2021-01-17 19:50:33,549 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:50:33,549 - type = vanillalstm
INFO - 2021-01-17 19:50:33,549 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:50:33,568 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:50:33,807 - vocab_size = 548
INFO - 2021-01-17 19:50:33,807 - embed_size = 200
INFO - 2021-01-17 19:50:33,807 - hidden_size = 128
INFO - 2021-01-17 19:50:33,807 - pre_embed = None
INFO - 2021-01-17 19:50:36,874 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:50:36,874 - hidden_size = 256
INFO - 2021-01-17 19:50:36,874 - output_size = 1
INFO - 2021-01-17 19:50:36,874 - use_attention = True
INFO - 2021-01-17 19:50:36,874 - regularizer_attention = None
INFO - 2021-01-17 19:50:36,875 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b2e2da99050> and extras set()
INFO - 2021-01-17 19:50:36,875 - attention.type = tanh
INFO - 2021-01-17 19:50:36,875 - type = tanh
INFO - 2021-01-17 19:50:36,875 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b2e2da99050> and extras set()
INFO - 2021-01-17 19:50:36,875 - attention.hidden_size = 256
INFO - 2021-01-17 19:50:36,875 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.953, BCE loss: 0.696, Diversity Loss: 0.516                     (Diversity_weight = 0.5)
Epoch: 0 Step: 1 Total Loss: 0.948, BCE loss: 0.695, Diversity Loss: 0.506                     (Diversity_weight = 0.5)
Epoch: 0 Step: 2 Total Loss: 0.885, BCE loss: 0.697, Diversity Loss: 0.375                     (Diversity_weight = 0.5)
Epoch: 0 Step: 3 Total Loss: 0.865, BCE loss: 0.694, Diversity Loss: 0.342                     (Diversity_weight = 0.5)
Epoch: 0 Step: 4 Total Loss: 0.829, BCE loss: 0.690, Diversity Loss: 0.278                     (Diversity_weight = 0.5)
Epoch: 0 Step: 5 Total Loss: 0.840, BCE loss: 0.693, Diversity Loss: 0.293                     (Diversity_weight = 0.5)
Epoch: 0 Step: 6 Total Loss: 0.821, BCE loss: 0.689, Diversity Loss: 0.264                     (Diversity_weight = 0.5)
Epoch: 0 Step: 7 Total Loss: 0.814, BCE loss: 0.688, Diversity Loss: 0.251                     (Diversity_weight = 0.5)
Epoch: 0 Step: 8 Total Loss: 0.788, BCE loss: 0.689, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 0 Step: 9 Total Loss: 0.821, BCE loss: 0.687, Diversity Loss: 0.267                     (Diversity_weight = 0.5)
Epoch: 0 Step: 10 Total Loss: 0.810, BCE loss: 0.689, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 0 Step: 11 Total Loss: 0.870, BCE loss: 0.693, Diversity Loss: 0.353                     (Diversity_weight = 0.5)
Epoch: 0 Step: 12 Total Loss: 0.805, BCE loss: 0.693, Diversity Loss: 0.224                     (Diversity_weight = 0.5)
Epoch: 0 Step: 13 Total Loss: 0.825, BCE loss: 0.689, Diversity Loss: 0.274                     (Diversity_weight = 0.5)
Epoch: 0 Step: 14 Total Loss: 0.802, BCE loss: 0.694, Diversity Loss: 0.217                     (Diversity_weight = 0.5)
Epoch: 0 Step: 15 Total Loss: 0.831, BCE loss: 0.682, Diversity Loss: 0.298                     (Diversity_weight = 0.5)
Epoch: 0 Step: 16 Total Loss: 0.784, BCE loss: 0.687, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 0 Step: 17 Total Loss: 0.781, BCE loss: 0.687, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 0 Step: 18 Total Loss: 0.805, BCE loss: 0.690, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 0 Step: 19 Total Loss: 0.797, BCE loss: 0.669, Diversity Loss: 0.256                     (Diversity_weight = 0.5)
Epoch: 0 Step: 20 Total Loss: 0.815, BCE loss: 0.699, Diversity Loss: 0.232                     (Diversity_weight = 0.5)
Epoch: 0 Step: 21 Total Loss: 0.865, BCE loss: 0.660, Diversity Loss: 0.410                     (Diversity_weight = 0.5)
Epoch: 0 Step: 22 Total Loss: 0.814, BCE loss: 0.693, Diversity Loss: 0.243                     (Diversity_weight = 0.5)
Epoch: 0 Step: 23 Total Loss: 0.779, BCE loss: 0.683, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 0 Step: 24 Total Loss: 0.774, BCE loss: 0.697, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 0 Step: 25 Total Loss: 0.759, BCE loss: 0.681, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 0 Step: 26 Total Loss: 0.787, BCE loss: 0.685, Diversity Loss: 0.203                     (Diversity_weight = 0.5)
Epoch: 0 Step: 27 Total Loss: 0.781, BCE loss: 0.687, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 0 Step: 28 Total Loss: 0.768, BCE loss: 0.680, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 0 Step: 29 Total Loss: 0.787, BCE loss: 0.677, Diversity Loss: 0.219                     (Diversity_weight = 0.5)
Epoch: 0 Step: 30 Total Loss: 0.765, BCE loss: 0.673, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 0 Step: 31 Total Loss: 0.765, BCE loss: 0.666, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 0 Step: 32 Total Loss: 0.760, BCE loss: 0.687, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 0 Step: 33 Total Loss: 0.762, BCE loss: 0.687, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 0 Step: 34 Total Loss: 0.788, BCE loss: 0.690, Diversity Loss: 0.197                     (Diversity_weight = 0.5)
Epoch: 0 Step: 35 Total Loss: 0.763, BCE loss: 0.690, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 0 Step: 36 Total Loss: 0.770, BCE loss: 0.679, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 0 Step: 37 Total Loss: 0.801, BCE loss: 0.676, Diversity Loss: 0.249                     (Diversity_weight = 0.5)
Epoch: 0 Step: 38 Total Loss: 0.763, BCE loss: 0.666, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 0 Step: 39 Total Loss: 0.786, BCE loss: 0.689, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 0 Step: 40 Total Loss: 0.749, BCE loss: 0.676, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 0 Step: 41 Total Loss: 0.787, BCE loss: 0.679, Diversity Loss: 0.216                     (Diversity_weight = 0.5)
Epoch: 0 Step: 42 Total Loss: 0.757, BCE loss: 0.656, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 0 Step: 43 Total Loss: 0.757, BCE loss: 0.685, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 0 Step: 44 Total Loss: 0.768, BCE loss: 0.668, Diversity Loss: 0.201                     (Diversity_weight = 0.5)
Epoch: 0 Step: 45 Total Loss: 0.750, BCE loss: 0.663, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 0 Step: 46 Total Loss: 0.767, BCE loss: 0.687, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 0 Step: 47 Total Loss: 0.760, BCE loss: 0.640, Diversity Loss: 0.240                     (Diversity_weight = 0.5)
Epoch: 0 Step: 48 Total Loss: 0.790, BCE loss: 0.646, Diversity Loss: 0.289                     (Diversity_weight = 0.5)
Epoch: 0 Step: 49 Total Loss: 0.746, BCE loss: 0.625, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 0 Step: 50 Total Loss: 0.697, BCE loss: 0.620, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 0 Step: 51 Total Loss: 0.752, BCE loss: 0.672, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 0 Step: 52 Total Loss: 0.805, BCE loss: 0.614, Diversity Loss: 0.382                     (Diversity_weight = 0.5)
Epoch: 0 Step: 53 Total Loss: 0.793, BCE loss: 0.722, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 0 Step: 54 Total Loss: 0.741, BCE loss: 0.671, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 0 Step: 55 Total Loss: 0.744, BCE loss: 0.676, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 0 Step: 56 Total Loss: 0.723, BCE loss: 0.654, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 0 Step: 57 Total Loss: 0.706, BCE loss: 0.624, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 0 Step: 58 Total Loss: 0.730, BCE loss: 0.656, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 0 Step: 59 Total Loss: 0.712, BCE loss: 0.645, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 0 Step: 60 Total Loss: 0.709, BCE loss: 0.635, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 0 Step: 61 Total Loss: 0.672, BCE loss: 0.591, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 0 Step: 62 Total Loss: 0.780, BCE loss: 0.692, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
{'accuracy': 0.6733466933867736, 'roc_auc': 0.7216826003824091, 'pr_auc': 0.6831324680534556, 'conicity_mean': 0.1705754, 'conicity_std': 0.060506746}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.705    0.635      0.673      0.670         0.671
precision    0.670    0.679      0.673      0.674         0.674
recall       0.744    0.596      0.673      0.670         0.673
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7216826003824091
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:50:37_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.725, BCE loss: 0.644, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 1 Step: 1 Total Loss: 0.747, BCE loss: 0.674, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 1 Step: 2 Total Loss: 0.662, BCE loss: 0.546, Diversity Loss: 0.233                     (Diversity_weight = 0.5)
Epoch: 1 Step: 3 Total Loss: 0.722, BCE loss: 0.654, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 1 Step: 4 Total Loss: 0.714, BCE loss: 0.639, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 1 Step: 5 Total Loss: 0.694, BCE loss: 0.608, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 1 Step: 6 Total Loss: 0.696, BCE loss: 0.610, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 1 Step: 7 Total Loss: 0.680, BCE loss: 0.621, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 1 Step: 8 Total Loss: 0.603, BCE loss: 0.521, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 1 Step: 9 Total Loss: 0.655, BCE loss: 0.586, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 1 Step: 10 Total Loss: 0.652, BCE loss: 0.565, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 1 Step: 11 Total Loss: 0.742, BCE loss: 0.661, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 1 Step: 12 Total Loss: 0.679, BCE loss: 0.590, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 1 Step: 13 Total Loss: 0.607, BCE loss: 0.534, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 1 Step: 14 Total Loss: 0.766, BCE loss: 0.698, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 1 Step: 15 Total Loss: 0.816, BCE loss: 0.738, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 1 Step: 16 Total Loss: 0.852, BCE loss: 0.713, Diversity Loss: 0.278                     (Diversity_weight = 0.5)
Epoch: 1 Step: 17 Total Loss: 0.662, BCE loss: 0.575, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 1 Step: 18 Total Loss: 0.683, BCE loss: 0.613, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 1 Step: 19 Total Loss: 0.612, BCE loss: 0.518, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 1 Step: 20 Total Loss: 0.801, BCE loss: 0.735, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 1 Step: 21 Total Loss: 0.664, BCE loss: 0.585, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 1 Step: 22 Total Loss: 0.758, BCE loss: 0.681, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 1 Step: 23 Total Loss: 0.829, BCE loss: 0.765, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 1 Step: 24 Total Loss: 0.622, BCE loss: 0.548, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 1 Step: 25 Total Loss: 0.771, BCE loss: 0.692, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 1 Step: 26 Total Loss: 0.620, BCE loss: 0.557, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 1 Step: 27 Total Loss: 0.639, BCE loss: 0.527, Diversity Loss: 0.225                     (Diversity_weight = 0.5)
Epoch: 1 Step: 28 Total Loss: 0.670, BCE loss: 0.608, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 1 Step: 29 Total Loss: 0.675, BCE loss: 0.602, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 1 Step: 30 Total Loss: 0.677, BCE loss: 0.623, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 1 Step: 31 Total Loss: 0.650, BCE loss: 0.564, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 1 Step: 32 Total Loss: 0.635, BCE loss: 0.529, Diversity Loss: 0.211                     (Diversity_weight = 0.5)
Epoch: 1 Step: 33 Total Loss: 0.662, BCE loss: 0.597, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 1 Step: 34 Total Loss: 0.644, BCE loss: 0.544, Diversity Loss: 0.201                     (Diversity_weight = 0.5)
Epoch: 1 Step: 35 Total Loss: 0.690, BCE loss: 0.523, Diversity Loss: 0.334                     (Diversity_weight = 0.5)
Epoch: 1 Step: 36 Total Loss: 0.707, BCE loss: 0.649, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 1 Step: 37 Total Loss: 0.645, BCE loss: 0.580, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 1 Step: 38 Total Loss: 0.657, BCE loss: 0.590, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 1 Step: 39 Total Loss: 0.564, BCE loss: 0.486, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 1 Step: 40 Total Loss: 0.625, BCE loss: 0.540, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 1 Step: 41 Total Loss: 0.746, BCE loss: 0.693, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 1 Step: 42 Total Loss: 0.725, BCE loss: 0.666, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 1 Step: 43 Total Loss: 0.704, BCE loss: 0.645, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 1 Step: 44 Total Loss: 0.617, BCE loss: 0.529, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 1 Step: 45 Total Loss: 0.747, BCE loss: 0.686, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 1 Step: 46 Total Loss: 0.661, BCE loss: 0.526, Diversity Loss: 0.271                     (Diversity_weight = 0.5)
Epoch: 1 Step: 47 Total Loss: 0.596, BCE loss: 0.526, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 1 Step: 48 Total Loss: 0.681, BCE loss: 0.620, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 1 Step: 49 Total Loss: 0.906, BCE loss: 0.801, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 1 Step: 50 Total Loss: 0.791, BCE loss: 0.716, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 1 Step: 51 Total Loss: 0.668, BCE loss: 0.605, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 1 Step: 52 Total Loss: 0.784, BCE loss: 0.708, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 1 Step: 53 Total Loss: 0.611, BCE loss: 0.552, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 1 Step: 54 Total Loss: 0.643, BCE loss: 0.586, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 1 Step: 55 Total Loss: 0.611, BCE loss: 0.510, Diversity Loss: 0.202                     (Diversity_weight = 0.5)
Epoch: 1 Step: 56 Total Loss: 0.614, BCE loss: 0.528, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 1 Step: 57 Total Loss: 0.693, BCE loss: 0.643, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 1 Step: 58 Total Loss: 0.665, BCE loss: 0.603, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 59 Total Loss: 0.566, BCE loss: 0.500, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 1 Step: 60 Total Loss: 0.703, BCE loss: 0.604, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 1 Step: 61 Total Loss: 0.639, BCE loss: 0.565, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 1 Step: 62 Total Loss: 0.606, BCE loss: 0.519, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
{'accuracy': 0.685370741482966, 'roc_auc': 0.7424856596558317, 'pr_auc': 0.701801194527835, 'conicity_mean': 0.16998471, 'conicity_std': 0.057831194}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.727    0.629      0.685      0.678         0.680
precision    0.667    0.717      0.685      0.692         0.691
recall       0.799    0.560      0.685      0.680         0.685
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7424856596558317
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:50:37_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.740, BCE loss: 0.685, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 2 Step: 1 Total Loss: 0.639, BCE loss: 0.575, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 2 Step: 2 Total Loss: 0.619, BCE loss: 0.564, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 2 Step: 3 Total Loss: 0.518, BCE loss: 0.446, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 2 Step: 4 Total Loss: 0.690, BCE loss: 0.604, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 2 Step: 5 Total Loss: 0.717, BCE loss: 0.655, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 2 Step: 6 Total Loss: 0.559, BCE loss: 0.484, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 2 Step: 7 Total Loss: 0.606, BCE loss: 0.500, Diversity Loss: 0.213                     (Diversity_weight = 0.5)
Epoch: 2 Step: 8 Total Loss: 0.692, BCE loss: 0.621, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 2 Step: 9 Total Loss: 0.676, BCE loss: 0.613, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 2 Step: 10 Total Loss: 0.701, BCE loss: 0.633, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 2 Step: 11 Total Loss: 0.584, BCE loss: 0.525, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 2 Step: 12 Total Loss: 0.556, BCE loss: 0.478, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 2 Step: 13 Total Loss: 0.652, BCE loss: 0.567, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 2 Step: 14 Total Loss: 0.558, BCE loss: 0.487, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 2 Step: 15 Total Loss: 0.637, BCE loss: 0.580, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 2 Step: 16 Total Loss: 0.609, BCE loss: 0.501, Diversity Loss: 0.214                     (Diversity_weight = 0.5)
Epoch: 2 Step: 17 Total Loss: 0.731, BCE loss: 0.642, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 2 Step: 18 Total Loss: 0.707, BCE loss: 0.661, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 2 Step: 19 Total Loss: 0.680, BCE loss: 0.606, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 2 Step: 20 Total Loss: 0.642, BCE loss: 0.568, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 2 Step: 21 Total Loss: 0.676, BCE loss: 0.552, Diversity Loss: 0.249                     (Diversity_weight = 0.5)
Epoch: 2 Step: 22 Total Loss: 0.488, BCE loss: 0.415, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 2 Step: 23 Total Loss: 0.615, BCE loss: 0.541, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 2 Step: 24 Total Loss: 0.531, BCE loss: 0.463, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 2 Step: 25 Total Loss: 0.660, BCE loss: 0.584, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 2 Step: 26 Total Loss: 0.508, BCE loss: 0.443, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 2 Step: 27 Total Loss: 0.582, BCE loss: 0.516, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 2 Step: 28 Total Loss: 0.526, BCE loss: 0.433, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 2 Step: 29 Total Loss: 0.601, BCE loss: 0.497, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 2 Step: 30 Total Loss: 0.583, BCE loss: 0.506, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 2 Step: 31 Total Loss: 0.453, BCE loss: 0.379, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 2 Step: 32 Total Loss: 0.523, BCE loss: 0.430, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 2 Step: 33 Total Loss: 0.571, BCE loss: 0.517, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 2 Step: 34 Total Loss: 0.484, BCE loss: 0.390, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 2 Step: 35 Total Loss: 0.589, BCE loss: 0.514, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 2 Step: 36 Total Loss: 0.556, BCE loss: 0.490, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 2 Step: 37 Total Loss: 0.667, BCE loss: 0.602, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 2 Step: 38 Total Loss: 0.551, BCE loss: 0.478, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 2 Step: 39 Total Loss: 0.637, BCE loss: 0.572, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 2 Step: 40 Total Loss: 0.657, BCE loss: 0.595, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 2 Step: 41 Total Loss: 0.697, BCE loss: 0.646, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 2 Step: 42 Total Loss: 0.783, BCE loss: 0.704, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 2 Step: 43 Total Loss: 0.652, BCE loss: 0.577, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 2 Step: 44 Total Loss: 0.713, BCE loss: 0.628, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 2 Step: 45 Total Loss: 0.454, BCE loss: 0.379, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 2 Step: 46 Total Loss: 0.482, BCE loss: 0.397, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 2 Step: 47 Total Loss: 0.602, BCE loss: 0.538, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 2 Step: 48 Total Loss: 0.590, BCE loss: 0.513, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 2 Step: 49 Total Loss: 0.582, BCE loss: 0.515, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 2 Step: 50 Total Loss: 0.571, BCE loss: 0.435, Diversity Loss: 0.272                     (Diversity_weight = 0.5)
Epoch: 2 Step: 51 Total Loss: 0.586, BCE loss: 0.526, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 2 Step: 52 Total Loss: 0.497, BCE loss: 0.412, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 2 Step: 53 Total Loss: 0.566, BCE loss: 0.492, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 2 Step: 54 Total Loss: 0.619, BCE loss: 0.514, Diversity Loss: 0.211                     (Diversity_weight = 0.5)
Epoch: 2 Step: 55 Total Loss: 0.545, BCE loss: 0.486, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 2 Step: 56 Total Loss: 0.599, BCE loss: 0.485, Diversity Loss: 0.228                     (Diversity_weight = 0.5)
Epoch: 2 Step: 57 Total Loss: 0.647, BCE loss: 0.579, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 2 Step: 58 Total Loss: 0.572, BCE loss: 0.496, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 2 Step: 59 Total Loss: 0.632, BCE loss: 0.573, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 2 Step: 60 Total Loss: 0.616, BCE loss: 0.516, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 2 Step: 61 Total Loss: 0.485, BCE loss: 0.421, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 2 Step: 62 Total Loss: 0.826, BCE loss: 0.671, Diversity Loss: 0.310                     (Diversity_weight = 0.5)
{'accuracy': 0.688376753507014, 'roc_auc': 0.7785891114018315, 'pr_auc': 0.7345330237975679, 'conicity_mean': 0.15901005, 'conicity_std': 0.050756335}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.645    0.722      0.688      0.684         0.682
precision    0.799    0.627      0.688      0.713         0.718
recall       0.541    0.851      0.688      0.696         0.688
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7785891114018315
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:50:37_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.599, BCE loss: 0.505, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 3 Step: 1 Total Loss: 0.649, BCE loss: 0.571, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 3 Step: 2 Total Loss: 0.543, BCE loss: 0.468, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 3 Step: 3 Total Loss: 0.510, BCE loss: 0.401, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 3 Step: 4 Total Loss: 0.405, BCE loss: 0.316, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 3 Step: 5 Total Loss: 0.537, BCE loss: 0.467, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 3 Step: 6 Total Loss: 0.678, BCE loss: 0.538, Diversity Loss: 0.280                     (Diversity_weight = 0.5)
Epoch: 3 Step: 7 Total Loss: 0.507, BCE loss: 0.433, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 3 Step: 8 Total Loss: 0.584, BCE loss: 0.515, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 3 Step: 9 Total Loss: 0.541, BCE loss: 0.474, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 3 Step: 10 Total Loss: 0.621, BCE loss: 0.528, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 3 Step: 11 Total Loss: 0.425, BCE loss: 0.346, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 3 Step: 12 Total Loss: 0.499, BCE loss: 0.399, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 3 Step: 13 Total Loss: 0.477, BCE loss: 0.401, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 3 Step: 14 Total Loss: 0.510, BCE loss: 0.453, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 3 Step: 15 Total Loss: 0.406, BCE loss: 0.336, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 3 Step: 16 Total Loss: 0.412, BCE loss: 0.350, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 3 Step: 17 Total Loss: 0.541, BCE loss: 0.454, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 3 Step: 18 Total Loss: 0.602, BCE loss: 0.494, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 3 Step: 19 Total Loss: 0.484, BCE loss: 0.400, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 3 Step: 20 Total Loss: 0.413, BCE loss: 0.300, Diversity Loss: 0.226                     (Diversity_weight = 0.5)
Epoch: 3 Step: 21 Total Loss: 0.542, BCE loss: 0.475, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 3 Step: 22 Total Loss: 0.422, BCE loss: 0.370, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 3 Step: 23 Total Loss: 0.457, BCE loss: 0.383, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 3 Step: 24 Total Loss: 0.364, BCE loss: 0.295, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 3 Step: 25 Total Loss: 0.519, BCE loss: 0.444, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 26 Total Loss: 0.335, BCE loss: 0.236, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 3 Step: 27 Total Loss: 0.633, BCE loss: 0.539, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 3 Step: 28 Total Loss: 0.501, BCE loss: 0.434, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 3 Step: 29 Total Loss: 0.482, BCE loss: 0.412, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 3 Step: 30 Total Loss: 0.426, BCE loss: 0.347, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 3 Step: 31 Total Loss: 0.381, BCE loss: 0.330, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 3 Step: 32 Total Loss: 0.526, BCE loss: 0.439, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 3 Step: 33 Total Loss: 0.476, BCE loss: 0.416, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 3 Step: 34 Total Loss: 0.514, BCE loss: 0.435, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 3 Step: 35 Total Loss: 0.585, BCE loss: 0.485, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 3 Step: 36 Total Loss: 0.514, BCE loss: 0.428, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 3 Step: 37 Total Loss: 0.601, BCE loss: 0.541, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 3 Step: 38 Total Loss: 0.441, BCE loss: 0.373, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 3 Step: 39 Total Loss: 0.504, BCE loss: 0.440, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 3 Step: 40 Total Loss: 0.400, BCE loss: 0.331, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 3 Step: 41 Total Loss: 0.458, BCE loss: 0.386, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 3 Step: 42 Total Loss: 0.445, BCE loss: 0.387, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 3 Step: 43 Total Loss: 0.404, BCE loss: 0.344, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 3 Step: 44 Total Loss: 0.741, BCE loss: 0.589, Diversity Loss: 0.304                     (Diversity_weight = 0.5)
Epoch: 3 Step: 45 Total Loss: 0.635, BCE loss: 0.559, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 3 Step: 46 Total Loss: 0.370, BCE loss: 0.292, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 3 Step: 47 Total Loss: 0.593, BCE loss: 0.546, Diversity Loss: 0.094                     (Diversity_weight = 0.5)
Epoch: 3 Step: 48 Total Loss: 0.526, BCE loss: 0.447, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 3 Step: 49 Total Loss: 0.391, BCE loss: 0.310, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 3 Step: 50 Total Loss: 0.611, BCE loss: 0.506, Diversity Loss: 0.210                     (Diversity_weight = 0.5)
Epoch: 3 Step: 51 Total Loss: 0.653, BCE loss: 0.598, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 3 Step: 52 Total Loss: 0.735, BCE loss: 0.658, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 3 Step: 53 Total Loss: 0.411, BCE loss: 0.336, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 3 Step: 54 Total Loss: 0.518, BCE loss: 0.459, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 3 Step: 55 Total Loss: 0.530, BCE loss: 0.456, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 3 Step: 56 Total Loss: 0.453, BCE loss: 0.389, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 3 Step: 57 Total Loss: 0.543, BCE loss: 0.483, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 3 Step: 58 Total Loss: 0.417, BCE loss: 0.337, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 3 Step: 59 Total Loss: 0.468, BCE loss: 0.398, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 3 Step: 60 Total Loss: 0.479, BCE loss: 0.412, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 3 Step: 61 Total Loss: 0.451, BCE loss: 0.380, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 3 Step: 62 Total Loss: 0.499, BCE loss: 0.373, Diversity Loss: 0.253                     (Diversity_weight = 0.5)
{'accuracy': 0.7114228456913828, 'roc_auc': 0.7843936801851665, 'pr_auc': 0.7430972491363153, 'conicity_mean': 0.15500481, 'conicity_std': 0.047328167}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.684    0.735      0.711      0.709         0.708
precision    0.804    0.653      0.711      0.728         0.732
recall       0.595    0.840      0.711      0.717         0.711
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7843936801851665
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:50:37_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.300, BCE loss: 0.206, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 4 Step: 1 Total Loss: 0.556, BCE loss: 0.471, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 4 Step: 2 Total Loss: 0.312, BCE loss: 0.226, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 4 Step: 3 Total Loss: 0.490, BCE loss: 0.418, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 4 Step: 4 Total Loss: 0.455, BCE loss: 0.408, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 4 Step: 5 Total Loss: 0.421, BCE loss: 0.360, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 4 Step: 6 Total Loss: 0.418, BCE loss: 0.342, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 4 Step: 7 Total Loss: 0.319, BCE loss: 0.246, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 4 Step: 8 Total Loss: 0.328, BCE loss: 0.250, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 4 Step: 9 Total Loss: 0.437, BCE loss: 0.360, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 4 Step: 10 Total Loss: 0.526, BCE loss: 0.453, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 4 Step: 11 Total Loss: 0.445, BCE loss: 0.372, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 4 Step: 12 Total Loss: 0.361, BCE loss: 0.255, Diversity Loss: 0.212                     (Diversity_weight = 0.5)
Epoch: 4 Step: 13 Total Loss: 0.327, BCE loss: 0.261, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 4 Step: 14 Total Loss: 0.331, BCE loss: 0.261, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 4 Step: 15 Total Loss: 0.442, BCE loss: 0.369, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 4 Step: 16 Total Loss: 0.417, BCE loss: 0.333, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 4 Step: 17 Total Loss: 0.473, BCE loss: 0.410, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 4 Step: 18 Total Loss: 0.450, BCE loss: 0.372, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 4 Step: 19 Total Loss: 0.428, BCE loss: 0.348, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 4 Step: 20 Total Loss: 0.341, BCE loss: 0.282, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 4 Step: 21 Total Loss: 0.480, BCE loss: 0.399, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 4 Step: 22 Total Loss: 0.319, BCE loss: 0.240, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 4 Step: 23 Total Loss: 0.497, BCE loss: 0.427, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 4 Step: 24 Total Loss: 0.295, BCE loss: 0.232, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 4 Step: 25 Total Loss: 0.457, BCE loss: 0.359, Diversity Loss: 0.197                     (Diversity_weight = 0.5)
Epoch: 4 Step: 26 Total Loss: 0.338, BCE loss: 0.274, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 4 Step: 27 Total Loss: 0.377, BCE loss: 0.319, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 4 Step: 28 Total Loss: 0.617, BCE loss: 0.470, Diversity Loss: 0.294                     (Diversity_weight = 0.5)
Epoch: 4 Step: 29 Total Loss: 0.403, BCE loss: 0.344, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 4 Step: 30 Total Loss: 0.259, BCE loss: 0.198, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 4 Step: 31 Total Loss: 0.537, BCE loss: 0.475, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 4 Step: 32 Total Loss: 0.343, BCE loss: 0.286, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 4 Step: 33 Total Loss: 0.299, BCE loss: 0.227, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 4 Step: 34 Total Loss: 0.446, BCE loss: 0.352, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 4 Step: 35 Total Loss: 0.361, BCE loss: 0.276, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 4 Step: 36 Total Loss: 0.247, BCE loss: 0.178, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 4 Step: 37 Total Loss: 0.393, BCE loss: 0.327, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 4 Step: 38 Total Loss: 0.559, BCE loss: 0.479, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 4 Step: 39 Total Loss: 0.331, BCE loss: 0.259, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 4 Step: 40 Total Loss: 0.539, BCE loss: 0.471, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 4 Step: 41 Total Loss: 0.333, BCE loss: 0.215, Diversity Loss: 0.236                     (Diversity_weight = 0.5)
Epoch: 4 Step: 42 Total Loss: 0.406, BCE loss: 0.331, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 4 Step: 43 Total Loss: 0.361, BCE loss: 0.305, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 4 Step: 44 Total Loss: 0.297, BCE loss: 0.209, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 4 Step: 45 Total Loss: 0.533, BCE loss: 0.415, Diversity Loss: 0.236                     (Diversity_weight = 0.5)
Epoch: 4 Step: 46 Total Loss: 0.422, BCE loss: 0.325, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 4 Step: 47 Total Loss: 0.317, BCE loss: 0.247, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 4 Step: 48 Total Loss: 0.611, BCE loss: 0.475, Diversity Loss: 0.271                     (Diversity_weight = 0.5)
Epoch: 4 Step: 49 Total Loss: 0.382, BCE loss: 0.323, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 4 Step: 50 Total Loss: 0.343, BCE loss: 0.271, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 4 Step: 51 Total Loss: 0.467, BCE loss: 0.390, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 4 Step: 52 Total Loss: 0.451, BCE loss: 0.388, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 4 Step: 53 Total Loss: 0.494, BCE loss: 0.439, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 4 Step: 54 Total Loss: 0.413, BCE loss: 0.308, Diversity Loss: 0.211                     (Diversity_weight = 0.5)
Epoch: 4 Step: 55 Total Loss: 0.331, BCE loss: 0.266, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 4 Step: 56 Total Loss: 0.333, BCE loss: 0.273, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 4 Step: 57 Total Loss: 0.389, BCE loss: 0.286, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 4 Step: 58 Total Loss: 0.410, BCE loss: 0.328, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 4 Step: 59 Total Loss: 0.543, BCE loss: 0.455, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 4 Step: 60 Total Loss: 0.405, BCE loss: 0.336, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 4 Step: 61 Total Loss: 0.265, BCE loss: 0.220, Diversity Loss: 0.091                     (Diversity_weight = 0.5)
Epoch: 4 Step: 62 Total Loss: 0.324, BCE loss: 0.243, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
{'accuracy': 0.7224448897795591, 'roc_auc': 0.771037536479823, 'pr_auc': 0.7275505584909459, 'conicity_mean': 0.15001503, 'conicity_std': 0.048488975}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.740    0.702      0.722      0.721         0.722
precision    0.726    0.718      0.722      0.722         0.722
recall       0.755    0.686      0.722      0.721         0.722
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.771037536479823
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:50:37_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.297, BCE loss: 0.237, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 5 Step: 1 Total Loss: 0.239, BCE loss: 0.129, Diversity Loss: 0.220                     (Diversity_weight = 0.5)
Epoch: 5 Step: 2 Total Loss: 0.398, BCE loss: 0.267, Diversity Loss: 0.264                     (Diversity_weight = 0.5)
Epoch: 5 Step: 3 Total Loss: 0.302, BCE loss: 0.217, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 5 Step: 4 Total Loss: 0.258, BCE loss: 0.198, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 5 Step: 5 Total Loss: 0.215, BCE loss: 0.115, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 5 Step: 6 Total Loss: 0.170, BCE loss: 0.102, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 5 Step: 7 Total Loss: 0.210, BCE loss: 0.151, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 5 Step: 8 Total Loss: 0.261, BCE loss: 0.191, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 5 Step: 9 Total Loss: 0.314, BCE loss: 0.252, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 5 Step: 10 Total Loss: 0.353, BCE loss: 0.270, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 5 Step: 11 Total Loss: 0.318, BCE loss: 0.262, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 5 Step: 12 Total Loss: 0.288, BCE loss: 0.195, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 5 Step: 13 Total Loss: 0.317, BCE loss: 0.264, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 5 Step: 14 Total Loss: 0.364, BCE loss: 0.301, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 5 Step: 15 Total Loss: 0.203, BCE loss: 0.128, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 5 Step: 16 Total Loss: 0.274, BCE loss: 0.178, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 5 Step: 17 Total Loss: 0.175, BCE loss: 0.104, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 5 Step: 18 Total Loss: 0.343, BCE loss: 0.270, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 5 Step: 19 Total Loss: 0.162, BCE loss: 0.103, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 5 Step: 20 Total Loss: 0.286, BCE loss: 0.217, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 5 Step: 21 Total Loss: 0.389, BCE loss: 0.313, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 5 Step: 22 Total Loss: 0.175, BCE loss: 0.116, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 5 Step: 23 Total Loss: 0.239, BCE loss: 0.174, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 5 Step: 24 Total Loss: 0.311, BCE loss: 0.219, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 5 Step: 25 Total Loss: 0.254, BCE loss: 0.172, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 5 Step: 26 Total Loss: 0.414, BCE loss: 0.341, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 5 Step: 27 Total Loss: 0.187, BCE loss: 0.121, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 5 Step: 28 Total Loss: 0.308, BCE loss: 0.228, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 5 Step: 29 Total Loss: 0.276, BCE loss: 0.208, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 5 Step: 30 Total Loss: 0.305, BCE loss: 0.226, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 5 Step: 31 Total Loss: 0.279, BCE loss: 0.222, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 5 Step: 32 Total Loss: 0.237, BCE loss: 0.176, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 5 Step: 33 Total Loss: 0.210, BCE loss: 0.153, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 5 Step: 34 Total Loss: 0.215, BCE loss: 0.133, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 5 Step: 35 Total Loss: 0.166, BCE loss: 0.097, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 5 Step: 36 Total Loss: 0.141, BCE loss: 0.096, Diversity Loss: 0.090                     (Diversity_weight = 0.5)
Epoch: 5 Step: 37 Total Loss: 0.269, BCE loss: 0.213, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 5 Step: 38 Total Loss: 0.217, BCE loss: 0.144, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 5 Step: 39 Total Loss: 0.292, BCE loss: 0.215, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 5 Step: 40 Total Loss: 0.463, BCE loss: 0.381, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 5 Step: 41 Total Loss: 0.233, BCE loss: 0.177, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 5 Step: 42 Total Loss: 0.598, BCE loss: 0.449, Diversity Loss: 0.296                     (Diversity_weight = 0.5)
Epoch: 5 Step: 43 Total Loss: 0.333, BCE loss: 0.265, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 5 Step: 44 Total Loss: 0.381, BCE loss: 0.304, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 5 Step: 45 Total Loss: 0.215, BCE loss: 0.110, Diversity Loss: 0.210                     (Diversity_weight = 0.5)
Epoch: 5 Step: 46 Total Loss: 0.408, BCE loss: 0.331, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 5 Step: 47 Total Loss: 0.160, BCE loss: 0.088, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 5 Step: 48 Total Loss: 0.328, BCE loss: 0.237, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 5 Step: 49 Total Loss: 0.305, BCE loss: 0.193, Diversity Loss: 0.224                     (Diversity_weight = 0.5)
Epoch: 5 Step: 50 Total Loss: 0.461, BCE loss: 0.394, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 5 Step: 51 Total Loss: 0.327, BCE loss: 0.256, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 5 Step: 52 Total Loss: 0.210, BCE loss: 0.125, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 5 Step: 53 Total Loss: 0.495, BCE loss: 0.376, Diversity Loss: 0.240                     (Diversity_weight = 0.5)
Epoch: 5 Step: 54 Total Loss: 0.261, BCE loss: 0.183, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 5 Step: 55 Total Loss: 0.222, BCE loss: 0.154, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 5 Step: 56 Total Loss: 0.334, BCE loss: 0.236, Diversity Loss: 0.197                     (Diversity_weight = 0.5)
Epoch: 5 Step: 57 Total Loss: 0.192, BCE loss: 0.119, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 5 Step: 58 Total Loss: 0.283, BCE loss: 0.218, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 5 Step: 59 Total Loss: 0.378, BCE loss: 0.299, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 5 Step: 60 Total Loss: 0.257, BCE loss: 0.175, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 5 Step: 61 Total Loss: 0.552, BCE loss: 0.507, Diversity Loss: 0.092                     (Diversity_weight = 0.5)
Epoch: 5 Step: 62 Total Loss: 0.435, BCE loss: 0.349, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
{'accuracy': 0.7024048096192385, 'roc_auc': 0.7535151454161216, 'pr_auc': 0.7064156714992009, 'conicity_mean': 0.15084472, 'conicity_std': 0.04825788}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.707    0.697      0.702      0.702         0.703
precision    0.730    0.676      0.702      0.703         0.704
recall       0.686    0.720      0.702      0.703         0.702
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7535151454161216
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:50:37_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.192, BCE loss: 0.129, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 6 Step: 1 Total Loss: 0.214, BCE loss: 0.146, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 6 Step: 2 Total Loss: 0.214, BCE loss: 0.133, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 6 Step: 3 Total Loss: 0.287, BCE loss: 0.157, Diversity Loss: 0.260                     (Diversity_weight = 0.5)
Epoch: 6 Step: 4 Total Loss: 0.241, BCE loss: 0.172, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 6 Step: 5 Total Loss: 0.181, BCE loss: 0.116, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 6 Step: 6 Total Loss: 0.308, BCE loss: 0.222, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 6 Step: 7 Total Loss: 0.232, BCE loss: 0.144, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 6 Step: 8 Total Loss: 0.361, BCE loss: 0.289, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 6 Step: 9 Total Loss: 0.254, BCE loss: 0.134, Diversity Loss: 0.240                     (Diversity_weight = 0.5)
Epoch: 6 Step: 10 Total Loss: 0.143, BCE loss: 0.089, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 6 Step: 11 Total Loss: 0.224, BCE loss: 0.143, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 6 Step: 12 Total Loss: 0.331, BCE loss: 0.274, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 6 Step: 13 Total Loss: 0.170, BCE loss: 0.108, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 6 Step: 14 Total Loss: 0.410, BCE loss: 0.354, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 6 Step: 15 Total Loss: 0.399, BCE loss: 0.250, Diversity Loss: 0.298                     (Diversity_weight = 0.5)
Epoch: 6 Step: 16 Total Loss: 0.238, BCE loss: 0.175, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 6 Step: 17 Total Loss: 0.220, BCE loss: 0.127, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 6 Step: 18 Total Loss: 0.208, BCE loss: 0.131, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 6 Step: 19 Total Loss: 0.307, BCE loss: 0.207, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 6 Step: 20 Total Loss: 0.458, BCE loss: 0.385, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 6 Step: 21 Total Loss: 0.265, BCE loss: 0.195, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 6 Step: 22 Total Loss: 0.344, BCE loss: 0.276, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 6 Step: 23 Total Loss: 0.232, BCE loss: 0.163, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 6 Step: 24 Total Loss: 0.172, BCE loss: 0.114, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 6 Step: 25 Total Loss: 0.352, BCE loss: 0.235, Diversity Loss: 0.234                     (Diversity_weight = 0.5)
Epoch: 6 Step: 26 Total Loss: 0.171, BCE loss: 0.104, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 6 Step: 27 Total Loss: 0.182, BCE loss: 0.095, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 6 Step: 28 Total Loss: 0.307, BCE loss: 0.243, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 6 Step: 29 Total Loss: 0.194, BCE loss: 0.100, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 6 Step: 30 Total Loss: 0.143, BCE loss: 0.077, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 6 Step: 31 Total Loss: 0.260, BCE loss: 0.189, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 6 Step: 32 Total Loss: 0.314, BCE loss: 0.217, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 6 Step: 33 Total Loss: 0.228, BCE loss: 0.153, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 6 Step: 34 Total Loss: 0.219, BCE loss: 0.113, Diversity Loss: 0.211                     (Diversity_weight = 0.5)
Epoch: 6 Step: 35 Total Loss: 0.126, BCE loss: 0.062, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 6 Step: 36 Total Loss: 0.123, BCE loss: 0.043, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 6 Step: 37 Total Loss: 0.133, BCE loss: 0.051, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 6 Step: 38 Total Loss: 0.167, BCE loss: 0.110, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 6 Step: 39 Total Loss: 0.257, BCE loss: 0.187, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 6 Step: 40 Total Loss: 0.242, BCE loss: 0.131, Diversity Loss: 0.223                     (Diversity_weight = 0.5)
Epoch: 6 Step: 41 Total Loss: 0.245, BCE loss: 0.186, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 6 Step: 42 Total Loss: 0.282, BCE loss: 0.197, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 6 Step: 43 Total Loss: 0.132, BCE loss: 0.066, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 6 Step: 44 Total Loss: 0.161, BCE loss: 0.089, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 6 Step: 45 Total Loss: 0.155, BCE loss: 0.096, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 6 Step: 46 Total Loss: 0.161, BCE loss: 0.097, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 6 Step: 47 Total Loss: 0.110, BCE loss: 0.037, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 6 Step: 48 Total Loss: 0.238, BCE loss: 0.153, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 6 Step: 49 Total Loss: 0.149, BCE loss: 0.093, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 6 Step: 50 Total Loss: 0.163, BCE loss: 0.073, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 6 Step: 51 Total Loss: 0.242, BCE loss: 0.196, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 6 Step: 52 Total Loss: 0.208, BCE loss: 0.129, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 6 Step: 53 Total Loss: 0.123, BCE loss: 0.053, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 6 Step: 54 Total Loss: 0.103, BCE loss: 0.059, Diversity Loss: 0.088                     (Diversity_weight = 0.5)
Epoch: 6 Step: 55 Total Loss: 0.112, BCE loss: 0.050, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 6 Step: 56 Total Loss: 0.156, BCE loss: 0.078, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 6 Step: 57 Total Loss: 0.501, BCE loss: 0.409, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 6 Step: 58 Total Loss: 0.196, BCE loss: 0.145, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 6 Step: 59 Total Loss: 0.162, BCE loss: 0.087, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 6 Step: 60 Total Loss: 0.099, BCE loss: 0.032, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 6 Step: 61 Total Loss: 0.177, BCE loss: 0.099, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 6 Step: 62 Total Loss: 0.161, BCE loss: 0.091, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
{'accuracy': 0.6993987975951904, 'roc_auc': 0.7658931266981988, 'pr_auc': 0.7263263553299704, 'conicity_mean': 0.15514073, 'conicity_std': 0.051178023}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.718    0.678      0.699      0.698         0.699
precision    0.706    0.691      0.699      0.699         0.699
recall       0.730    0.665      0.699      0.698         0.699
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7658931266981988
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:50:37_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.236, BCE loss: 0.177, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 7 Step: 1 Total Loss: 0.172, BCE loss: 0.098, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 7 Step: 2 Total Loss: 0.210, BCE loss: 0.139, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 7 Step: 3 Total Loss: 0.247, BCE loss: 0.158, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 7 Step: 4 Total Loss: 0.113, BCE loss: 0.064, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 7 Step: 5 Total Loss: 0.236, BCE loss: 0.154, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 7 Step: 6 Total Loss: 0.191, BCE loss: 0.125, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 7 Step: 7 Total Loss: 0.186, BCE loss: 0.131, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 7 Step: 8 Total Loss: 0.188, BCE loss: 0.113, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 7 Step: 9 Total Loss: 0.110, BCE loss: 0.050, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 7 Step: 10 Total Loss: 0.093, BCE loss: 0.031, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 7 Step: 11 Total Loss: 0.208, BCE loss: 0.142, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 7 Step: 12 Total Loss: 0.121, BCE loss: 0.056, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 7 Step: 13 Total Loss: 0.164, BCE loss: 0.091, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 7 Step: 14 Total Loss: 0.130, BCE loss: 0.049, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 7 Step: 15 Total Loss: 0.204, BCE loss: 0.147, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 7 Step: 16 Total Loss: 0.368, BCE loss: 0.311, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 7 Step: 17 Total Loss: 0.204, BCE loss: 0.147, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 7 Step: 18 Total Loss: 0.365, BCE loss: 0.233, Diversity Loss: 0.263                     (Diversity_weight = 0.5)
Epoch: 7 Step: 19 Total Loss: 0.115, BCE loss: 0.022, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 7 Step: 20 Total Loss: 0.314, BCE loss: 0.248, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 7 Step: 21 Total Loss: 0.231, BCE loss: 0.163, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 7 Step: 22 Total Loss: 0.150, BCE loss: 0.072, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 7 Step: 23 Total Loss: 0.380, BCE loss: 0.233, Diversity Loss: 0.292                     (Diversity_weight = 0.5)
Epoch: 7 Step: 24 Total Loss: 0.232, BCE loss: 0.109, Diversity Loss: 0.247                     (Diversity_weight = 0.5)
Epoch: 7 Step: 25 Total Loss: 0.224, BCE loss: 0.138, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 7 Step: 26 Total Loss: 0.107, BCE loss: 0.039, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 7 Step: 27 Total Loss: 0.160, BCE loss: 0.090, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 7 Step: 28 Total Loss: 0.159, BCE loss: 0.077, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 7 Step: 29 Total Loss: 0.152, BCE loss: 0.061, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 7 Step: 30 Total Loss: 0.223, BCE loss: 0.150, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 7 Step: 31 Total Loss: 0.146, BCE loss: 0.101, Diversity Loss: 0.089                     (Diversity_weight = 0.5)
Epoch: 7 Step: 32 Total Loss: 0.221, BCE loss: 0.159, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 7 Step: 33 Total Loss: 0.326, BCE loss: 0.245, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 7 Step: 34 Total Loss: 0.110, BCE loss: 0.039, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 7 Step: 35 Total Loss: 0.116, BCE loss: 0.051, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 7 Step: 36 Total Loss: 0.146, BCE loss: 0.096, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 7 Step: 37 Total Loss: 0.184, BCE loss: 0.080, Diversity Loss: 0.208                     (Diversity_weight = 0.5)
Epoch: 7 Step: 38 Total Loss: 0.155, BCE loss: 0.083, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 7 Step: 39 Total Loss: 0.119, BCE loss: 0.059, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 7 Step: 40 Total Loss: 0.144, BCE loss: 0.082, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 7 Step: 41 Total Loss: 0.135, BCE loss: 0.062, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 7 Step: 42 Total Loss: 0.092, BCE loss: 0.027, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 7 Step: 43 Total Loss: 0.166, BCE loss: 0.081, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 7 Step: 44 Total Loss: 0.288, BCE loss: 0.198, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 7 Step: 45 Total Loss: 0.183, BCE loss: 0.111, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 7 Step: 46 Total Loss: 0.267, BCE loss: 0.194, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 7 Step: 47 Total Loss: 0.154, BCE loss: 0.090, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 7 Step: 48 Total Loss: 0.079, BCE loss: 0.025, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 7 Step: 49 Total Loss: 0.210, BCE loss: 0.115, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 7 Step: 50 Total Loss: 0.208, BCE loss: 0.094, Diversity Loss: 0.228                     (Diversity_weight = 0.5)
Epoch: 7 Step: 51 Total Loss: 0.118, BCE loss: 0.023, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 7 Step: 52 Total Loss: 0.079, BCE loss: 0.011, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 7 Step: 53 Total Loss: 0.278, BCE loss: 0.200, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 7 Step: 54 Total Loss: 0.235, BCE loss: 0.170, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 7 Step: 55 Total Loss: 0.116, BCE loss: 0.060, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 7 Step: 56 Total Loss: 0.137, BCE loss: 0.054, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 7 Step: 57 Total Loss: 0.181, BCE loss: 0.070, Diversity Loss: 0.223                     (Diversity_weight = 0.5)
Epoch: 7 Step: 58 Total Loss: 0.264, BCE loss: 0.203, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 7 Step: 59 Total Loss: 0.220, BCE loss: 0.122, Diversity Loss: 0.198                     (Diversity_weight = 0.5)
Epoch: 7 Step: 60 Total Loss: 0.143, BCE loss: 0.072, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 7 Step: 61 Total Loss: 0.254, BCE loss: 0.174, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 7 Step: 62 Total Loss: 0.108, BCE loss: 0.034, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
{'accuracy': 0.7084168336673347, 'roc_auc': 0.7686625742175708, 'pr_auc': 0.7165184646167884, 'conicity_mean': 0.15278208, 'conicity_std': 0.04978997}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.737    0.673      0.708      0.705         0.707
precision    0.699    0.721      0.708      0.710         0.710
recall       0.778    0.632      0.708      0.705         0.708
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7686625742175708
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:50:37_2021
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:51:24,075 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:51:24,075 - type = vanillalstm
INFO - 2021-01-17 19:51:24,075 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:51:24,075 - vocab_size = 548
INFO - 2021-01-17 19:51:24,076 - embed_size = 200
INFO - 2021-01-17 19:51:24,076 - hidden_size = 128
INFO - 2021-01-17 19:51:24,076 - pre_embed = None
INFO - 2021-01-17 19:51:24,090 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:51:24,091 - hidden_size = 256
INFO - 2021-01-17 19:51:24,091 - output_size = 1
INFO - 2021-01-17 19:51:24,091 - use_attention = True
INFO - 2021-01-17 19:51:24,091 - regularizer_attention = None
INFO - 2021-01-17 19:51:24,091 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b2e2daeb090> and extras set()
INFO - 2021-01-17 19:51:24,091 - attention.type = tanh
INFO - 2021-01-17 19:51:24,091 - type = tanh
INFO - 2021-01-17 19:51:24,091 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b2e2daeb090> and extras set()
INFO - 2021-01-17 19:51:24,091 - attention.hidden_size = 256
INFO - 2021-01-17 19:51:24,091 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7197197197197197, 'roc_auc': 0.8013700281237444, 'pr_auc': 0.8119240630778276, 'conicity_mean': '0.15208489', 'conicity_std': '0.04399081'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.672    0.755       0.72      0.714         0.716
precision    0.757    0.697       0.72      0.727         0.726
recall       0.604    0.824       0.72      0.714         0.720
support    475.000  524.000     999.00    999.000       999.000
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:51:25,350 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:51:25,350 - type = vanillalstm
INFO - 2021-01-17 19:51:25,351 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:51:25,351 - vocab_size = 548
INFO - 2021-01-17 19:51:25,351 - embed_size = 200
INFO - 2021-01-17 19:51:25,351 - hidden_size = 128
INFO - 2021-01-17 19:51:25,351 - pre_embed = None
INFO - 2021-01-17 19:51:25,365 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:51:25,365 - hidden_size = 256
INFO - 2021-01-17 19:51:25,365 - output_size = 1
INFO - 2021-01-17 19:51:25,365 - use_attention = True
INFO - 2021-01-17 19:51:25,365 - regularizer_attention = None
INFO - 2021-01-17 19:51:25,365 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b2e2da8af10> and extras set()
INFO - 2021-01-17 19:51:25,365 - attention.type = tanh
INFO - 2021-01-17 19:51:25,365 - type = tanh
INFO - 2021-01-17 19:51:25,365 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b2e2da8af10> and extras set()
INFO - 2021-01-17 19:51:25,365 - attention.hidden_size = 256
INFO - 2021-01-17 19:51:25,366 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7197197197197197, 'roc_auc': 0.8013700281237444, 'pr_auc': 0.8119240630778276, 'conicity_mean': '0.15208489', 'conicity_std': '0.04399081'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.672    0.755       0.72      0.714         0.716
precision    0.757    0.697       0.72      0.727         0.726
recall       0.604    0.824       0.72      0.714         0.720
support    475.000  524.000     999.00    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 28, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

==============================================================================
Running epilogue script on indigo51.

Submit time  : 2021-01-17T19:33:50
Start time   : 2021-01-17T19:34:49
End time     : 2021-01-17T19:51:20
Elapsed time : 00:16:31 (Timelimit=1-06:00:00)

Job ID: 1248475
Cluster: i5
User/Group: kjk1n18/fp
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:16:17
CPU Efficiency: 98.59% of 00:16:31 core-walltime
Job Wall-clock time: 00:16:31
Memory Utilized: 2.23 GB
Memory Efficiency: 0.00% of 0.00 MB

