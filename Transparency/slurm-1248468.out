Running SLURM prolog script on indigo51.cluster.local
===============================================================================
Job started on Sun Jan 17 19:11:32 GMT 2021
Job ID          : 1248468
Job name        : run_all_non_english.sh
WorkDir         : /mainfs/home/kjk1n18/Transparency/Transparency
Command         : /mainfs/home/kjk1n18/Transparency/Transparency/batch/run_all_non_english.sh
Partition       : gpu
Num hosts       : 1
Num cores       : 1
Num of tasks    : 1
Hosts allocated : indigo51
Job Output Follows ...
===============================================================================
/tmp/slurmd/job1248468/slurm_script: line 12: activate: No such file or directory
cls_en vanilla_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:11:37,964 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:11:37,964 - type = vanillalstm
INFO - 2021-01-17 19:11:37,965 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:11:38,518 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:11:43,269 - vocab_size = 666
INFO - 2021-01-17 19:11:43,270 - embed_size = 200
INFO - 2021-01-17 19:11:43,270 - hidden_size = 128
INFO - 2021-01-17 19:11:43,270 - pre_embed = None
INFO - 2021-01-17 19:11:50,309 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:11:50,309 - hidden_size = 256
INFO - 2021-01-17 19:11:50,310 - output_size = 1
INFO - 2021-01-17 19:11:50,310 - use_attention = True
INFO - 2021-01-17 19:11:50,310 - regularizer_attention = None
INFO - 2021-01-17 19:11:50,310 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b6cff0988d0> and extras set()
INFO - 2021-01-17 19:11:50,310 - attention.type = tanh
INFO - 2021-01-17 19:11:50,310 - type = tanh
INFO - 2021-01-17 19:11:50,310 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b6cff0988d0> and extras set()
INFO - 2021-01-17 19:11:50,310 - attention.hidden_size = 256
INFO - 2021-01-17 19:11:50,311 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.430                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.451                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.416                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.703, BCE loss: 0.703, Diversity Loss: 0.441                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.724, BCE loss: 0.724, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.713, BCE loss: 0.713, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.442                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.411                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.456                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.442                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.664, BCE loss: 0.664, Diversity Loss: 0.416                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.681, BCE loss: 0.681, Diversity Loss: 0.439                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.412                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.458                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.415                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.615, BCE loss: 0.615, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.707, BCE loss: 0.707, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.639, BCE loss: 0.639, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.651, BCE loss: 0.651, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.610, BCE loss: 0.610, Diversity Loss: 0.457                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.614, BCE loss: 0.614, Diversity Loss: 0.411                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.435                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.617, BCE loss: 0.617, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.548, BCE loss: 0.548, Diversity Loss: 0.434                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.851, BCE loss: 0.851, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.727, BCE loss: 0.727, Diversity Loss: 0.470                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.523, BCE loss: 0.523, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.639, BCE loss: 0.639, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.745, BCE loss: 0.745, Diversity Loss: 0.413                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.621, BCE loss: 0.621, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.485, BCE loss: 0.485, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.537, BCE loss: 0.537, Diversity Loss: 0.650                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.456                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.499, BCE loss: 0.499, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.606, BCE loss: 0.606, Diversity Loss: 0.410                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.578, BCE loss: 0.578, Diversity Loss: 0.583                     (Diversity_weight = 0)
{'accuracy': 0.6676676676676677, 'roc_auc': 0.7528270419785226, 'pr_auc': 0.7899731202050889, 'conicity_mean': 0.48854092, 'conicity_std': 0.11355838}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.652    0.682      0.668      0.667         0.669
precision    0.604    0.736      0.668      0.670         0.678
recall       0.708    0.636      0.668      0.672         0.668
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7528270419785226
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:11:51_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.552, BCE loss: 0.552, Diversity Loss: 0.466                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.593, BCE loss: 0.593, Diversity Loss: 0.429                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.510, BCE loss: 0.510, Diversity Loss: 0.445                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.521, BCE loss: 0.521, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.504, BCE loss: 0.504, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.306, BCE loss: 0.306, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.550, BCE loss: 0.550, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.355, BCE loss: 0.355, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.509, BCE loss: 0.509, Diversity Loss: 0.661                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.517, BCE loss: 0.517, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.626, BCE loss: 0.626, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.600, BCE loss: 0.600, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.491, BCE loss: 0.491, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.665, BCE loss: 0.665, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.748, BCE loss: 0.748, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.725, BCE loss: 0.725, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.478, BCE loss: 0.478, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.606, BCE loss: 0.606, Diversity Loss: 0.432                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.607, BCE loss: 0.607, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.554, BCE loss: 0.554, Diversity Loss: 0.424                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.654, BCE loss: 0.654, Diversity Loss: 0.460                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.403                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.602, BCE loss: 0.602, Diversity Loss: 0.418                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.575, BCE loss: 0.575, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.462, BCE loss: 0.462, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.644, BCE loss: 0.644, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.602, BCE loss: 0.602, Diversity Loss: 0.400                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.592, BCE loss: 0.592, Diversity Loss: 0.470                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.709, BCE loss: 0.709, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.534, BCE loss: 0.534, Diversity Loss: 0.443                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.529, BCE loss: 0.529, Diversity Loss: 0.457                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.568, BCE loss: 0.568, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.736, BCE loss: 0.736, Diversity Loss: 0.420                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.545, BCE loss: 0.545, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.435, BCE loss: 0.435, Diversity Loss: 0.439                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.384, BCE loss: 0.384, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.451                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.575, BCE loss: 0.575, Diversity Loss: 0.406                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.556, BCE loss: 0.556, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.423                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.534, BCE loss: 0.534, Diversity Loss: 0.612                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.557, BCE loss: 0.557, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.565, BCE loss: 0.565, Diversity Loss: 0.642                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.475, BCE loss: 0.475, Diversity Loss: 0.471                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.557, BCE loss: 0.557, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.628, BCE loss: 0.628, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.526, BCE loss: 0.526, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.434, BCE loss: 0.434, Diversity Loss: 0.470                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.473, BCE loss: 0.473, Diversity Loss: 0.460                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.652, BCE loss: 0.652, Diversity Loss: 0.678                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.482, BCE loss: 0.482, Diversity Loss: 0.537                     (Diversity_weight = 0)
{'accuracy': 0.7097097097097097, 'roc_auc': 0.7824886104783599, 'pr_auc': 0.8280847786984873, 'conicity_mean': 0.53079265, 'conicity_std': 0.12820692}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.673    0.739       0.71      0.706          0.71
precision    0.667    0.745       0.71      0.706          0.71
recall       0.679    0.734       0.71      0.706          0.71
support    439.000  560.000     999.00    999.000        999.00
Model Saved on  roc_auc 0.7824886104783599
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:11:51_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.598, BCE loss: 0.598, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.479, BCE loss: 0.479, Diversity Loss: 0.436                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.450, BCE loss: 0.450, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.556, BCE loss: 0.556, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.458, BCE loss: 0.458, Diversity Loss: 0.694                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.327, BCE loss: 0.327, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.331, BCE loss: 0.331, Diversity Loss: 0.743                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.600, BCE loss: 0.600, Diversity Loss: 0.658                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.432, BCE loss: 0.432, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.466, BCE loss: 0.466, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.420, BCE loss: 0.420, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.384, BCE loss: 0.384, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.305, BCE loss: 0.305, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.333, BCE loss: 0.333, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.458, BCE loss: 0.458, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.478, BCE loss: 0.478, Diversity Loss: 0.450                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.424, BCE loss: 0.424, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.413, BCE loss: 0.413, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.347, BCE loss: 0.347, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.295, BCE loss: 0.295, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.478, BCE loss: 0.478, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.508, BCE loss: 0.508, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.308, BCE loss: 0.308, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.478, BCE loss: 0.478, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.482, BCE loss: 0.482, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.427, BCE loss: 0.427, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.400, BCE loss: 0.400, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.644, BCE loss: 0.644, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.323, BCE loss: 0.323, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.383, BCE loss: 0.383, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.363, BCE loss: 0.363, Diversity Loss: 0.440                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.316, BCE loss: 0.316, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.338, BCE loss: 0.338, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.623                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.589, BCE loss: 0.589, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.379, BCE loss: 0.379, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.442, BCE loss: 0.442, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.404                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.584, BCE loss: 0.584, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.463, BCE loss: 0.463, Diversity Loss: 0.422                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.536, BCE loss: 0.536, Diversity Loss: 0.435                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.614, BCE loss: 0.614, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.505, BCE loss: 0.505, Diversity Loss: 0.456                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.408                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.430, BCE loss: 0.430, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.482, BCE loss: 0.482, Diversity Loss: 0.404                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.419                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.441                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.561, BCE loss: 0.561, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.341, BCE loss: 0.341, Diversity Loss: 0.404                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.404, BCE loss: 0.404, Diversity Loss: 0.408                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.498, BCE loss: 0.498, Diversity Loss: 0.418                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.428                     (Diversity_weight = 0)
{'accuracy': 0.7207207207207207, 'roc_auc': 0.8090953465668728, 'pr_auc': 0.8469050983937543, 'conicity_mean': 0.4515338, 'conicity_std': 0.11604443}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.714    0.727      0.721      0.721         0.721
precision    0.649    0.805      0.721      0.727         0.736
recall       0.795    0.662      0.721      0.729         0.721
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.8090953465668728
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:11:51_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.360, BCE loss: 0.360, Diversity Loss: 0.466                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.326, BCE loss: 0.326, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.388, BCE loss: 0.388, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.523, BCE loss: 0.523, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.191, BCE loss: 0.191, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.422, BCE loss: 0.422, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.325, BCE loss: 0.325, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.340, BCE loss: 0.340, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.153, BCE loss: 0.153, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.254, BCE loss: 0.254, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.318, BCE loss: 0.318, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.339, BCE loss: 0.339, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.306, BCE loss: 0.306, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.260, BCE loss: 0.260, Diversity Loss: 0.582                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.484, BCE loss: 0.484, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.459, BCE loss: 0.459, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.578, BCE loss: 0.578, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.510, BCE loss: 0.510, Diversity Loss: 0.440                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.439                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.379, BCE loss: 0.379, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.257, BCE loss: 0.257, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.443                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.292, BCE loss: 0.292, Diversity Loss: 0.422                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.424, BCE loss: 0.424, Diversity Loss: 0.424                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.540, BCE loss: 0.540, Diversity Loss: 0.417                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.292, BCE loss: 0.292, Diversity Loss: 0.452                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.637                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.463, BCE loss: 0.463, Diversity Loss: 0.435                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.496, BCE loss: 0.496, Diversity Loss: 0.453                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.612, BCE loss: 0.612, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.505, BCE loss: 0.505, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.430                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.531, BCE loss: 0.531, Diversity Loss: 0.443                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.443, BCE loss: 0.443, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.369, BCE loss: 0.369, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.627                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.592, BCE loss: 0.592, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.453, BCE loss: 0.453, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.410, BCE loss: 0.410, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.427                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.287, BCE loss: 0.287, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.561, BCE loss: 0.561, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.301, BCE loss: 0.301, Diversity Loss: 0.450                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.406, BCE loss: 0.406, Diversity Loss: 0.430                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.441                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.399, BCE loss: 0.399, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.415, BCE loss: 0.415, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.343, BCE loss: 0.343, Diversity Loss: 0.451                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.333, BCE loss: 0.333, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.440                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.391, BCE loss: 0.391, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.450, BCE loss: 0.450, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.312, BCE loss: 0.312, Diversity Loss: 0.438                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.238, BCE loss: 0.238, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.345, BCE loss: 0.345, Diversity Loss: 0.432                     (Diversity_weight = 0)
{'accuracy': 0.7227227227227228, 'roc_auc': 0.8051090139928408, 'pr_auc': 0.8338256525104613, 'conicity_mean': 0.45758435, 'conicity_std': 0.10834416}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.695    0.746      0.723      0.720         0.724
precision    0.672    0.767      0.723      0.720         0.726
recall       0.720    0.725      0.723      0.722         0.723
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.8051090139928408
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:11:51_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.191, BCE loss: 0.191, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.355, BCE loss: 0.355, Diversity Loss: 0.417                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.325, BCE loss: 0.325, Diversity Loss: 0.435                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.164, BCE loss: 0.164, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.443                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.411                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.128, BCE loss: 0.128, Diversity Loss: 0.452                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.298, BCE loss: 0.298, Diversity Loss: 0.433                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.521, BCE loss: 0.521, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.456                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.250, BCE loss: 0.250, Diversity Loss: 0.432                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.585                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.437                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.198, BCE loss: 0.198, Diversity Loss: 0.450                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.231, BCE loss: 0.231, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.189, BCE loss: 0.189, Diversity Loss: 0.466                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.254, BCE loss: 0.254, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.130, BCE loss: 0.130, Diversity Loss: 0.452                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.337, BCE loss: 0.337, Diversity Loss: 0.460                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.291, BCE loss: 0.291, Diversity Loss: 0.428                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.453                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.294, BCE loss: 0.294, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.255, BCE loss: 0.255, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.485, BCE loss: 0.485, Diversity Loss: 0.452                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.458                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.294, BCE loss: 0.294, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.217, BCE loss: 0.217, Diversity Loss: 0.441                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.172, BCE loss: 0.172, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.369, BCE loss: 0.369, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.313, BCE loss: 0.313, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.207, BCE loss: 0.207, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.471, BCE loss: 0.471, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.386, BCE loss: 0.386, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.207, BCE loss: 0.207, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.236, BCE loss: 0.236, Diversity Loss: 0.411                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.245, BCE loss: 0.245, Diversity Loss: 0.466                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.216, BCE loss: 0.216, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.420                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.584                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.469, BCE loss: 0.469, Diversity Loss: 0.445                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.329, BCE loss: 0.329, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.200, BCE loss: 0.200, Diversity Loss: 0.424                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.296, BCE loss: 0.296, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.439                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.454                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.442                     (Diversity_weight = 0)
{'accuracy': 0.7257257257257257, 'roc_auc': 0.7994752684672959, 'pr_auc': 0.8346994350927905, 'conicity_mean': 0.45746982, 'conicity_std': 0.10807146}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.713    0.738      0.726      0.725         0.727
precision    0.660    0.795      0.726      0.728         0.736
recall       0.774    0.688      0.726      0.731         0.726
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7994752684672959
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:11:51_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.437                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.416                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.433                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.299, BCE loss: 0.299, Diversity Loss: 0.384                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.419                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.313, BCE loss: 0.313, Diversity Loss: 0.441                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.430                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.114, BCE loss: 0.114, Diversity Loss: 0.442                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.160, BCE loss: 0.160, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.126, BCE loss: 0.126, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.255, BCE loss: 0.255, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.200, BCE loss: 0.200, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.104, BCE loss: 0.104, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.302, BCE loss: 0.302, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.188, BCE loss: 0.188, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.165, BCE loss: 0.165, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.225, BCE loss: 0.225, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.133, BCE loss: 0.133, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.450                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.237, BCE loss: 0.237, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.460                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.138, BCE loss: 0.138, Diversity Loss: 0.423                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.179, BCE loss: 0.179, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.443                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.443                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.189, BCE loss: 0.189, Diversity Loss: 0.438                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.335, BCE loss: 0.335, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.425                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.291, BCE loss: 0.291, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.413, BCE loss: 0.413, Diversity Loss: 0.428                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.454                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.417                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.436                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.484, BCE loss: 0.484, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.094, BCE loss: 0.094, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.238, BCE loss: 0.238, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.128, BCE loss: 0.128, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.390                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.187, BCE loss: 0.187, Diversity Loss: 0.415                     (Diversity_weight = 0)
{'accuracy': 0.7317317317317318, 'roc_auc': 0.8009884477709079, 'pr_auc': 0.8309299702132326, 'conicity_mean': 0.44494897, 'conicity_std': 0.10944731}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.698    0.759      0.732      0.728         0.732
precision    0.690    0.765      0.732      0.728         0.732
recall       0.706    0.752      0.732      0.729         0.732
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.8009884477709079
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:11:51_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.424                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.452                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.108, BCE loss: 0.108, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.435                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.411                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.429                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.413                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.405                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.128, BCE loss: 0.128, Diversity Loss: 0.429                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.425                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.400                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.418                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.138, BCE loss: 0.138, Diversity Loss: 0.430                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.429                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.446                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.415                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.412                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.441                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.433                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.458                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.109, BCE loss: 0.109, Diversity Loss: 0.456                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.225, BCE loss: 0.225, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.110, BCE loss: 0.110, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.247, BCE loss: 0.247, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.466                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.453                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.422                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.409                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.094, BCE loss: 0.094, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.399                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.104, BCE loss: 0.104, Diversity Loss: 0.425                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.428, BCE loss: 0.428, Diversity Loss: 0.451                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.432                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.265, BCE loss: 0.265, Diversity Loss: 0.453                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.265, BCE loss: 0.265, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.313, BCE loss: 0.313, Diversity Loss: 0.436                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.445                     (Diversity_weight = 0)
{'accuracy': 0.7347347347347347, 'roc_auc': 0.8002440611780021, 'pr_auc': 0.8351129767248344, 'conicity_mean': 0.44733247, 'conicity_std': 0.10466807}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.702    0.761      0.735      0.731         0.735
precision    0.693    0.769      0.735      0.731         0.736
recall       0.711    0.754      0.735      0.732         0.735
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.8002440611780021
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:11:51_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.436                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.153, BCE loss: 0.153, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.406                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.268, BCE loss: 0.268, Diversity Loss: 0.441                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.433                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.450                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.422                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.209, BCE loss: 0.209, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.440                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.418                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.443                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.427                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.436                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.435                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.438                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.445                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.102, BCE loss: 0.102, Diversity Loss: 0.393                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.425                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.410                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.435                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.216, BCE loss: 0.216, Diversity Loss: 0.445                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.440                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.393                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.290, BCE loss: 0.290, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.333, BCE loss: 0.333, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.264, BCE loss: 0.264, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.438                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.412                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.433                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.449                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.389                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.408, BCE loss: 0.408, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.456                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.412                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.408                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.430                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.445                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.228, BCE loss: 0.228, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.434                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.426                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.336, BCE loss: 0.336, Diversity Loss: 0.404                     (Diversity_weight = 0)
{'accuracy': 0.7137137137137137, 'roc_auc': 0.8024406117800196, 'pr_auc': 0.8387882186511669, 'conicity_mean': 0.44955003, 'conicity_std': 0.0996588}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.704    0.723      0.714      0.713         0.715
precision    0.645    0.790      0.714      0.718         0.726
recall       0.774    0.666      0.714      0.720         0.714
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.8024406117800196
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
./experiments/cls_en/cls_en/lstm+tanh/Sun_Jan_17_19:11:51_2021
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:12:45,761 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:12:45,761 - type = vanillalstm
INFO - 2021-01-17 19:12:45,761 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:12:45,762 - vocab_size = 666
INFO - 2021-01-17 19:12:45,762 - embed_size = 200
INFO - 2021-01-17 19:12:45,762 - hidden_size = 128
INFO - 2021-01-17 19:12:45,762 - pre_embed = None
INFO - 2021-01-17 19:12:45,777 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:12:45,777 - hidden_size = 256
INFO - 2021-01-17 19:12:45,777 - output_size = 1
INFO - 2021-01-17 19:12:45,777 - use_attention = True
INFO - 2021-01-17 19:12:45,777 - regularizer_attention = None
INFO - 2021-01-17 19:12:45,777 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b6cff7165d0> and extras set()
INFO - 2021-01-17 19:12:45,777 - attention.type = tanh
INFO - 2021-01-17 19:12:45,777 - type = tanh
INFO - 2021-01-17 19:12:45,777 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b6cff7165d0> and extras set()
INFO - 2021-01-17 19:12:45,777 - attention.hidden_size = 256
INFO - 2021-01-17 19:12:45,778 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7677677677677678, 'roc_auc': 0.8371140901358467, 'pr_auc': 0.8014844008029138, 'conicity_mean': '0.4509715', 'conicity_std': '0.1112052'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.803    0.717      0.768      0.760         0.765
precision    0.767    0.770      0.768      0.768         0.768
recall       0.843    0.671      0.768      0.757         0.768
support    561.000  438.000    999.000    999.000       999.000
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:12:47,217 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:12:47,217 - type = vanillalstm
INFO - 2021-01-17 19:12:47,218 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:12:47,218 - vocab_size = 666
INFO - 2021-01-17 19:12:47,218 - embed_size = 200
INFO - 2021-01-17 19:12:47,218 - hidden_size = 128
INFO - 2021-01-17 19:12:47,218 - pre_embed = None
INFO - 2021-01-17 19:12:47,232 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:12:47,232 - hidden_size = 256
INFO - 2021-01-17 19:12:47,232 - output_size = 1
INFO - 2021-01-17 19:12:47,232 - use_attention = True
INFO - 2021-01-17 19:12:47,232 - regularizer_attention = None
INFO - 2021-01-17 19:12:47,232 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b6c26d03610> and extras set()
INFO - 2021-01-17 19:12:47,233 - attention.type = tanh
INFO - 2021-01-17 19:12:47,233 - type = tanh
INFO - 2021-01-17 19:12:47,233 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b6c26d03610> and extras set()
INFO - 2021-01-17 19:12:47,233 - attention.hidden_size = 256
INFO - 2021-01-17 19:12:47,233 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7677677677677678, 'roc_auc': 0.8371140901358467, 'pr_auc': 0.8014844008029138, 'conicity_mean': '0.4509715', 'conicity_std': '0.1112052'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.803    0.717      0.768      0.760         0.765
precision    0.767    0.770      0.768      0.768         0.768
recall       0.843    0.671      0.768      0.757         0.768
support    561.000  438.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_en ortho_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:12:59,496 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:12:59,496 - type = ortholstm
INFO - 2021-01-17 19:12:59,496 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:12:59,541 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:12:59,980 - vocab_size = 666
INFO - 2021-01-17 19:12:59,980 - embed_size = 200
INFO - 2021-01-17 19:12:59,980 - hidden_size = 128
INFO - 2021-01-17 19:12:59,980 - pre_embed = None
INFO - 2021-01-17 19:13:03,172 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:13:03,173 - hidden_size = 256
INFO - 2021-01-17 19:13:03,173 - output_size = 1
INFO - 2021-01-17 19:13:03,173 - use_attention = True
INFO - 2021-01-17 19:13:03,173 - regularizer_attention = None
INFO - 2021-01-17 19:13:03,173 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b6f9b1ea9d0> and extras set()
INFO - 2021-01-17 19:13:03,173 - attention.type = tanh
INFO - 2021-01-17 19:13:03,173 - type = tanh
INFO - 2021-01-17 19:13:03,174 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b6f9b1ea9d0> and extras set()
INFO - 2021-01-17 19:13:03,174 - attention.hidden_size = 256
INFO - 2021-01-17 19:13:03,174 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.702, BCE loss: 0.702, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.259                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.701, BCE loss: 0.701, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.109                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.735, BCE loss: 0.735, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.665, BCE loss: 0.665, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.289                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.249                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.619, BCE loss: 0.619, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.718, BCE loss: 0.718, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.650, BCE loss: 0.650, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.651, BCE loss: 0.651, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.644, BCE loss: 0.644, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.635, BCE loss: 0.635, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.645, BCE loss: 0.645, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.719, BCE loss: 0.719, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.714, BCE loss: 0.714, Diversity Loss: 0.120                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.585, BCE loss: 0.585, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.114                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.603, BCE loss: 0.603, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.544, BCE loss: 0.544, Diversity Loss: 0.235                     (Diversity_weight = 0)
{'accuracy': 0.6376376376376376, 'roc_auc': 0.7234461438333876, 'pr_auc': 0.7510149046956888, 'conicity_mean': 0.18110178, 'conicity_std': 0.042877838}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.653    0.621      0.638      0.637         0.635
precision    0.564    0.751      0.638      0.657         0.669
recall       0.777    0.529      0.638      0.653         0.638
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7234461438333876
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:13:03_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.623, BCE loss: 0.623, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.616, BCE loss: 0.616, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.573, BCE loss: 0.573, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.508, BCE loss: 0.508, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.500, BCE loss: 0.500, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.433, BCE loss: 0.433, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.651, BCE loss: 0.651, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.447, BCE loss: 0.447, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.655, BCE loss: 0.655, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.542, BCE loss: 0.542, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.673, BCE loss: 0.673, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.516, BCE loss: 0.516, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.412, BCE loss: 0.412, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.648, BCE loss: 0.648, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.575, BCE loss: 0.575, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.639, BCE loss: 0.639, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.531, BCE loss: 0.531, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.498, BCE loss: 0.498, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.645, BCE loss: 0.645, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.486, BCE loss: 0.486, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.445, BCE loss: 0.445, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.508, BCE loss: 0.508, Diversity Loss: 0.240                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.443, BCE loss: 0.443, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.249                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.481, BCE loss: 0.481, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.710, BCE loss: 0.710, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.283                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.594, BCE loss: 0.594, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.491, BCE loss: 0.491, Diversity Loss: 0.307                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.525, BCE loss: 0.525, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.508, BCE loss: 0.508, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.103                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.654, BCE loss: 0.654, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.518, BCE loss: 0.518, Diversity Loss: 0.267                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.486, BCE loss: 0.486, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.510, BCE loss: 0.510, Diversity Loss: 0.207                     (Diversity_weight = 0)
{'accuracy': 0.6916916916916916, 'roc_auc': 0.7460909534656687, 'pr_auc': 0.7803242274586569, 'conicity_mean': 0.19512694, 'conicity_std': 0.04414716}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.615    0.743      0.692      0.679         0.687
precision    0.681    0.697      0.692      0.689         0.690
recall       0.560    0.795      0.692      0.678         0.692
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7460909534656687
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:13:03_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.533, BCE loss: 0.533, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.366, BCE loss: 0.366, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.492, BCE loss: 0.492, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.487, BCE loss: 0.487, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.272, BCE loss: 0.272, Diversity Loss: 0.240                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.452, BCE loss: 0.452, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.488, BCE loss: 0.488, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.510, BCE loss: 0.510, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.500, BCE loss: 0.500, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.360, BCE loss: 0.360, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.310, BCE loss: 0.310, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.523, BCE loss: 0.523, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.523, BCE loss: 0.523, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.542, BCE loss: 0.542, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.390, BCE loss: 0.390, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.401, BCE loss: 0.401, Diversity Loss: 0.307                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.335, BCE loss: 0.335, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.494, BCE loss: 0.494, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.381, BCE loss: 0.381, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.432, BCE loss: 0.432, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.352, BCE loss: 0.352, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.312, BCE loss: 0.312, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.356, BCE loss: 0.356, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.337, BCE loss: 0.337, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.494, BCE loss: 0.494, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.363, BCE loss: 0.363, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.761, BCE loss: 0.761, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.377, BCE loss: 0.377, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.249                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.513, BCE loss: 0.513, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.320, BCE loss: 0.320, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.102                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.245, BCE loss: 0.245, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.327, BCE loss: 0.327, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.551, BCE loss: 0.551, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.331, BCE loss: 0.331, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.517, BCE loss: 0.517, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.409, BCE loss: 0.409, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.333, BCE loss: 0.333, Diversity Loss: 0.246                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.490, BCE loss: 0.490, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.234                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.589, BCE loss: 0.589, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.498, BCE loss: 0.498, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.325, BCE loss: 0.325, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.372, BCE loss: 0.372, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.460, BCE loss: 0.460, Diversity Loss: 0.257                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.369, BCE loss: 0.369, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.641, BCE loss: 0.641, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.359, BCE loss: 0.359, Diversity Loss: 0.272                     (Diversity_weight = 0)
{'accuracy': 0.6986986986986987, 'roc_auc': 0.7569272697689553, 'pr_auc': 0.7931458105456608, 'conicity_mean': 0.19243474, 'conicity_std': 0.042905964}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.652    0.734      0.699      0.693         0.698
precision    0.662    0.726      0.699      0.694         0.698
recall       0.642    0.743      0.699      0.693         0.699
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7569272697689553
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:13:03_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.240                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.461, BCE loss: 0.461, Diversity Loss: 0.228                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.396, BCE loss: 0.396, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.328, BCE loss: 0.328, Diversity Loss: 0.270                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.452, BCE loss: 0.452, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.518, BCE loss: 0.518, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.270, BCE loss: 0.270, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.242, BCE loss: 0.242, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.253, BCE loss: 0.253, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.227, BCE loss: 0.227, Diversity Loss: 0.249                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.299, BCE loss: 0.299, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.320, BCE loss: 0.320, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.229, BCE loss: 0.229, Diversity Loss: 0.242                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.228, BCE loss: 0.228, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.359, BCE loss: 0.359, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.284, BCE loss: 0.284, Diversity Loss: 0.309                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.257, BCE loss: 0.257, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.226, BCE loss: 0.226, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.240, BCE loss: 0.240, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.289, BCE loss: 0.289, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.330, BCE loss: 0.330, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.208, BCE loss: 0.208, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.294, BCE loss: 0.294, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.191, BCE loss: 0.191, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.383, BCE loss: 0.383, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.372, BCE loss: 0.372, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.289, BCE loss: 0.289, Diversity Loss: 0.266                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.355, BCE loss: 0.355, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.200, BCE loss: 0.200, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.231, BCE loss: 0.231, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.332, BCE loss: 0.332, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.326, BCE loss: 0.326, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.244, BCE loss: 0.244, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.296, BCE loss: 0.296, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.465, BCE loss: 0.465, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.290, BCE loss: 0.290, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.565, BCE loss: 0.565, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.109                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.257, BCE loss: 0.257, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.301, BCE loss: 0.301, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.316, BCE loss: 0.316, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.352, BCE loss: 0.352, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.320, BCE loss: 0.320, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.207, BCE loss: 0.207, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.289, BCE loss: 0.289, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.267, BCE loss: 0.267, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.347, BCE loss: 0.347, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.497, BCE loss: 0.497, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.398, BCE loss: 0.398, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.262, BCE loss: 0.262, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.208                     (Diversity_weight = 0)
{'accuracy': 0.6866866866866866, 'roc_auc': 0.7513301334201106, 'pr_auc': 0.7862129102849258, 'conicity_mean': 0.19575526, 'conicity_std': 0.04406036}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.645    0.720      0.687      0.682         0.687
precision    0.643    0.722      0.687      0.682         0.687
recall       0.647    0.718      0.687      0.682         0.687
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7513301334201106
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:13:03_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.296, BCE loss: 0.296, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.153, BCE loss: 0.153, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.319, BCE loss: 0.319, Diversity Loss: 0.307                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.098, BCE loss: 0.098, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.198, BCE loss: 0.198, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.211, BCE loss: 0.211, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.281, BCE loss: 0.281, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.244                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.161, BCE loss: 0.161, Diversity Loss: 0.273                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.209, BCE loss: 0.209, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.326, BCE loss: 0.326, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.104, BCE loss: 0.104, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.269                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.262, BCE loss: 0.262, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.230, BCE loss: 0.230, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.206, BCE loss: 0.206, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.192, BCE loss: 0.192, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.102                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.136, BCE loss: 0.136, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.211, BCE loss: 0.211, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.136, BCE loss: 0.136, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.364, BCE loss: 0.364, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.263, BCE loss: 0.263, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.293, BCE loss: 0.293, Diversity Loss: 0.245                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.426, BCE loss: 0.426, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.295, BCE loss: 0.295, Diversity Loss: 0.165                     (Diversity_weight = 0)
{'accuracy': 0.6996996996996997, 'roc_auc': 0.7495159453302961, 'pr_auc': 0.7918112196497301, 'conicity_mean': 0.1917263, 'conicity_std': 0.044718497}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.672    0.723        0.7      0.698         0.701
precision    0.646    0.749        0.7      0.697         0.704
recall       0.702    0.698        0.7      0.700         0.700
support    439.000  560.000      999.0    999.000       999.000
Model not saved on  roc_auc 0.7495159453302961
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:13:03_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.117                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.114, BCE loss: 0.114, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.110, BCE loss: 0.110, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.275                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.182, BCE loss: 0.182, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.241, BCE loss: 0.241, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.297, BCE loss: 0.297, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.102                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.304                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.109, BCE loss: 0.109, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.240                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.293, BCE loss: 0.293, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.150, BCE loss: 0.150, Diversity Loss: 0.243                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.244, BCE loss: 0.244, Diversity Loss: 0.262                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.357, BCE loss: 0.357, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.118, BCE loss: 0.118, Diversity Loss: 0.174                     (Diversity_weight = 0)
{'accuracy': 0.7127127127127127, 'roc_auc': 0.7673486820696388, 'pr_auc': 0.787358276108648, 'conicity_mean': 0.18792538, 'conicity_std': 0.04364373}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.674    0.743      0.713      0.709         0.713
precision    0.672    0.745      0.713      0.709         0.713
recall       0.677    0.741      0.713      0.709         0.713
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7673486820696388
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:13:03_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.301                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.138, BCE loss: 0.138, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.244                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.151, BCE loss: 0.151, Diversity Loss: 0.117                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.128, BCE loss: 0.128, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.105                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.249                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.102, BCE loss: 0.102, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.281                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.050, BCE loss: 0.050, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.118, BCE loss: 0.118, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.124, BCE loss: 0.124, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.216, BCE loss: 0.216, Diversity Loss: 0.248                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.195                     (Diversity_weight = 0)
{'accuracy': 0.6866866866866866, 'roc_auc': 0.7481573381060853, 'pr_auc': 0.7823056606225498, 'conicity_mean': 0.18921638, 'conicity_std': 0.043563005}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.671    0.701      0.687      0.686         0.688
precision    0.623    0.754      0.687      0.688         0.696
recall       0.727    0.655      0.687      0.691         0.687
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7481573381060853
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:13:03_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.014, BCE loss: 0.014, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.243                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.127, BCE loss: 0.127, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.173, BCE loss: 0.173, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.117                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.300                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.134, BCE loss: 0.134, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.011, BCE loss: 0.011, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.264, BCE loss: 0.264, Diversity Loss: 0.257                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.400, BCE loss: 0.400, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.164, BCE loss: 0.164, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.105                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.234                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.204, BCE loss: 0.204, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.237                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.153, BCE loss: 0.153, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.410, BCE loss: 0.410, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.271                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.181                     (Diversity_weight = 0)
{'accuracy': 0.6896896896896897, 'roc_auc': 0.7403961926456233, 'pr_auc': 0.7665944850529502, 'conicity_mean': 0.1880688, 'conicity_std': 0.042688206}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.656    0.718       0.69      0.687         0.690
precision    0.640    0.732       0.69      0.686         0.692
recall       0.672    0.704       0.69      0.688         0.690
support    439.000  560.000     999.00    999.000       999.000
Model not saved on  roc_auc 0.7403961926456233
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
./experiments/cls_en/cls_en/ortho_lstm+tanh/Sun_Jan_17_19:13:03_2021
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:85: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:86: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:14:35,239 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:14:35,240 - type = ortholstm
INFO - 2021-01-17 19:14:35,240 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:14:35,240 - vocab_size = 666
INFO - 2021-01-17 19:14:35,240 - embed_size = 200
INFO - 2021-01-17 19:14:35,240 - hidden_size = 128
INFO - 2021-01-17 19:14:35,240 - pre_embed = None
INFO - 2021-01-17 19:14:35,255 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:14:35,255 - hidden_size = 256
INFO - 2021-01-17 19:14:35,255 - output_size = 1
INFO - 2021-01-17 19:14:35,255 - use_attention = True
INFO - 2021-01-17 19:14:35,255 - regularizer_attention = None
INFO - 2021-01-17 19:14:35,255 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b6f9b877150> and extras set()
INFO - 2021-01-17 19:14:35,255 - attention.type = tanh
INFO - 2021-01-17 19:14:35,255 - type = tanh
INFO - 2021-01-17 19:14:35,256 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b6f9b877150> and extras set()
INFO - 2021-01-17 19:14:35,256 - attention.hidden_size = 256
INFO - 2021-01-17 19:14:35,256 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7297297297297297, 'roc_auc': 0.7826573551795147, 'pr_auc': 0.6886037385039298, 'conicity_mean': '0.18758546', 'conicity_std': '0.04326885'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.746    0.711       0.73      0.729         0.731
precision    0.789    0.669       0.73      0.729         0.737
recall       0.708    0.758       0.73      0.733         0.730
support    561.000  438.000     999.00    999.000       999.000
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:14:37,501 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:14:37,501 - type = ortholstm
INFO - 2021-01-17 19:14:37,501 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:14:37,501 - vocab_size = 666
INFO - 2021-01-17 19:14:37,501 - embed_size = 200
INFO - 2021-01-17 19:14:37,502 - hidden_size = 128
INFO - 2021-01-17 19:14:37,502 - pre_embed = None
INFO - 2021-01-17 19:14:37,516 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:14:37,516 - hidden_size = 256
INFO - 2021-01-17 19:14:37,516 - output_size = 1
INFO - 2021-01-17 19:14:37,516 - use_attention = True
INFO - 2021-01-17 19:14:37,516 - regularizer_attention = None
INFO - 2021-01-17 19:14:37,516 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b6f9b1ea490> and extras set()
INFO - 2021-01-17 19:14:37,517 - attention.type = tanh
INFO - 2021-01-17 19:14:37,517 - type = tanh
INFO - 2021-01-17 19:14:37,517 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b6f9b1ea490> and extras set()
INFO - 2021-01-17 19:14:37,517 - attention.hidden_size = 256
INFO - 2021-01-17 19:14:37,517 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7297297297297297, 'roc_auc': 0.7826573551795147, 'pr_auc': 0.6886037385039298, 'conicity_mean': '0.18758546', 'conicity_std': '0.04326885'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.746    0.711       0.73      0.729         0.731
precision    0.789    0.669       0.73      0.729         0.737
recall       0.708    0.758       0.73      0.733         0.730
support    561.000  438.000     999.00    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_en diversity_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:14:50,820 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:14:50,820 - type = vanillalstm
INFO - 2021-01-17 19:14:50,820 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:14:50,836 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:14:51,158 - vocab_size = 666
INFO - 2021-01-17 19:14:51,158 - embed_size = 200
INFO - 2021-01-17 19:14:51,158 - hidden_size = 128
INFO - 2021-01-17 19:14:51,158 - pre_embed = None
INFO - 2021-01-17 19:14:54,241 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:14:54,242 - hidden_size = 256
INFO - 2021-01-17 19:14:54,242 - output_size = 1
INFO - 2021-01-17 19:14:54,242 - use_attention = True
INFO - 2021-01-17 19:14:54,242 - regularizer_attention = None
INFO - 2021-01-17 19:14:54,243 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b4512084a10> and extras set()
INFO - 2021-01-17 19:14:54,243 - attention.type = tanh
INFO - 2021-01-17 19:14:54,243 - type = tanh
INFO - 2021-01-17 19:14:54,243 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b4512084a10> and extras set()
INFO - 2021-01-17 19:14:54,243 - attention.hidden_size = 256
INFO - 2021-01-17 19:14:54,243 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.905, BCE loss: 0.683, Diversity Loss: 0.445                     (Diversity_weight = 0.5)
Epoch: 0 Step: 1 Total Loss: 0.857, BCE loss: 0.672, Diversity Loss: 0.370                     (Diversity_weight = 0.5)
Epoch: 0 Step: 2 Total Loss: 0.899, BCE loss: 0.676, Diversity Loss: 0.446                     (Diversity_weight = 0.5)
Epoch: 0 Step: 3 Total Loss: 0.818, BCE loss: 0.661, Diversity Loss: 0.313                     (Diversity_weight = 0.5)
Epoch: 0 Step: 4 Total Loss: 1.135, BCE loss: 0.871, Diversity Loss: 0.526                     (Diversity_weight = 0.5)
Epoch: 0 Step: 5 Total Loss: 0.820, BCE loss: 0.691, Diversity Loss: 0.257                     (Diversity_weight = 0.5)
Epoch: 0 Step: 6 Total Loss: 0.842, BCE loss: 0.698, Diversity Loss: 0.288                     (Diversity_weight = 0.5)
Epoch: 0 Step: 7 Total Loss: 0.823, BCE loss: 0.687, Diversity Loss: 0.272                     (Diversity_weight = 0.5)
Epoch: 0 Step: 8 Total Loss: 0.821, BCE loss: 0.692, Diversity Loss: 0.257                     (Diversity_weight = 0.5)
Epoch: 0 Step: 9 Total Loss: 0.848, BCE loss: 0.699, Diversity Loss: 0.298                     (Diversity_weight = 0.5)
Epoch: 0 Step: 10 Total Loss: 0.850, BCE loss: 0.684, Diversity Loss: 0.334                     (Diversity_weight = 0.5)
Epoch: 0 Step: 11 Total Loss: 0.816, BCE loss: 0.695, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 0 Step: 12 Total Loss: 0.814, BCE loss: 0.691, Diversity Loss: 0.246                     (Diversity_weight = 0.5)
Epoch: 0 Step: 13 Total Loss: 0.823, BCE loss: 0.696, Diversity Loss: 0.254                     (Diversity_weight = 0.5)
Epoch: 0 Step: 14 Total Loss: 0.796, BCE loss: 0.691, Diversity Loss: 0.211                     (Diversity_weight = 0.5)
Epoch: 0 Step: 15 Total Loss: 0.860, BCE loss: 0.689, Diversity Loss: 0.342                     (Diversity_weight = 0.5)
Epoch: 0 Step: 16 Total Loss: 0.841, BCE loss: 0.686, Diversity Loss: 0.310                     (Diversity_weight = 0.5)
Epoch: 0 Step: 17 Total Loss: 0.786, BCE loss: 0.686, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 0 Step: 18 Total Loss: 0.900, BCE loss: 0.679, Diversity Loss: 0.442                     (Diversity_weight = 0.5)
Epoch: 0 Step: 19 Total Loss: 0.773, BCE loss: 0.684, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 0 Step: 20 Total Loss: 0.789, BCE loss: 0.697, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 0 Step: 21 Total Loss: 0.797, BCE loss: 0.691, Diversity Loss: 0.212                     (Diversity_weight = 0.5)
Epoch: 0 Step: 22 Total Loss: 0.774, BCE loss: 0.673, Diversity Loss: 0.203                     (Diversity_weight = 0.5)
Epoch: 0 Step: 23 Total Loss: 0.796, BCE loss: 0.700, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 0 Step: 24 Total Loss: 0.770, BCE loss: 0.690, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 0 Step: 25 Total Loss: 0.812, BCE loss: 0.682, Diversity Loss: 0.260                     (Diversity_weight = 0.5)
Epoch: 0 Step: 26 Total Loss: 0.789, BCE loss: 0.684, Diversity Loss: 0.210                     (Diversity_weight = 0.5)
Epoch: 0 Step: 27 Total Loss: 0.770, BCE loss: 0.687, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 0 Step: 28 Total Loss: 0.770, BCE loss: 0.692, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 0 Step: 29 Total Loss: 0.777, BCE loss: 0.683, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 0 Step: 30 Total Loss: 0.793, BCE loss: 0.679, Diversity Loss: 0.227                     (Diversity_weight = 0.5)
Epoch: 0 Step: 31 Total Loss: 0.769, BCE loss: 0.683, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 0 Step: 32 Total Loss: 0.827, BCE loss: 0.683, Diversity Loss: 0.287                     (Diversity_weight = 0.5)
Epoch: 0 Step: 33 Total Loss: 0.785, BCE loss: 0.699, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 0 Step: 34 Total Loss: 0.781, BCE loss: 0.689, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 0 Step: 35 Total Loss: 0.771, BCE loss: 0.691, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 0 Step: 36 Total Loss: 0.767, BCE loss: 0.690, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 0 Step: 37 Total Loss: 0.798, BCE loss: 0.701, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 0 Step: 38 Total Loss: 0.809, BCE loss: 0.684, Diversity Loss: 0.250                     (Diversity_weight = 0.5)
Epoch: 0 Step: 39 Total Loss: 0.814, BCE loss: 0.686, Diversity Loss: 0.255                     (Diversity_weight = 0.5)
Epoch: 0 Step: 40 Total Loss: 0.782, BCE loss: 0.686, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 0 Step: 41 Total Loss: 0.765, BCE loss: 0.692, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 0 Step: 42 Total Loss: 0.772, BCE loss: 0.705, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 0 Step: 43 Total Loss: 0.785, BCE loss: 0.681, Diversity Loss: 0.208                     (Diversity_weight = 0.5)
Epoch: 0 Step: 44 Total Loss: 0.756, BCE loss: 0.692, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 0 Step: 45 Total Loss: 0.775, BCE loss: 0.686, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 0 Step: 46 Total Loss: 0.821, BCE loss: 0.671, Diversity Loss: 0.300                     (Diversity_weight = 0.5)
Epoch: 0 Step: 47 Total Loss: 0.804, BCE loss: 0.678, Diversity Loss: 0.253                     (Diversity_weight = 0.5)
Epoch: 0 Step: 48 Total Loss: 0.819, BCE loss: 0.663, Diversity Loss: 0.312                     (Diversity_weight = 0.5)
Epoch: 0 Step: 49 Total Loss: 0.852, BCE loss: 0.686, Diversity Loss: 0.333                     (Diversity_weight = 0.5)
Epoch: 0 Step: 50 Total Loss: 0.762, BCE loss: 0.674, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 0 Step: 51 Total Loss: 0.758, BCE loss: 0.674, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 0 Step: 52 Total Loss: 0.737, BCE loss: 0.660, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 0 Step: 53 Total Loss: 0.761, BCE loss: 0.693, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 0 Step: 54 Total Loss: 0.756, BCE loss: 0.658, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 0 Step: 55 Total Loss: 0.745, BCE loss: 0.681, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 0 Step: 56 Total Loss: 0.750, BCE loss: 0.680, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 0 Step: 57 Total Loss: 0.739, BCE loss: 0.680, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 0 Step: 58 Total Loss: 0.770, BCE loss: 0.673, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 0 Step: 59 Total Loss: 0.722, BCE loss: 0.651, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 0 Step: 60 Total Loss: 0.723, BCE loss: 0.641, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 0 Step: 61 Total Loss: 0.702, BCE loss: 0.645, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 0 Step: 62 Total Loss: 0.801, BCE loss: 0.709, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
{'accuracy': 0.6906906906906907, 'roc_auc': 0.7170924178327367, 'pr_auc': 0.7259146385469737, 'conicity_mean': 0.16874085, 'conicity_std': 0.06268098}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.625    0.737      0.691      0.681         0.688
precision    0.669    0.704      0.691      0.687         0.689
recall       0.585    0.773      0.691      0.679         0.691
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7170924178327367
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:14:54_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.704, BCE loss: 0.637, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 1 Step: 1 Total Loss: 0.672, BCE loss: 0.551, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 1 Step: 2 Total Loss: 0.804, BCE loss: 0.736, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 1 Step: 3 Total Loss: 0.702, BCE loss: 0.641, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 1 Step: 4 Total Loss: 0.691, BCE loss: 0.613, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 1 Step: 5 Total Loss: 0.720, BCE loss: 0.644, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 1 Step: 6 Total Loss: 0.772, BCE loss: 0.702, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 1 Step: 7 Total Loss: 0.693, BCE loss: 0.616, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 1 Step: 8 Total Loss: 0.715, BCE loss: 0.628, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 1 Step: 9 Total Loss: 0.713, BCE loss: 0.641, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 1 Step: 10 Total Loss: 0.752, BCE loss: 0.692, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 1 Step: 11 Total Loss: 0.783, BCE loss: 0.721, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 1 Step: 12 Total Loss: 0.791, BCE loss: 0.691, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 1 Step: 13 Total Loss: 0.806, BCE loss: 0.731, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 1 Step: 14 Total Loss: 0.731, BCE loss: 0.666, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 1 Step: 15 Total Loss: 0.734, BCE loss: 0.645, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 1 Step: 16 Total Loss: 0.719, BCE loss: 0.647, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 1 Step: 17 Total Loss: 0.714, BCE loss: 0.630, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 1 Step: 18 Total Loss: 0.731, BCE loss: 0.666, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 1 Step: 19 Total Loss: 0.740, BCE loss: 0.647, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 1 Step: 20 Total Loss: 0.733, BCE loss: 0.671, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 1 Step: 21 Total Loss: 0.797, BCE loss: 0.686, Diversity Loss: 0.222                     (Diversity_weight = 0.5)
Epoch: 1 Step: 22 Total Loss: 0.733, BCE loss: 0.625, Diversity Loss: 0.216                     (Diversity_weight = 0.5)
Epoch: 1 Step: 23 Total Loss: 0.799, BCE loss: 0.629, Diversity Loss: 0.340                     (Diversity_weight = 0.5)
Epoch: 1 Step: 24 Total Loss: 0.663, BCE loss: 0.575, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 1 Step: 25 Total Loss: 0.651, BCE loss: 0.569, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 1 Step: 26 Total Loss: 0.694, BCE loss: 0.642, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 1 Step: 27 Total Loss: 0.747, BCE loss: 0.671, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 1 Step: 28 Total Loss: 0.632, BCE loss: 0.476, Diversity Loss: 0.311                     (Diversity_weight = 0.5)
Epoch: 1 Step: 29 Total Loss: 0.678, BCE loss: 0.584, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 1 Step: 30 Total Loss: 0.799, BCE loss: 0.713, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 1 Step: 31 Total Loss: 0.631, BCE loss: 0.560, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 1 Step: 32 Total Loss: 0.678, BCE loss: 0.589, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 1 Step: 33 Total Loss: 0.690, BCE loss: 0.597, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 1 Step: 34 Total Loss: 0.713, BCE loss: 0.635, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 1 Step: 35 Total Loss: 0.759, BCE loss: 0.700, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 1 Step: 36 Total Loss: 0.773, BCE loss: 0.720, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 1 Step: 37 Total Loss: 0.666, BCE loss: 0.588, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 1 Step: 38 Total Loss: 0.671, BCE loss: 0.554, Diversity Loss: 0.234                     (Diversity_weight = 0.5)
Epoch: 1 Step: 39 Total Loss: 0.666, BCE loss: 0.536, Diversity Loss: 0.259                     (Diversity_weight = 0.5)
Epoch: 1 Step: 40 Total Loss: 0.677, BCE loss: 0.583, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 1 Step: 41 Total Loss: 0.642, BCE loss: 0.579, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 1 Step: 42 Total Loss: 0.683, BCE loss: 0.609, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 1 Step: 43 Total Loss: 0.672, BCE loss: 0.588, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 1 Step: 44 Total Loss: 0.680, BCE loss: 0.621, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 1 Step: 45 Total Loss: 0.599, BCE loss: 0.517, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 1 Step: 46 Total Loss: 0.751, BCE loss: 0.695, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 1 Step: 47 Total Loss: 0.595, BCE loss: 0.474, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 1 Step: 48 Total Loss: 0.700, BCE loss: 0.647, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 1 Step: 49 Total Loss: 0.728, BCE loss: 0.670, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 1 Step: 50 Total Loss: 0.652, BCE loss: 0.599, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 1 Step: 51 Total Loss: 0.655, BCE loss: 0.579, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 1 Step: 52 Total Loss: 0.678, BCE loss: 0.600, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 1 Step: 53 Total Loss: 0.756, BCE loss: 0.681, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 1 Step: 54 Total Loss: 0.645, BCE loss: 0.520, Diversity Loss: 0.250                     (Diversity_weight = 0.5)
Epoch: 1 Step: 55 Total Loss: 0.704, BCE loss: 0.599, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 1 Step: 56 Total Loss: 0.694, BCE loss: 0.615, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 1 Step: 57 Total Loss: 0.691, BCE loss: 0.621, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 1 Step: 58 Total Loss: 0.765, BCE loss: 0.657, Diversity Loss: 0.217                     (Diversity_weight = 0.5)
Epoch: 1 Step: 59 Total Loss: 0.676, BCE loss: 0.595, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 1 Step: 60 Total Loss: 0.660, BCE loss: 0.597, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 1 Step: 61 Total Loss: 0.695, BCE loss: 0.568, Diversity Loss: 0.253                     (Diversity_weight = 0.5)
Epoch: 1 Step: 62 Total Loss: 0.614, BCE loss: 0.547, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
{'accuracy': 0.6646646646646647, 'roc_auc': 0.7214814513504719, 'pr_auc': 0.7643251724374509, 'conicity_mean': 0.1708072, 'conicity_std': 0.062512435}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.625    0.697      0.665      0.661         0.665
precision    0.615    0.706      0.665      0.660         0.666
recall       0.636    0.688      0.665      0.662         0.665
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7214814513504719
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:14:54_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.610, BCE loss: 0.503, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 2 Step: 1 Total Loss: 0.570, BCE loss: 0.426, Diversity Loss: 0.288                     (Diversity_weight = 0.5)
Epoch: 2 Step: 2 Total Loss: 0.590, BCE loss: 0.488, Diversity Loss: 0.204                     (Diversity_weight = 0.5)
Epoch: 2 Step: 3 Total Loss: 0.661, BCE loss: 0.604, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 2 Step: 4 Total Loss: 0.694, BCE loss: 0.640, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 2 Step: 5 Total Loss: 0.671, BCE loss: 0.618, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 2 Step: 6 Total Loss: 0.571, BCE loss: 0.500, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 2 Step: 7 Total Loss: 0.647, BCE loss: 0.581, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 2 Step: 8 Total Loss: 0.527, BCE loss: 0.460, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 2 Step: 9 Total Loss: 0.598, BCE loss: 0.511, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 2 Step: 10 Total Loss: 0.525, BCE loss: 0.408, Diversity Loss: 0.233                     (Diversity_weight = 0.5)
Epoch: 2 Step: 11 Total Loss: 0.592, BCE loss: 0.506, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 2 Step: 12 Total Loss: 0.410, BCE loss: 0.294, Diversity Loss: 0.232                     (Diversity_weight = 0.5)
Epoch: 2 Step: 13 Total Loss: 0.766, BCE loss: 0.718, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 2 Step: 14 Total Loss: 0.617, BCE loss: 0.515, Diversity Loss: 0.204                     (Diversity_weight = 0.5)
Epoch: 2 Step: 15 Total Loss: 0.586, BCE loss: 0.465, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 2 Step: 16 Total Loss: 0.685, BCE loss: 0.623, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 2 Step: 17 Total Loss: 0.563, BCE loss: 0.472, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 2 Step: 18 Total Loss: 0.524, BCE loss: 0.439, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 2 Step: 19 Total Loss: 0.699, BCE loss: 0.626, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 2 Step: 20 Total Loss: 0.490, BCE loss: 0.429, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 2 Step: 21 Total Loss: 0.535, BCE loss: 0.414, Diversity Loss: 0.243                     (Diversity_weight = 0.5)
Epoch: 2 Step: 22 Total Loss: 0.537, BCE loss: 0.445, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 2 Step: 23 Total Loss: 0.491, BCE loss: 0.420, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 2 Step: 24 Total Loss: 0.703, BCE loss: 0.596, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 2 Step: 25 Total Loss: 0.515, BCE loss: 0.384, Diversity Loss: 0.263                     (Diversity_weight = 0.5)
Epoch: 2 Step: 26 Total Loss: 0.512, BCE loss: 0.436, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 2 Step: 27 Total Loss: 0.500, BCE loss: 0.389, Diversity Loss: 0.223                     (Diversity_weight = 0.5)
Epoch: 2 Step: 28 Total Loss: 0.489, BCE loss: 0.404, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 2 Step: 29 Total Loss: 0.703, BCE loss: 0.644, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 2 Step: 30 Total Loss: 0.571, BCE loss: 0.496, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 2 Step: 31 Total Loss: 0.502, BCE loss: 0.444, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 2 Step: 32 Total Loss: 0.644, BCE loss: 0.546, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 2 Step: 33 Total Loss: 0.544, BCE loss: 0.456, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 2 Step: 34 Total Loss: 0.872, BCE loss: 0.781, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 2 Step: 35 Total Loss: 0.723, BCE loss: 0.645, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 2 Step: 36 Total Loss: 0.613, BCE loss: 0.537, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 2 Step: 37 Total Loss: 0.562, BCE loss: 0.477, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 2 Step: 38 Total Loss: 0.811, BCE loss: 0.718, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 2 Step: 39 Total Loss: 0.684, BCE loss: 0.586, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 2 Step: 40 Total Loss: 0.742, BCE loss: 0.675, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 2 Step: 41 Total Loss: 0.523, BCE loss: 0.427, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 2 Step: 42 Total Loss: 0.617, BCE loss: 0.564, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 2 Step: 43 Total Loss: 0.616, BCE loss: 0.549, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 2 Step: 44 Total Loss: 0.503, BCE loss: 0.420, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 2 Step: 45 Total Loss: 0.517, BCE loss: 0.387, Diversity Loss: 0.261                     (Diversity_weight = 0.5)
Epoch: 2 Step: 46 Total Loss: 0.573, BCE loss: 0.511, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 2 Step: 47 Total Loss: 0.670, BCE loss: 0.597, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 2 Step: 48 Total Loss: 0.579, BCE loss: 0.506, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 2 Step: 49 Total Loss: 0.652, BCE loss: 0.594, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 2 Step: 50 Total Loss: 0.534, BCE loss: 0.457, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 2 Step: 51 Total Loss: 0.615, BCE loss: 0.558, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 2 Step: 52 Total Loss: 0.590, BCE loss: 0.499, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 2 Step: 53 Total Loss: 0.596, BCE loss: 0.510, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 2 Step: 54 Total Loss: 0.693, BCE loss: 0.536, Diversity Loss: 0.315                     (Diversity_weight = 0.5)
Epoch: 2 Step: 55 Total Loss: 0.527, BCE loss: 0.463, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 2 Step: 56 Total Loss: 0.561, BCE loss: 0.483, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 2 Step: 57 Total Loss: 0.583, BCE loss: 0.494, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 2 Step: 58 Total Loss: 0.617, BCE loss: 0.549, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 2 Step: 59 Total Loss: 0.489, BCE loss: 0.417, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 2 Step: 60 Total Loss: 0.511, BCE loss: 0.437, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 2 Step: 61 Total Loss: 0.606, BCE loss: 0.555, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 2 Step: 62 Total Loss: 0.507, BCE loss: 0.445, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
{'accuracy': 0.6606606606606606, 'roc_auc': 0.758611291897169, 'pr_auc': 0.8037335942250861, 'conicity_mean': 0.15812606, 'conicity_std': 0.055767767}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.686    0.631      0.661      0.658         0.655
precision    0.578    0.808      0.661      0.693         0.707
recall       0.843    0.518      0.661      0.680         0.661
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.758611291897169
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:14:54_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.571, BCE loss: 0.483, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 3 Step: 1 Total Loss: 0.444, BCE loss: 0.363, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 3 Step: 2 Total Loss: 0.481, BCE loss: 0.406, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 3 Step: 3 Total Loss: 0.550, BCE loss: 0.484, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 3 Step: 4 Total Loss: 0.393, BCE loss: 0.309, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 3 Step: 5 Total Loss: 0.573, BCE loss: 0.516, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 3 Step: 6 Total Loss: 0.431, BCE loss: 0.358, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 3 Step: 7 Total Loss: 0.460, BCE loss: 0.398, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 3 Step: 8 Total Loss: 0.685, BCE loss: 0.627, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 3 Step: 9 Total Loss: 0.428, BCE loss: 0.362, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 3 Step: 10 Total Loss: 0.404, BCE loss: 0.301, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 3 Step: 11 Total Loss: 0.580, BCE loss: 0.510, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 3 Step: 12 Total Loss: 0.476, BCE loss: 0.349, Diversity Loss: 0.253                     (Diversity_weight = 0.5)
Epoch: 3 Step: 13 Total Loss: 0.338, BCE loss: 0.259, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 3 Step: 14 Total Loss: 0.420, BCE loss: 0.316, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 3 Step: 15 Total Loss: 0.516, BCE loss: 0.444, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 3 Step: 16 Total Loss: 0.316, BCE loss: 0.198, Diversity Loss: 0.235                     (Diversity_weight = 0.5)
Epoch: 3 Step: 17 Total Loss: 0.518, BCE loss: 0.422, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 3 Step: 18 Total Loss: 0.388, BCE loss: 0.326, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 3 Step: 19 Total Loss: 0.439, BCE loss: 0.374, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 3 Step: 20 Total Loss: 0.548, BCE loss: 0.476, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 3 Step: 21 Total Loss: 0.402, BCE loss: 0.287, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 3 Step: 22 Total Loss: 0.443, BCE loss: 0.369, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 3 Step: 23 Total Loss: 0.375, BCE loss: 0.301, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 24 Total Loss: 0.474, BCE loss: 0.403, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 3 Step: 25 Total Loss: 0.495, BCE loss: 0.437, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 3 Step: 26 Total Loss: 0.469, BCE loss: 0.417, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 3 Step: 27 Total Loss: 0.538, BCE loss: 0.476, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 3 Step: 28 Total Loss: 0.455, BCE loss: 0.392, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 3 Step: 29 Total Loss: 0.589, BCE loss: 0.490, Diversity Loss: 0.198                     (Diversity_weight = 0.5)
Epoch: 3 Step: 30 Total Loss: 0.344, BCE loss: 0.266, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 3 Step: 31 Total Loss: 0.481, BCE loss: 0.402, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 3 Step: 32 Total Loss: 0.434, BCE loss: 0.345, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 3 Step: 33 Total Loss: 0.625, BCE loss: 0.567, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 3 Step: 34 Total Loss: 0.575, BCE loss: 0.409, Diversity Loss: 0.331                     (Diversity_weight = 0.5)
Epoch: 3 Step: 35 Total Loss: 0.533, BCE loss: 0.471, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 3 Step: 36 Total Loss: 0.425, BCE loss: 0.360, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 3 Step: 37 Total Loss: 0.574, BCE loss: 0.509, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 3 Step: 38 Total Loss: 0.354, BCE loss: 0.271, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 3 Step: 39 Total Loss: 0.477, BCE loss: 0.414, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 3 Step: 40 Total Loss: 0.321, BCE loss: 0.206, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 3 Step: 41 Total Loss: 0.417, BCE loss: 0.327, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 3 Step: 42 Total Loss: 0.395, BCE loss: 0.334, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 3 Step: 43 Total Loss: 0.588, BCE loss: 0.497, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 3 Step: 44 Total Loss: 0.505, BCE loss: 0.455, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 3 Step: 45 Total Loss: 0.568, BCE loss: 0.488, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 3 Step: 46 Total Loss: 0.484, BCE loss: 0.434, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 3 Step: 47 Total Loss: 0.524, BCE loss: 0.447, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 3 Step: 48 Total Loss: 0.626, BCE loss: 0.572, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 3 Step: 49 Total Loss: 0.490, BCE loss: 0.391, Diversity Loss: 0.198                     (Diversity_weight = 0.5)
Epoch: 3 Step: 50 Total Loss: 0.502, BCE loss: 0.435, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 3 Step: 51 Total Loss: 0.378, BCE loss: 0.286, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 3 Step: 52 Total Loss: 0.564, BCE loss: 0.431, Diversity Loss: 0.267                     (Diversity_weight = 0.5)
Epoch: 3 Step: 53 Total Loss: 0.501, BCE loss: 0.430, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 3 Step: 54 Total Loss: 0.502, BCE loss: 0.425, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 3 Step: 55 Total Loss: 0.360, BCE loss: 0.249, Diversity Loss: 0.221                     (Diversity_weight = 0.5)
Epoch: 3 Step: 56 Total Loss: 0.479, BCE loss: 0.394, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 3 Step: 57 Total Loss: 0.293, BCE loss: 0.245, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 3 Step: 58 Total Loss: 0.658, BCE loss: 0.606, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 3 Step: 59 Total Loss: 0.419, BCE loss: 0.338, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 3 Step: 60 Total Loss: 0.570, BCE loss: 0.490, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 3 Step: 61 Total Loss: 0.409, BCE loss: 0.321, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 3 Step: 62 Total Loss: 0.590, BCE loss: 0.496, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
{'accuracy': 0.7067067067067067, 'roc_auc': 0.7718109339407744, 'pr_auc': 0.8053742538979358, 'conicity_mean': 0.15883745, 'conicity_std': 0.05658885}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.653    0.746      0.707      0.700         0.705
precision    0.680    0.725      0.707      0.702         0.705
recall       0.629    0.768      0.707      0.698         0.707
support    439.000  560.000    999.000    999.000       999.000
Model Saved on  roc_auc 0.7718109339407744
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:14:54_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.449, BCE loss: 0.374, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 4 Step: 1 Total Loss: 0.375, BCE loss: 0.299, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 4 Step: 2 Total Loss: 0.516, BCE loss: 0.461, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 4 Step: 3 Total Loss: 0.327, BCE loss: 0.206, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 4 Step: 4 Total Loss: 0.332, BCE loss: 0.266, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 4 Step: 5 Total Loss: 0.322, BCE loss: 0.224, Diversity Loss: 0.196                     (Diversity_weight = 0.5)
Epoch: 4 Step: 6 Total Loss: 0.281, BCE loss: 0.172, Diversity Loss: 0.217                     (Diversity_weight = 0.5)
Epoch: 4 Step: 7 Total Loss: 0.436, BCE loss: 0.379, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 4 Step: 8 Total Loss: 0.320, BCE loss: 0.252, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 4 Step: 9 Total Loss: 0.465, BCE loss: 0.396, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 4 Step: 10 Total Loss: 0.253, BCE loss: 0.187, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 4 Step: 11 Total Loss: 0.354, BCE loss: 0.260, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 4 Step: 12 Total Loss: 0.465, BCE loss: 0.373, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 4 Step: 13 Total Loss: 0.375, BCE loss: 0.250, Diversity Loss: 0.249                     (Diversity_weight = 0.5)
Epoch: 4 Step: 14 Total Loss: 0.358, BCE loss: 0.277, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 4 Step: 15 Total Loss: 0.359, BCE loss: 0.298, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 4 Step: 16 Total Loss: 0.318, BCE loss: 0.247, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 4 Step: 17 Total Loss: 0.389, BCE loss: 0.326, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 4 Step: 18 Total Loss: 0.298, BCE loss: 0.246, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 4 Step: 19 Total Loss: 0.424, BCE loss: 0.334, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 4 Step: 20 Total Loss: 0.398, BCE loss: 0.334, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 4 Step: 21 Total Loss: 0.313, BCE loss: 0.174, Diversity Loss: 0.278                     (Diversity_weight = 0.5)
Epoch: 4 Step: 22 Total Loss: 0.391, BCE loss: 0.315, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 4 Step: 23 Total Loss: 0.330, BCE loss: 0.278, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 4 Step: 24 Total Loss: 0.503, BCE loss: 0.346, Diversity Loss: 0.314                     (Diversity_weight = 0.5)
Epoch: 4 Step: 25 Total Loss: 0.281, BCE loss: 0.217, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 4 Step: 26 Total Loss: 0.319, BCE loss: 0.236, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 4 Step: 27 Total Loss: 0.489, BCE loss: 0.432, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 4 Step: 28 Total Loss: 0.408, BCE loss: 0.319, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 4 Step: 29 Total Loss: 0.557, BCE loss: 0.494, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 4 Step: 30 Total Loss: 0.338, BCE loss: 0.258, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 4 Step: 31 Total Loss: 0.349, BCE loss: 0.283, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 4 Step: 32 Total Loss: 0.266, BCE loss: 0.146, Diversity Loss: 0.239                     (Diversity_weight = 0.5)
Epoch: 4 Step: 33 Total Loss: 0.419, BCE loss: 0.341, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 4 Step: 34 Total Loss: 0.247, BCE loss: 0.185, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 4 Step: 35 Total Loss: 0.447, BCE loss: 0.391, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 4 Step: 36 Total Loss: 0.264, BCE loss: 0.165, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 4 Step: 37 Total Loss: 0.315, BCE loss: 0.242, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 4 Step: 38 Total Loss: 0.279, BCE loss: 0.181, Diversity Loss: 0.196                     (Diversity_weight = 0.5)
Epoch: 4 Step: 39 Total Loss: 0.402, BCE loss: 0.356, Diversity Loss: 0.092                     (Diversity_weight = 0.5)
Epoch: 4 Step: 40 Total Loss: 0.489, BCE loss: 0.413, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 4 Step: 41 Total Loss: 0.395, BCE loss: 0.339, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 4 Step: 42 Total Loss: 0.678, BCE loss: 0.575, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 4 Step: 43 Total Loss: 0.381, BCE loss: 0.304, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 4 Step: 44 Total Loss: 0.468, BCE loss: 0.376, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 4 Step: 45 Total Loss: 0.337, BCE loss: 0.248, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 4 Step: 46 Total Loss: 0.353, BCE loss: 0.283, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 4 Step: 47 Total Loss: 0.286, BCE loss: 0.205, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 4 Step: 48 Total Loss: 0.380, BCE loss: 0.302, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 4 Step: 49 Total Loss: 0.534, BCE loss: 0.451, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 4 Step: 50 Total Loss: 0.416, BCE loss: 0.320, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 4 Step: 51 Total Loss: 0.342, BCE loss: 0.262, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 4 Step: 52 Total Loss: 0.495, BCE loss: 0.426, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 4 Step: 53 Total Loss: 0.277, BCE loss: 0.170, Diversity Loss: 0.214                     (Diversity_weight = 0.5)
Epoch: 4 Step: 54 Total Loss: 0.375, BCE loss: 0.298, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 4 Step: 55 Total Loss: 0.344, BCE loss: 0.271, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 4 Step: 56 Total Loss: 0.432, BCE loss: 0.354, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 4 Step: 57 Total Loss: 0.337, BCE loss: 0.277, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 4 Step: 58 Total Loss: 0.400, BCE loss: 0.319, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 4 Step: 59 Total Loss: 0.286, BCE loss: 0.200, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 4 Step: 60 Total Loss: 0.495, BCE loss: 0.434, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 4 Step: 61 Total Loss: 0.278, BCE loss: 0.160, Diversity Loss: 0.235                     (Diversity_weight = 0.5)
Epoch: 4 Step: 62 Total Loss: 0.438, BCE loss: 0.372, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
{'accuracy': 0.6746746746746747, 'roc_auc': 0.7491132443865929, 'pr_auc': 0.7951510038979037, 'conicity_mean': 0.15551203, 'conicity_std': 0.05544315}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.642    0.702      0.675      0.672         0.676
precision    0.622    0.721      0.675      0.672         0.678
recall       0.663    0.684      0.675      0.673         0.675
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7491132443865929
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:14:54_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.284, BCE loss: 0.225, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 5 Step: 1 Total Loss: 0.268, BCE loss: 0.205, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 5 Step: 2 Total Loss: 0.367, BCE loss: 0.315, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 5 Step: 3 Total Loss: 0.184, BCE loss: 0.125, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 5 Step: 4 Total Loss: 0.278, BCE loss: 0.214, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 5 Step: 5 Total Loss: 0.257, BCE loss: 0.144, Diversity Loss: 0.227                     (Diversity_weight = 0.5)
Epoch: 5 Step: 6 Total Loss: 0.291, BCE loss: 0.205, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 5 Step: 7 Total Loss: 0.403, BCE loss: 0.316, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 5 Step: 8 Total Loss: 0.194, BCE loss: 0.087, Diversity Loss: 0.213                     (Diversity_weight = 0.5)
Epoch: 5 Step: 9 Total Loss: 0.239, BCE loss: 0.161, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 5 Step: 10 Total Loss: 0.238, BCE loss: 0.152, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 5 Step: 11 Total Loss: 0.235, BCE loss: 0.173, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 5 Step: 12 Total Loss: 0.246, BCE loss: 0.161, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 5 Step: 13 Total Loss: 0.241, BCE loss: 0.191, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 5 Step: 14 Total Loss: 0.227, BCE loss: 0.156, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 5 Step: 15 Total Loss: 0.188, BCE loss: 0.111, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 5 Step: 16 Total Loss: 0.148, BCE loss: 0.077, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 5 Step: 17 Total Loss: 0.408, BCE loss: 0.351, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 5 Step: 18 Total Loss: 0.319, BCE loss: 0.252, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 5 Step: 19 Total Loss: 0.295, BCE loss: 0.201, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 5 Step: 20 Total Loss: 0.195, BCE loss: 0.129, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 5 Step: 21 Total Loss: 0.246, BCE loss: 0.146, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 5 Step: 22 Total Loss: 0.296, BCE loss: 0.207, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 5 Step: 23 Total Loss: 0.190, BCE loss: 0.069, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 5 Step: 24 Total Loss: 0.198, BCE loss: 0.116, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 5 Step: 25 Total Loss: 0.260, BCE loss: 0.157, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 5 Step: 26 Total Loss: 0.331, BCE loss: 0.264, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 5 Step: 27 Total Loss: 0.325, BCE loss: 0.247, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 5 Step: 28 Total Loss: 0.235, BCE loss: 0.143, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 5 Step: 29 Total Loss: 0.251, BCE loss: 0.125, Diversity Loss: 0.252                     (Diversity_weight = 0.5)
Epoch: 5 Step: 30 Total Loss: 0.373, BCE loss: 0.297, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 5 Step: 31 Total Loss: 0.353, BCE loss: 0.294, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 5 Step: 32 Total Loss: 0.229, BCE loss: 0.127, Diversity Loss: 0.203                     (Diversity_weight = 0.5)
Epoch: 5 Step: 33 Total Loss: 0.178, BCE loss: 0.086, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 5 Step: 34 Total Loss: 0.294, BCE loss: 0.217, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 5 Step: 35 Total Loss: 0.346, BCE loss: 0.268, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 5 Step: 36 Total Loss: 0.215, BCE loss: 0.159, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 5 Step: 37 Total Loss: 0.327, BCE loss: 0.270, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 5 Step: 38 Total Loss: 0.248, BCE loss: 0.177, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 5 Step: 39 Total Loss: 0.367, BCE loss: 0.289, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 5 Step: 40 Total Loss: 0.286, BCE loss: 0.225, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 5 Step: 41 Total Loss: 0.226, BCE loss: 0.166, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 5 Step: 42 Total Loss: 0.213, BCE loss: 0.137, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 5 Step: 43 Total Loss: 0.436, BCE loss: 0.386, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 5 Step: 44 Total Loss: 0.351, BCE loss: 0.292, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 5 Step: 45 Total Loss: 0.502, BCE loss: 0.349, Diversity Loss: 0.307                     (Diversity_weight = 0.5)
Epoch: 5 Step: 46 Total Loss: 0.378, BCE loss: 0.289, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 5 Step: 47 Total Loss: 0.693, BCE loss: 0.622, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 5 Step: 48 Total Loss: 0.362, BCE loss: 0.277, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 5 Step: 49 Total Loss: 0.250, BCE loss: 0.162, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 5 Step: 50 Total Loss: 0.271, BCE loss: 0.161, Diversity Loss: 0.220                     (Diversity_weight = 0.5)
Epoch: 5 Step: 51 Total Loss: 0.169, BCE loss: 0.093, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 5 Step: 52 Total Loss: 0.151, BCE loss: 0.077, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 5 Step: 53 Total Loss: 0.396, BCE loss: 0.337, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 5 Step: 54 Total Loss: 0.312, BCE loss: 0.234, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 5 Step: 55 Total Loss: 0.379, BCE loss: 0.285, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 5 Step: 56 Total Loss: 0.388, BCE loss: 0.266, Diversity Loss: 0.244                     (Diversity_weight = 0.5)
Epoch: 5 Step: 57 Total Loss: 0.226, BCE loss: 0.139, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 5 Step: 58 Total Loss: 0.205, BCE loss: 0.131, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 5 Step: 59 Total Loss: 0.397, BCE loss: 0.342, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 5 Step: 60 Total Loss: 0.133, BCE loss: 0.065, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 5 Step: 61 Total Loss: 0.310, BCE loss: 0.244, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 5 Step: 62 Total Loss: 0.321, BCE loss: 0.180, Diversity Loss: 0.281                     (Diversity_weight = 0.5)
{'accuracy': 0.6926926926926927, 'roc_auc': 0.7460787504067687, 'pr_auc': 0.7882992617482378, 'conicity_mean': 0.15889362, 'conicity_std': 0.057050206}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.662    0.718      0.693      0.690         0.694
precision    0.640    0.739      0.693      0.690         0.696
recall       0.686    0.698      0.693      0.692         0.693
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7460787504067687
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:14:54_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.292, BCE loss: 0.234, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 6 Step: 1 Total Loss: 0.213, BCE loss: 0.161, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 6 Step: 2 Total Loss: 0.217, BCE loss: 0.165, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 6 Step: 3 Total Loss: 0.279, BCE loss: 0.216, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 6 Step: 4 Total Loss: 0.167, BCE loss: 0.088, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 6 Step: 5 Total Loss: 0.229, BCE loss: 0.138, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 6 Step: 6 Total Loss: 0.191, BCE loss: 0.089, Diversity Loss: 0.204                     (Diversity_weight = 0.5)
Epoch: 6 Step: 7 Total Loss: 0.206, BCE loss: 0.139, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 6 Step: 8 Total Loss: 0.214, BCE loss: 0.130, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 6 Step: 9 Total Loss: 0.186, BCE loss: 0.106, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 6 Step: 10 Total Loss: 0.167, BCE loss: 0.112, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 6 Step: 11 Total Loss: 0.145, BCE loss: 0.076, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 6 Step: 12 Total Loss: 0.208, BCE loss: 0.115, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 6 Step: 13 Total Loss: 0.176, BCE loss: 0.121, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 6 Step: 14 Total Loss: 0.280, BCE loss: 0.203, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 6 Step: 15 Total Loss: 0.176, BCE loss: 0.106, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 6 Step: 16 Total Loss: 0.158, BCE loss: 0.057, Diversity Loss: 0.202                     (Diversity_weight = 0.5)
Epoch: 6 Step: 17 Total Loss: 0.166, BCE loss: 0.093, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 6 Step: 18 Total Loss: 0.248, BCE loss: 0.188, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 6 Step: 19 Total Loss: 0.159, BCE loss: 0.110, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 6 Step: 20 Total Loss: 0.097, BCE loss: 0.047, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 6 Step: 21 Total Loss: 0.185, BCE loss: 0.140, Diversity Loss: 0.090                     (Diversity_weight = 0.5)
Epoch: 6 Step: 22 Total Loss: 0.128, BCE loss: 0.056, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 6 Step: 23 Total Loss: 0.179, BCE loss: 0.103, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 6 Step: 24 Total Loss: 0.371, BCE loss: 0.280, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 6 Step: 25 Total Loss: 0.294, BCE loss: 0.160, Diversity Loss: 0.269                     (Diversity_weight = 0.5)
Epoch: 6 Step: 26 Total Loss: 0.178, BCE loss: 0.121, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 6 Step: 27 Total Loss: 0.236, BCE loss: 0.176, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 6 Step: 28 Total Loss: 0.321, BCE loss: 0.248, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 6 Step: 29 Total Loss: 0.189, BCE loss: 0.107, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 6 Step: 30 Total Loss: 0.182, BCE loss: 0.096, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 6 Step: 31 Total Loss: 0.383, BCE loss: 0.320, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 6 Step: 32 Total Loss: 0.223, BCE loss: 0.160, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 6 Step: 33 Total Loss: 0.277, BCE loss: 0.211, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 6 Step: 34 Total Loss: 0.262, BCE loss: 0.164, Diversity Loss: 0.197                     (Diversity_weight = 0.5)
Epoch: 6 Step: 35 Total Loss: 0.208, BCE loss: 0.117, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 6 Step: 36 Total Loss: 0.293, BCE loss: 0.219, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 6 Step: 37 Total Loss: 0.179, BCE loss: 0.114, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 6 Step: 38 Total Loss: 0.386, BCE loss: 0.235, Diversity Loss: 0.303                     (Diversity_weight = 0.5)
Epoch: 6 Step: 39 Total Loss: 0.138, BCE loss: 0.051, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 6 Step: 40 Total Loss: 0.250, BCE loss: 0.163, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 6 Step: 41 Total Loss: 0.141, BCE loss: 0.063, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 6 Step: 42 Total Loss: 0.197, BCE loss: 0.127, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 6 Step: 43 Total Loss: 0.224, BCE loss: 0.112, Diversity Loss: 0.225                     (Diversity_weight = 0.5)
Epoch: 6 Step: 44 Total Loss: 0.199, BCE loss: 0.116, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 6 Step: 45 Total Loss: 0.378, BCE loss: 0.317, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 6 Step: 46 Total Loss: 0.175, BCE loss: 0.116, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 6 Step: 47 Total Loss: 0.173, BCE loss: 0.105, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 6 Step: 48 Total Loss: 0.230, BCE loss: 0.120, Diversity Loss: 0.219                     (Diversity_weight = 0.5)
Epoch: 6 Step: 49 Total Loss: 0.281, BCE loss: 0.182, Diversity Loss: 0.198                     (Diversity_weight = 0.5)
Epoch: 6 Step: 50 Total Loss: 0.329, BCE loss: 0.239, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 6 Step: 51 Total Loss: 0.202, BCE loss: 0.089, Diversity Loss: 0.227                     (Diversity_weight = 0.5)
Epoch: 6 Step: 52 Total Loss: 0.160, BCE loss: 0.082, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 6 Step: 53 Total Loss: 0.172, BCE loss: 0.047, Diversity Loss: 0.250                     (Diversity_weight = 0.5)
Epoch: 6 Step: 54 Total Loss: 0.290, BCE loss: 0.220, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 6 Step: 55 Total Loss: 0.202, BCE loss: 0.063, Diversity Loss: 0.277                     (Diversity_weight = 0.5)
Epoch: 6 Step: 56 Total Loss: 0.190, BCE loss: 0.116, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 6 Step: 57 Total Loss: 0.173, BCE loss: 0.090, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 6 Step: 58 Total Loss: 0.225, BCE loss: 0.137, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 6 Step: 59 Total Loss: 0.186, BCE loss: 0.124, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 6 Step: 60 Total Loss: 0.318, BCE loss: 0.254, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 6 Step: 61 Total Loss: 0.182, BCE loss: 0.120, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 6 Step: 62 Total Loss: 0.133, BCE loss: 0.052, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
{'accuracy': 0.6906906906906907, 'roc_auc': 0.7494589977220957, 'pr_auc': 0.7917348109798503, 'conicity_mean': 0.1584358, 'conicity_std': 0.056603715}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.659    0.717      0.691      0.688         0.691
precision    0.639    0.735      0.691      0.687         0.693
recall       0.679    0.700      0.691      0.689         0.691
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7494589977220957
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:14:54_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.128, BCE loss: 0.039, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 7 Step: 1 Total Loss: 0.142, BCE loss: 0.067, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 7 Step: 2 Total Loss: 0.202, BCE loss: 0.148, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 7 Step: 3 Total Loss: 0.217, BCE loss: 0.126, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 7 Step: 4 Total Loss: 0.191, BCE loss: 0.052, Diversity Loss: 0.280                     (Diversity_weight = 0.5)
Epoch: 7 Step: 5 Total Loss: 0.117, BCE loss: 0.045, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 7 Step: 6 Total Loss: 0.194, BCE loss: 0.117, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 7 Step: 7 Total Loss: 0.163, BCE loss: 0.069, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 7 Step: 8 Total Loss: 0.218, BCE loss: 0.123, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 7 Step: 9 Total Loss: 0.112, BCE loss: 0.041, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 7 Step: 10 Total Loss: 0.154, BCE loss: 0.066, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 7 Step: 11 Total Loss: 0.120, BCE loss: 0.069, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 7 Step: 12 Total Loss: 0.208, BCE loss: 0.136, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 7 Step: 13 Total Loss: 0.110, BCE loss: 0.034, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 7 Step: 14 Total Loss: 0.111, BCE loss: 0.031, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 7 Step: 15 Total Loss: 0.159, BCE loss: 0.092, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 7 Step: 16 Total Loss: 0.154, BCE loss: 0.082, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 7 Step: 17 Total Loss: 0.087, BCE loss: 0.016, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 7 Step: 18 Total Loss: 0.150, BCE loss: 0.089, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 7 Step: 19 Total Loss: 0.123, BCE loss: 0.050, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 7 Step: 20 Total Loss: 0.130, BCE loss: 0.021, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 7 Step: 21 Total Loss: 0.205, BCE loss: 0.133, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 7 Step: 22 Total Loss: 0.158, BCE loss: 0.074, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 7 Step: 23 Total Loss: 0.248, BCE loss: 0.155, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 7 Step: 24 Total Loss: 0.103, BCE loss: 0.030, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 7 Step: 25 Total Loss: 0.196, BCE loss: 0.080, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 7 Step: 26 Total Loss: 0.171, BCE loss: 0.102, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 7 Step: 27 Total Loss: 0.284, BCE loss: 0.129, Diversity Loss: 0.310                     (Diversity_weight = 0.5)
Epoch: 7 Step: 28 Total Loss: 0.138, BCE loss: 0.076, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 7 Step: 29 Total Loss: 0.228, BCE loss: 0.147, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 7 Step: 30 Total Loss: 0.138, BCE loss: 0.054, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 7 Step: 31 Total Loss: 0.143, BCE loss: 0.036, Diversity Loss: 0.214                     (Diversity_weight = 0.5)
Epoch: 7 Step: 32 Total Loss: 0.312, BCE loss: 0.249, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 7 Step: 33 Total Loss: 0.274, BCE loss: 0.153, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 7 Step: 34 Total Loss: 0.194, BCE loss: 0.145, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 7 Step: 35 Total Loss: 0.117, BCE loss: 0.044, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 7 Step: 36 Total Loss: 0.238, BCE loss: 0.179, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 7 Step: 37 Total Loss: 0.150, BCE loss: 0.087, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 7 Step: 38 Total Loss: 0.141, BCE loss: 0.055, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 7 Step: 39 Total Loss: 0.101, BCE loss: 0.038, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 7 Step: 40 Total Loss: 0.230, BCE loss: 0.179, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 7 Step: 41 Total Loss: 0.161, BCE loss: 0.103, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 7 Step: 42 Total Loss: 0.145, BCE loss: 0.098, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 7 Step: 43 Total Loss: 0.209, BCE loss: 0.121, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 7 Step: 44 Total Loss: 0.240, BCE loss: 0.160, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 7 Step: 45 Total Loss: 0.215, BCE loss: 0.131, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 7 Step: 46 Total Loss: 0.080, BCE loss: 0.031, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 7 Step: 47 Total Loss: 0.214, BCE loss: 0.159, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 7 Step: 48 Total Loss: 0.133, BCE loss: 0.070, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 7 Step: 49 Total Loss: 0.189, BCE loss: 0.071, Diversity Loss: 0.236                     (Diversity_weight = 0.5)
Epoch: 7 Step: 50 Total Loss: 0.176, BCE loss: 0.115, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 7 Step: 51 Total Loss: 0.126, BCE loss: 0.048, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 7 Step: 52 Total Loss: 0.152, BCE loss: 0.055, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 7 Step: 53 Total Loss: 0.211, BCE loss: 0.133, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 7 Step: 54 Total Loss: 0.319, BCE loss: 0.252, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 7 Step: 55 Total Loss: 0.140, BCE loss: 0.059, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 7 Step: 56 Total Loss: 0.213, BCE loss: 0.155, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 7 Step: 57 Total Loss: 0.188, BCE loss: 0.090, Diversity Loss: 0.197                     (Diversity_weight = 0.5)
Epoch: 7 Step: 58 Total Loss: 0.258, BCE loss: 0.203, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 7 Step: 59 Total Loss: 0.157, BCE loss: 0.037, Diversity Loss: 0.240                     (Diversity_weight = 0.5)
Epoch: 7 Step: 60 Total Loss: 0.097, BCE loss: 0.028, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 7 Step: 61 Total Loss: 0.152, BCE loss: 0.090, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 7 Step: 62 Total Loss: 0.085, BCE loss: 0.025, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
{'accuracy': 0.6926926926926927, 'roc_auc': 0.7523267165636186, 'pr_auc': 0.7866085677261545, 'conicity_mean': 0.15985745, 'conicity_std': 0.05606983}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.641    0.731      0.693      0.686         0.692
precision    0.659    0.717      0.693      0.688         0.691
recall       0.624    0.746      0.693      0.685         0.693
support    439.000  560.000    999.000    999.000       999.000
Model not saved on  roc_auc 0.7523267165636186
saved config  {'model': {'encoder': {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_en/cls_en/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:14:54_2021
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:15:49,549 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:15:49,549 - type = vanillalstm
INFO - 2021-01-17 19:15:49,550 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:15:49,550 - vocab_size = 666
INFO - 2021-01-17 19:15:49,550 - embed_size = 200
INFO - 2021-01-17 19:15:49,550 - hidden_size = 128
INFO - 2021-01-17 19:15:49,550 - pre_embed = None
INFO - 2021-01-17 19:15:49,564 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:15:49,565 - hidden_size = 256
INFO - 2021-01-17 19:15:49,565 - output_size = 1
INFO - 2021-01-17 19:15:49,565 - use_attention = True
INFO - 2021-01-17 19:15:49,565 - regularizer_attention = None
INFO - 2021-01-17 19:15:49,565 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b45126f50d0> and extras set()
INFO - 2021-01-17 19:15:49,565 - attention.type = tanh
INFO - 2021-01-17 19:15:49,565 - type = tanh
INFO - 2021-01-17 19:15:49,565 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b45126f50d0> and extras set()
INFO - 2021-01-17 19:15:49,565 - attention.hidden_size = 256
INFO - 2021-01-17 19:15:49,565 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7327327327327328, 'roc_auc': 0.8185236734793544, 'pr_auc': 0.7661077158660168, 'conicity_mean': '0.1594113', 'conicity_std': '0.059545398'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.743    0.722      0.733      0.732         0.734
precision    0.809    0.663      0.733      0.736         0.745
recall       0.686    0.792      0.733      0.739         0.733
support    561.000  438.000    999.000    999.000       999.000
encoder params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:15:51,009 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 666, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:15:51,010 - type = vanillalstm
INFO - 2021-01-17 19:15:51,010 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 666, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:15:51,010 - vocab_size = 666
INFO - 2021-01-17 19:15:51,010 - embed_size = 200
INFO - 2021-01-17 19:15:51,010 - hidden_size = 128
INFO - 2021-01-17 19:15:51,010 - pre_embed = None
INFO - 2021-01-17 19:15:51,024 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:15:51,024 - hidden_size = 256
INFO - 2021-01-17 19:15:51,024 - output_size = 1
INFO - 2021-01-17 19:15:51,024 - use_attention = True
INFO - 2021-01-17 19:15:51,024 - regularizer_attention = None
INFO - 2021-01-17 19:15:51,024 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b4512084850> and extras set()
INFO - 2021-01-17 19:15:51,025 - attention.type = tanh
INFO - 2021-01-17 19:15:51,025 - type = tanh
INFO - 2021-01-17 19:15:51,025 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b4512084850> and extras set()
INFO - 2021-01-17 19:15:51,025 - attention.hidden_size = 256
INFO - 2021-01-17 19:15:51,025 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_en', 'exp_dirname': 'cls_en/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7327327327327328, 'roc_auc': 0.8185236734793544, 'pr_auc': 0.7661077158660168, 'conicity_mean': '0.1594113', 'conicity_std': '0.059545398'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.743    0.722      0.733      0.732         0.734
precision    0.809    0.663      0.733      0.736         0.745
recall       0.686    0.792      0.733      0.739         0.733
support    561.000  438.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_de vanilla_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:16:01,523 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:16:01,523 - type = vanillalstm
INFO - 2021-01-17 19:16:01,523 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:16:01,539 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:16:01,863 - vocab_size = 648
INFO - 2021-01-17 19:16:01,863 - embed_size = 200
INFO - 2021-01-17 19:16:01,863 - hidden_size = 128
INFO - 2021-01-17 19:16:01,863 - pre_embed = None
INFO - 2021-01-17 19:16:04,950 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:16:04,950 - hidden_size = 256
INFO - 2021-01-17 19:16:04,950 - output_size = 1
INFO - 2021-01-17 19:16:04,950 - use_attention = True
INFO - 2021-01-17 19:16:04,950 - regularizer_attention = None
INFO - 2021-01-17 19:16:04,951 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ab0a00706d0> and extras set()
INFO - 2021-01-17 19:16:04,951 - attention.type = tanh
INFO - 2021-01-17 19:16:04,951 - type = tanh
INFO - 2021-01-17 19:16:04,951 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ab0a00706d0> and extras set()
INFO - 2021-01-17 19:16:04,951 - attention.hidden_size = 256
INFO - 2021-01-17 19:16:04,951 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.720, BCE loss: 0.720, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.724, BCE loss: 0.724, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.698                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.740, BCE loss: 0.740, Diversity Loss: 0.591                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.673, BCE loss: 0.673, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.663                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.728, BCE loss: 0.728, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.654, BCE loss: 0.654, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.701, BCE loss: 0.701, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.650, BCE loss: 0.650, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.714, BCE loss: 0.714, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.643                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.623, BCE loss: 0.623, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.655, BCE loss: 0.655, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.720                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.612, BCE loss: 0.612, Diversity Loss: 0.645                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.768                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.597, BCE loss: 0.597, Diversity Loss: 0.737                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.798, BCE loss: 0.798, Diversity Loss: 0.775                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.719, BCE loss: 0.719, Diversity Loss: 0.717                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.666, BCE loss: 0.666, Diversity Loss: 0.711                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.738                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.571, BCE loss: 0.571, Diversity Loss: 0.707                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.746, BCE loss: 0.746, Diversity Loss: 0.659                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.585, BCE loss: 0.585, Diversity Loss: 0.817                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.677                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.597, BCE loss: 0.597, Diversity Loss: 0.805                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.626, BCE loss: 0.626, Diversity Loss: 0.792                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.751, BCE loss: 0.751, Diversity Loss: 0.747                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.631, BCE loss: 0.631, Diversity Loss: 0.781                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.627, BCE loss: 0.627, Diversity Loss: 0.736                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.760                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.708                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.733, BCE loss: 0.733, Diversity Loss: 0.803                     (Diversity_weight = 0)
{'accuracy': 0.5616850551654965, 'roc_auc': 0.657426563518833, 'pr_auc': 0.6113797595727207, 'conicity_mean': 0.75344586, 'conicity_std': 0.09695634}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.314    0.678      0.562      0.496         0.493
precision    0.763    0.531      0.562      0.647         0.649
recall       0.198    0.937      0.562      0.567         0.562
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.657426563518833
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:16:05_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.715, BCE loss: 0.715, Diversity Loss: 0.711                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.761                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.634, BCE loss: 0.634, Diversity Loss: 0.684                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.662                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.643, BCE loss: 0.643, Diversity Loss: 0.689                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.603, BCE loss: 0.603, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.644, BCE loss: 0.644, Diversity Loss: 0.778                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.639, BCE loss: 0.639, Diversity Loss: 0.656                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.593, BCE loss: 0.593, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.582                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.644, BCE loss: 0.644, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.587, BCE loss: 0.587, Diversity Loss: 0.726                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.623, BCE loss: 0.623, Diversity Loss: 0.704                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.665, BCE loss: 0.665, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.600, BCE loss: 0.600, Diversity Loss: 0.757                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.553, BCE loss: 0.553, Diversity Loss: 0.643                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.597, BCE loss: 0.597, Diversity Loss: 0.620                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.599, BCE loss: 0.599, Diversity Loss: 0.613                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.612, BCE loss: 0.612, Diversity Loss: 0.689                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.609, BCE loss: 0.609, Diversity Loss: 0.756                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.607, BCE loss: 0.607, Diversity Loss: 0.731                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.587, BCE loss: 0.587, Diversity Loss: 0.769                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.750                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.591, BCE loss: 0.591, Diversity Loss: 0.649                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.569, BCE loss: 0.569, Diversity Loss: 0.706                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.613                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.645, BCE loss: 0.645, Diversity Loss: 0.836                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.657                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.700                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.646                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.548, BCE loss: 0.548, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.757                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.703, BCE loss: 0.703, Diversity Loss: 0.643                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.641                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.747, BCE loss: 0.747, Diversity Loss: 0.757                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.605, BCE loss: 0.605, Diversity Loss: 0.720                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.591, BCE loss: 0.591, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.570, BCE loss: 0.570, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.654, BCE loss: 0.654, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.826, BCE loss: 0.826, Diversity Loss: 0.692                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.630, BCE loss: 0.630, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.606, BCE loss: 0.606, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.612, BCE loss: 0.612, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.587, BCE loss: 0.587, Diversity Loss: 0.668                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.637, BCE loss: 0.637, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.561, BCE loss: 0.561, Diversity Loss: 0.617                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.620                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.616, BCE loss: 0.616, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.739, BCE loss: 0.739, Diversity Loss: 0.655                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.652, BCE loss: 0.652, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.569, BCE loss: 0.569, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.580, BCE loss: 0.580, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.509, BCE loss: 0.509, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.627, BCE loss: 0.627, Diversity Loss: 0.594                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.556, BCE loss: 0.556, Diversity Loss: 0.489                     (Diversity_weight = 0)
{'accuracy': 0.7001003009027081, 'roc_auc': 0.7502314386224773, 'pr_auc': 0.7035728759708196, 'conicity_mean': 0.5445338, 'conicity_std': 0.12671657}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.722    0.675        0.7      0.698         0.699
precision    0.682    0.724        0.7      0.703         0.703
recall       0.767    0.631        0.7      0.699         0.700
support    506.000  491.000      997.0    997.000       997.000
Model Saved on  roc_auc 0.7502314386224773
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:16:05_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.511, BCE loss: 0.511, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.454, BCE loss: 0.454, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.534, BCE loss: 0.534, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.567, BCE loss: 0.567, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.591                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.525, BCE loss: 0.525, Diversity Loss: 0.704                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.505, BCE loss: 0.505, Diversity Loss: 0.638                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.429, BCE loss: 0.429, Diversity Loss: 0.638                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.694                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.536, BCE loss: 0.536, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.496, BCE loss: 0.496, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.497, BCE loss: 0.497, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.511, BCE loss: 0.511, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.707, BCE loss: 0.707, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.676                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.440, BCE loss: 0.440, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.561, BCE loss: 0.561, Diversity Loss: 0.740                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.424, BCE loss: 0.424, Diversity Loss: 0.720                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.500, BCE loss: 0.500, Diversity Loss: 0.679                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.585, BCE loss: 0.585, Diversity Loss: 0.595                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.386, BCE loss: 0.386, Diversity Loss: 0.666                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.496, BCE loss: 0.496, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.559, BCE loss: 0.559, Diversity Loss: 0.753                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.513, BCE loss: 0.513, Diversity Loss: 0.638                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.559, BCE loss: 0.559, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.496, BCE loss: 0.496, Diversity Loss: 0.684                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.487, BCE loss: 0.487, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.768                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.645                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.629, BCE loss: 0.629, Diversity Loss: 0.691                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.548, BCE loss: 0.548, Diversity Loss: 0.663                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.439, BCE loss: 0.439, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.465, BCE loss: 0.465, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.592, BCE loss: 0.592, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.420, BCE loss: 0.420, Diversity Loss: 0.595                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.580, BCE loss: 0.580, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.492, BCE loss: 0.492, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.482, BCE loss: 0.482, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.462, BCE loss: 0.462, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.571, BCE loss: 0.571, Diversity Loss: 0.633                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.590, BCE loss: 0.590, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.432, BCE loss: 0.432, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.609, BCE loss: 0.609, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.490, BCE loss: 0.490, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.721, BCE loss: 0.721, Diversity Loss: 0.714                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.469, BCE loss: 0.469, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.510, BCE loss: 0.510, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.476, BCE loss: 0.476, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.559, BCE loss: 0.559, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.456, BCE loss: 0.456, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.438, BCE loss: 0.438, Diversity Loss: 0.582                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.530, BCE loss: 0.530, Diversity Loss: 0.617                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.466, BCE loss: 0.466, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.531                     (Diversity_weight = 0)
{'accuracy': 0.7271815446339017, 'roc_auc': 0.8029350442349645, 'pr_auc': 0.7685824407463131, 'conicity_mean': 0.5840009, 'conicity_std': 0.14092658}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.702    0.748      0.727      0.725         0.725
precision    0.787    0.686      0.727      0.736         0.737
recall       0.634    0.823      0.727      0.729         0.727
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.8029350442349645
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:16:05_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.339, BCE loss: 0.339, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.444, BCE loss: 0.444, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.421, BCE loss: 0.421, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.462, BCE loss: 0.462, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.308, BCE loss: 0.308, Diversity Loss: 0.607                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.316, BCE loss: 0.316, Diversity Loss: 0.594                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.616, BCE loss: 0.616, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.586                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.487, BCE loss: 0.487, Diversity Loss: 0.586                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.434, BCE loss: 0.434, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.397, BCE loss: 0.397, Diversity Loss: 0.574                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.520, BCE loss: 0.520, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.400, BCE loss: 0.400, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.374, BCE loss: 0.374, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.477, BCE loss: 0.477, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.400, BCE loss: 0.400, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.366, BCE loss: 0.366, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.357, BCE loss: 0.357, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.340, BCE loss: 0.340, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.597, BCE loss: 0.597, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.211, BCE loss: 0.211, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.434, BCE loss: 0.434, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.399, BCE loss: 0.399, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.409, BCE loss: 0.409, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.389, BCE loss: 0.389, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.585                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.357, BCE loss: 0.357, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.298, BCE loss: 0.298, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.374, BCE loss: 0.374, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.404, BCE loss: 0.404, Diversity Loss: 0.574                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.339, BCE loss: 0.339, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.287, BCE loss: 0.287, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.475, BCE loss: 0.475, Diversity Loss: 0.652                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.312, BCE loss: 0.312, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.381, BCE loss: 0.381, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.705                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.489, BCE loss: 0.489, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.571, BCE loss: 0.571, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.317, BCE loss: 0.317, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.346, BCE loss: 0.346, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.409, BCE loss: 0.409, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.417, BCE loss: 0.417, Diversity Loss: 0.669                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.338, BCE loss: 0.338, Diversity Loss: 0.645                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.516, BCE loss: 0.516, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.323, BCE loss: 0.323, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.353, BCE loss: 0.353, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.470                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.442, BCE loss: 0.442, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.428, BCE loss: 0.428, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.252, BCE loss: 0.252, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.457, BCE loss: 0.457, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.811, BCE loss: 0.811, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.366, BCE loss: 0.366, Diversity Loss: 0.500                     (Diversity_weight = 0)
{'accuracy': 0.7372116349047142, 'roc_auc': 0.8194013990967857, 'pr_auc': 0.8047193453614273, 'conicity_mean': 0.5009241, 'conicity_std': 0.120468006}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.749    0.724      0.737      0.737         0.737
precision    0.727    0.749      0.737      0.738         0.738
recall       0.773    0.701      0.737      0.737         0.737
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.8194013990967857
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:16:05_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.397, BCE loss: 0.397, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.340, BCE loss: 0.340, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.384, BCE loss: 0.384, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.349, BCE loss: 0.349, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.408, BCE loss: 0.408, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.279, BCE loss: 0.279, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.253, BCE loss: 0.253, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.224, BCE loss: 0.224, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.335, BCE loss: 0.335, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.360, BCE loss: 0.360, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.423, BCE loss: 0.423, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.620                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.607                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.659                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.159, BCE loss: 0.159, Diversity Loss: 0.639                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.179, BCE loss: 0.179, Diversity Loss: 0.607                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.255, BCE loss: 0.255, Diversity Loss: 0.630                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.387, BCE loss: 0.387, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.488, BCE loss: 0.488, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.340, BCE loss: 0.340, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.326, BCE loss: 0.326, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.337, BCE loss: 0.337, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.292, BCE loss: 0.292, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.263, BCE loss: 0.263, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.279, BCE loss: 0.279, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.280, BCE loss: 0.280, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.488, BCE loss: 0.488, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.450, BCE loss: 0.450, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.494, BCE loss: 0.494, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.155, BCE loss: 0.155, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.429, BCE loss: 0.429, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.325, BCE loss: 0.325, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.664, BCE loss: 0.664, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.419, BCE loss: 0.419, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.286, BCE loss: 0.286, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.307, BCE loss: 0.307, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.444, BCE loss: 0.444, Diversity Loss: 0.567                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.164, BCE loss: 0.164, Diversity Loss: 0.471                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.603, BCE loss: 0.603, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.432, BCE loss: 0.432, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.386, BCE loss: 0.386, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.397, BCE loss: 0.397, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.225, BCE loss: 0.225, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.249, BCE loss: 0.249, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.242, BCE loss: 0.242, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.408, BCE loss: 0.408, Diversity Loss: 0.537                     (Diversity_weight = 0)
{'accuracy': 0.7372116349047142, 'roc_auc': 0.8234183685790877, 'pr_auc': 0.8105717662613765, 'conicity_mean': 0.530652, 'conicity_std': 0.14070095}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.729    0.745      0.737      0.737         0.737
precision    0.764    0.714      0.737      0.739         0.739
recall       0.698    0.778      0.737      0.738         0.737
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.8234183685790877
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:16:05_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.126, BCE loss: 0.126, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.159, BCE loss: 0.159, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.349, BCE loss: 0.349, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.438, BCE loss: 0.438, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.252, BCE loss: 0.252, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.383, BCE loss: 0.383, Diversity Loss: 0.671                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.232, BCE loss: 0.232, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.347, BCE loss: 0.347, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.136, BCE loss: 0.136, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.372, BCE loss: 0.372, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.218, BCE loss: 0.218, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.187, BCE loss: 0.187, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.213, BCE loss: 0.213, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.207, BCE loss: 0.207, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.206, BCE loss: 0.206, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.195, BCE loss: 0.195, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.314, BCE loss: 0.314, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.323, BCE loss: 0.323, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.275, BCE loss: 0.275, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.607                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.597, BCE loss: 0.597, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.327, BCE loss: 0.327, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.218, BCE loss: 0.218, Diversity Loss: 0.581                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.204, BCE loss: 0.204, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.161, BCE loss: 0.161, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.232, BCE loss: 0.232, Diversity Loss: 0.536                     (Diversity_weight = 0)
{'accuracy': 0.7311935807422267, 'roc_auc': 0.8175015898827108, 'pr_auc': 0.8019690519453411, 'conicity_mean': 0.5146245, 'conicity_std': 0.12814717}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.740    0.722      0.731      0.731         0.731
precision    0.727    0.736      0.731      0.731         0.731
recall       0.753    0.709      0.731      0.731         0.731
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.8175015898827108
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:16:05_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.273, BCE loss: 0.273, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.124, BCE loss: 0.124, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.104, BCE loss: 0.104, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.125, BCE loss: 0.125, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.238, BCE loss: 0.238, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.128, BCE loss: 0.128, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.464, BCE loss: 0.464, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.114, BCE loss: 0.114, Diversity Loss: 0.466                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.235, BCE loss: 0.235, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.301, BCE loss: 0.301, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.050, BCE loss: 0.050, Diversity Loss: 0.561                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.127, BCE loss: 0.127, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.363, BCE loss: 0.363, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.540                     (Diversity_weight = 0)
{'accuracy': 0.724172517552658, 'roc_auc': 0.8079502185585601, 'pr_auc': 0.8036194572261061, 'conicity_mean': 0.51895946, 'conicity_std': 0.13104682}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.734    0.714      0.724      0.724         0.724
precision    0.719    0.730      0.724      0.724         0.724
recall       0.749    0.699      0.724      0.724         0.724
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.8079502185585601
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:16:05_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.014, BCE loss: 0.014, Diversity Loss: 0.643                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.011, BCE loss: 0.011, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.561                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.257, BCE loss: 0.257, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.119, BCE loss: 0.119, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.223, BCE loss: 0.223, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.336, BCE loss: 0.336, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.647                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.349, BCE loss: 0.349, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.216, BCE loss: 0.216, Diversity Loss: 0.470                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.128, BCE loss: 0.128, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.253, BCE loss: 0.253, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.525                     (Diversity_weight = 0)
{'accuracy': 0.7362086258776329, 'roc_auc': 0.8087994976775639, 'pr_auc': 0.8004743929529933, 'conicity_mean': 0.51383513, 'conicity_std': 0.12577705}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.729    0.743      0.736      0.736         0.736
precision    0.761    0.714      0.736      0.738         0.738
recall       0.700    0.774      0.736      0.737         0.736
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.8087994976775639
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
./experiments/cls_de/cls_de/lstm+tanh/Sun_Jan_17_19:16:05_2021
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:17:03,763 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:17:03,763 - type = vanillalstm
INFO - 2021-01-17 19:17:03,763 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:17:03,764 - vocab_size = 648
INFO - 2021-01-17 19:17:03,764 - embed_size = 200
INFO - 2021-01-17 19:17:03,764 - hidden_size = 128
INFO - 2021-01-17 19:17:03,764 - pre_embed = None
INFO - 2021-01-17 19:17:03,779 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:17:03,779 - hidden_size = 256
INFO - 2021-01-17 19:17:03,779 - output_size = 1
INFO - 2021-01-17 19:17:03,779 - use_attention = True
INFO - 2021-01-17 19:17:03,779 - regularizer_attention = None
INFO - 2021-01-17 19:17:03,779 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ab0a06cf0d0> and extras set()
INFO - 2021-01-17 19:17:03,779 - attention.type = tanh
INFO - 2021-01-17 19:17:03,779 - type = tanh
INFO - 2021-01-17 19:17:03,779 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ab0a06cf0d0> and extras set()
INFO - 2021-01-17 19:17:03,779 - attention.hidden_size = 256
INFO - 2021-01-17 19:17:03,780 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7557557557557557, 'roc_auc': 0.83562242427158, 'pr_auc': 0.8489421561883959, 'conicity_mean': '0.5331731', 'conicity_std': '0.1405543'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.746    0.764      0.756      0.755         0.756
precision    0.764    0.749      0.756      0.756         0.756
recall       0.730    0.781      0.756      0.755         0.756
support    492.000  507.000    999.000    999.000       999.000
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:17:05,220 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:17:05,221 - type = vanillalstm
INFO - 2021-01-17 19:17:05,221 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:17:05,221 - vocab_size = 648
INFO - 2021-01-17 19:17:05,221 - embed_size = 200
INFO - 2021-01-17 19:17:05,221 - hidden_size = 128
INFO - 2021-01-17 19:17:05,221 - pre_embed = None
INFO - 2021-01-17 19:17:05,235 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:17:05,235 - hidden_size = 256
INFO - 2021-01-17 19:17:05,235 - output_size = 1
INFO - 2021-01-17 19:17:05,235 - use_attention = True
INFO - 2021-01-17 19:17:05,235 - regularizer_attention = None
INFO - 2021-01-17 19:17:05,236 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ab0a00708d0> and extras set()
INFO - 2021-01-17 19:17:05,236 - attention.type = tanh
INFO - 2021-01-17 19:17:05,236 - type = tanh
INFO - 2021-01-17 19:17:05,236 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ab0a00708d0> and extras set()
INFO - 2021-01-17 19:17:05,236 - attention.hidden_size = 256
INFO - 2021-01-17 19:17:05,236 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7557557557557557, 'roc_auc': 0.83562242427158, 'pr_auc': 0.8489421561883959, 'conicity_mean': '0.5331731', 'conicity_std': '0.1405543'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.746    0.764      0.756      0.755         0.756
precision    0.764    0.749      0.756      0.756         0.756
recall       0.730    0.781      0.756      0.755         0.756
support    492.000  507.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_de ortho_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:17:15,700 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:17:15,700 - type = ortholstm
INFO - 2021-01-17 19:17:15,700 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:17:15,716 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:17:16,039 - vocab_size = 648
INFO - 2021-01-17 19:17:16,040 - embed_size = 200
INFO - 2021-01-17 19:17:16,040 - hidden_size = 128
INFO - 2021-01-17 19:17:16,040 - pre_embed = None
INFO - 2021-01-17 19:17:19,106 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:17:19,107 - hidden_size = 256
INFO - 2021-01-17 19:17:19,107 - output_size = 1
INFO - 2021-01-17 19:17:19,107 - use_attention = True
INFO - 2021-01-17 19:17:19,107 - regularizer_attention = None
INFO - 2021-01-17 19:17:19,107 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b69c39dc750> and extras set()
INFO - 2021-01-17 19:17:19,107 - attention.type = tanh
INFO - 2021-01-17 19:17:19,107 - type = tanh
INFO - 2021-01-17 19:17:19,108 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b69c39dc750> and extras set()
INFO - 2021-01-17 19:17:19,108 - attention.hidden_size = 256
INFO - 2021-01-17 19:17:19,108 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.266                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.088                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.109                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.119                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.724, BCE loss: 0.724, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.673, BCE loss: 0.673, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.707, BCE loss: 0.707, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.664, BCE loss: 0.664, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.645, BCE loss: 0.645, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.623, BCE loss: 0.623, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.717, BCE loss: 0.717, Diversity Loss: 0.118                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.720, BCE loss: 0.720, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.621, BCE loss: 0.621, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.100                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.108                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.648, BCE loss: 0.648, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.655, BCE loss: 0.655, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.118                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.621, BCE loss: 0.621, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.616, BCE loss: 0.616, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.707, BCE loss: 0.707, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.565, BCE loss: 0.565, Diversity Loss: 0.165                     (Diversity_weight = 0)
{'accuracy': 0.5436308926780341, 'roc_auc': 0.699371291950766, 'pr_auc': 0.6471370073410081, 'conicity_mean': 0.16083823, 'conicity_std': 0.040399082}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.670    0.260      0.544      0.465         0.468
precision    0.529    0.645      0.544      0.587         0.586
recall       0.913    0.163      0.544      0.538         0.544
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.699371291950766
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:17:19_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.620, BCE loss: 0.620, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.756, BCE loss: 0.756, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.550, BCE loss: 0.550, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.596, BCE loss: 0.596, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.703, BCE loss: 0.703, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.616, BCE loss: 0.616, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.620, BCE loss: 0.620, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.590, BCE loss: 0.590, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.541, BCE loss: 0.541, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.571, BCE loss: 0.571, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.665, BCE loss: 0.665, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.665, BCE loss: 0.665, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.618, BCE loss: 0.618, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.660, BCE loss: 0.660, Diversity Loss: 0.096                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.547, BCE loss: 0.547, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.255                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.629, BCE loss: 0.629, Diversity Loss: 0.110                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.560, BCE loss: 0.560, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.600, BCE loss: 0.600, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.519, BCE loss: 0.519, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.541, BCE loss: 0.541, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.641, BCE loss: 0.641, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.533, BCE loss: 0.533, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.552, BCE loss: 0.552, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.618, BCE loss: 0.618, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.497, BCE loss: 0.497, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.715, BCE loss: 0.715, Diversity Loss: 0.123                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.428, BCE loss: 0.428, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.628, BCE loss: 0.628, Diversity Loss: 0.114                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.602, BCE loss: 0.602, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.557, BCE loss: 0.557, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.622, BCE loss: 0.622, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.626, BCE loss: 0.626, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.478, BCE loss: 0.478, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.518, BCE loss: 0.518, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.708, BCE loss: 0.708, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.513, BCE loss: 0.513, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.717, BCE loss: 0.717, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.615, BCE loss: 0.615, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.645, BCE loss: 0.645, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.520, BCE loss: 0.520, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.570, BCE loss: 0.570, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.564, BCE loss: 0.564, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.123                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.531, BCE loss: 0.531, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.540, BCE loss: 0.540, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.631, BCE loss: 0.631, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.484, BCE loss: 0.484, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.648, BCE loss: 0.648, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.669, BCE loss: 0.669, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.599, BCE loss: 0.599, Diversity Loss: 0.105                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.578, BCE loss: 0.578, Diversity Loss: 0.096                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.488, BCE loss: 0.488, Diversity Loss: 0.165                     (Diversity_weight = 0)
{'accuracy': 0.6970912738214644, 'roc_auc': 0.7559429413232653, 'pr_auc': 0.7017635230117982, 'conicity_mean': 0.15690361, 'conicity_std': 0.039981246}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.679    0.713      0.697      0.696         0.696
precision    0.735    0.668      0.697      0.701         0.702
recall       0.630    0.766      0.697      0.698         0.697
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7559429413232653
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:17:19_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.626, BCE loss: 0.626, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.377, BCE loss: 0.377, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.482, BCE loss: 0.482, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.401, BCE loss: 0.401, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.437, BCE loss: 0.437, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.101                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.585, BCE loss: 0.585, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.443, BCE loss: 0.443, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.444, BCE loss: 0.444, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.536, BCE loss: 0.536, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.441, BCE loss: 0.441, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.269                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.417, BCE loss: 0.417, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.516, BCE loss: 0.516, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.547, BCE loss: 0.547, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.549, BCE loss: 0.549, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.439, BCE loss: 0.439, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.411, BCE loss: 0.411, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.487, BCE loss: 0.487, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.314, BCE loss: 0.314, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.475, BCE loss: 0.475, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.401, BCE loss: 0.401, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.570, BCE loss: 0.570, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.530, BCE loss: 0.530, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.453, BCE loss: 0.453, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.489, BCE loss: 0.489, Diversity Loss: 0.110                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.374, BCE loss: 0.374, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.351, BCE loss: 0.351, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.356, BCE loss: 0.356, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.117                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.415, BCE loss: 0.415, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.381, BCE loss: 0.381, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.571, BCE loss: 0.571, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.284, BCE loss: 0.284, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.457, BCE loss: 0.457, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.565, BCE loss: 0.565, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.413, BCE loss: 0.413, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.351, BCE loss: 0.351, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.603, BCE loss: 0.603, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.483, BCE loss: 0.483, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.616, BCE loss: 0.616, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.411, BCE loss: 0.411, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.592, BCE loss: 0.592, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.450, BCE loss: 0.450, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.609, BCE loss: 0.609, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.629, BCE loss: 0.629, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.540, BCE loss: 0.540, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.097                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.504, BCE loss: 0.504, Diversity Loss: 0.168                     (Diversity_weight = 0)
{'accuracy': 0.7051153460381143, 'roc_auc': 0.7813971647762492, 'pr_auc': 0.729602708238311, 'conicity_mean': 0.16486274, 'conicity_std': 0.041698713}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.665    0.737      0.705      0.701         0.700
precision    0.785    0.658      0.705      0.721         0.722
recall       0.577    0.837      0.705      0.707         0.705
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7813971647762492
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:17:19_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.453, BCE loss: 0.453, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.299, BCE loss: 0.299, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.529, BCE loss: 0.529, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.328, BCE loss: 0.328, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.416, BCE loss: 0.416, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.447, BCE loss: 0.447, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.461, BCE loss: 0.461, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.412, BCE loss: 0.412, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.439, BCE loss: 0.439, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.429, BCE loss: 0.429, Diversity Loss: 0.117                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.407, BCE loss: 0.407, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.251, BCE loss: 0.251, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.316, BCE loss: 0.316, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.279                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.462, BCE loss: 0.462, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.325, BCE loss: 0.325, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.371, BCE loss: 0.371, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.381, BCE loss: 0.381, Diversity Loss: 0.104                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.432, BCE loss: 0.432, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.299, BCE loss: 0.299, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.298, BCE loss: 0.298, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.442, BCE loss: 0.442, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.289, BCE loss: 0.289, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.390, BCE loss: 0.390, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.332, BCE loss: 0.332, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.330, BCE loss: 0.330, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.224, BCE loss: 0.224, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.471, BCE loss: 0.471, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.393, BCE loss: 0.393, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.471, BCE loss: 0.471, Diversity Loss: 0.118                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.241, BCE loss: 0.241, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.295, BCE loss: 0.295, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.266, BCE loss: 0.266, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.379, BCE loss: 0.379, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.354, BCE loss: 0.354, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.341, BCE loss: 0.341, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.264, BCE loss: 0.264, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.310, BCE loss: 0.310, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.332, BCE loss: 0.332, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.228, BCE loss: 0.228, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.399, BCE loss: 0.399, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.420, BCE loss: 0.420, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.253, BCE loss: 0.253, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.280, BCE loss: 0.280, Diversity Loss: 0.162                     (Diversity_weight = 0)
{'accuracy': 0.7131394182547643, 'roc_auc': 0.7892862030380847, 'pr_auc': 0.7602335360820349, 'conicity_mean': 0.16524467, 'conicity_std': 0.039933667}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.726    0.700      0.713      0.713         0.713
precision    0.705    0.722      0.713      0.714         0.714
recall       0.747    0.678      0.713      0.713         0.713
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7892862030380847
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:17:19_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.215, BCE loss: 0.215, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.153, BCE loss: 0.153, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.188, BCE loss: 0.188, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.550, BCE loss: 0.550, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.275, BCE loss: 0.275, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.233, BCE loss: 0.233, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.420, BCE loss: 0.420, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.173, BCE loss: 0.173, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.364, BCE loss: 0.364, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.227, BCE loss: 0.227, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.314, BCE loss: 0.314, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.308, BCE loss: 0.308, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.180, BCE loss: 0.180, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.198, BCE loss: 0.198, Diversity Loss: 0.275                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.440, BCE loss: 0.440, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.406, BCE loss: 0.406, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.559, BCE loss: 0.559, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.110, BCE loss: 0.110, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.136, BCE loss: 0.136, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.150, BCE loss: 0.150, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.279, BCE loss: 0.279, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.412, BCE loss: 0.412, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.286, BCE loss: 0.286, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.218, BCE loss: 0.218, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.232, BCE loss: 0.232, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.250, BCE loss: 0.250, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.248, BCE loss: 0.248, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.188, BCE loss: 0.188, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.279, BCE loss: 0.279, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.120                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.337, BCE loss: 0.337, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.172, BCE loss: 0.172, Diversity Loss: 0.105                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.229, BCE loss: 0.229, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.409, BCE loss: 0.409, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.232, BCE loss: 0.232, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.211, BCE loss: 0.211, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.233, BCE loss: 0.233, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.138                     (Diversity_weight = 0)
{'accuracy': 0.7151454363089268, 'roc_auc': 0.779980357904736, 'pr_auc': 0.742808116132329, 'conicity_mean': 0.168883, 'conicity_std': 0.0410551}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.719    0.711      0.715      0.715         0.715
precision    0.719    0.711      0.715      0.715         0.715
recall       0.719    0.711      0.715      0.715         0.715
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.779980357904736
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:17:19_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.273                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.176, BCE loss: 0.176, Diversity Loss: 0.234                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.136, BCE loss: 0.136, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.182, BCE loss: 0.182, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.050, BCE loss: 0.050, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.104, BCE loss: 0.104, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.114, BCE loss: 0.114, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.216, BCE loss: 0.216, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.155, BCE loss: 0.155, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.153, BCE loss: 0.153, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.123                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.106                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.167, BCE loss: 0.167, Diversity Loss: 0.100                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.291, BCE loss: 0.291, Diversity Loss: 0.154                     (Diversity_weight = 0)
{'accuracy': 0.7011033099297894, 'roc_auc': 0.7775733962309718, 'pr_auc': 0.7504423417171103, 'conicity_mean': 0.16804385, 'conicity_std': 0.040318873}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.688    0.713      0.701      0.701         0.700
precision    0.731    0.676      0.701      0.704         0.704
recall       0.650    0.754      0.701      0.702         0.701
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7775733962309718
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:17:19_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.229                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.102, BCE loss: 0.102, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.125, BCE loss: 0.125, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.277                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.108                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.099                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.233                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.098, BCE loss: 0.098, Diversity Loss: 0.119                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.104, BCE loss: 0.104, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.191, BCE loss: 0.191, Diversity Loss: 0.143                     (Diversity_weight = 0)
{'accuracy': 0.7131394182547643, 'roc_auc': 0.7677080733841559, 'pr_auc': 0.745443236202879, 'conicity_mean': 0.167676, 'conicity_std': 0.040995393}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.731    0.693      0.713      0.712         0.712
precision    0.698    0.732      0.713      0.715         0.715
recall       0.767    0.658      0.713      0.712         0.713
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7677080733841559
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:17:19_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.119, BCE loss: 0.119, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.008, BCE loss: 0.008, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.116                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.121                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.014, BCE loss: 0.014, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.117, BCE loss: 0.117, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.119, BCE loss: 0.119, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.008, BCE loss: 0.008, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.273                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.177, BCE loss: 0.177, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.206, BCE loss: 0.206, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.215, BCE loss: 0.215, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.107                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.217                     (Diversity_weight = 0)
{'accuracy': 0.7011033099297894, 'roc_auc': 0.7630611883467635, 'pr_auc': 0.7263891962061104, 'conicity_mean': 0.16787876, 'conicity_std': 0.039741553}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.708    0.693      0.701      0.701         0.701
precision    0.702    0.701      0.701      0.701         0.701
recall       0.715    0.686      0.701      0.701         0.701
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7630611883467635
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
./experiments/cls_de/cls_de/ortho_lstm+tanh/Sun_Jan_17_19:17:19_2021
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:85: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:86: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:18:56,677 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:18:56,678 - type = ortholstm
INFO - 2021-01-17 19:18:56,678 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:18:56,678 - vocab_size = 648
INFO - 2021-01-17 19:18:56,678 - embed_size = 200
INFO - 2021-01-17 19:18:56,678 - hidden_size = 128
INFO - 2021-01-17 19:18:56,678 - pre_embed = None
INFO - 2021-01-17 19:18:56,693 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:18:56,693 - hidden_size = 256
INFO - 2021-01-17 19:18:56,693 - output_size = 1
INFO - 2021-01-17 19:18:56,693 - use_attention = True
INFO - 2021-01-17 19:18:56,693 - regularizer_attention = None
INFO - 2021-01-17 19:18:56,693 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b69c3e56490> and extras set()
INFO - 2021-01-17 19:18:56,693 - attention.type = tanh
INFO - 2021-01-17 19:18:56,693 - type = tanh
INFO - 2021-01-17 19:18:56,693 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b69c3e56490> and extras set()
INFO - 2021-01-17 19:18:56,693 - attention.hidden_size = 256
INFO - 2021-01-17 19:18:56,694 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7287287287287287, 'roc_auc': 0.79244239187954, 'pr_auc': 0.7779902104336698, 'conicity_mean': '0.1652322', 'conicity_std': '0.038002856'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.736    0.721      0.729      0.729         0.728
precision    0.707    0.753      0.729      0.730         0.731
recall       0.766    0.692      0.729      0.729         0.729
support    492.000  507.000    999.000    999.000       999.000
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:18:58,916 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:18:58,916 - type = ortholstm
INFO - 2021-01-17 19:18:58,917 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:18:58,917 - vocab_size = 648
INFO - 2021-01-17 19:18:58,917 - embed_size = 200
INFO - 2021-01-17 19:18:58,917 - hidden_size = 128
INFO - 2021-01-17 19:18:58,917 - pre_embed = None
INFO - 2021-01-17 19:18:58,931 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:18:58,931 - hidden_size = 256
INFO - 2021-01-17 19:18:58,931 - output_size = 1
INFO - 2021-01-17 19:18:58,931 - use_attention = True
INFO - 2021-01-17 19:18:58,931 - regularizer_attention = None
INFO - 2021-01-17 19:18:58,931 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b69c39dcad0> and extras set()
INFO - 2021-01-17 19:18:58,931 - attention.type = tanh
INFO - 2021-01-17 19:18:58,931 - type = tanh
INFO - 2021-01-17 19:18:58,932 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b69c39dcad0> and extras set()
INFO - 2021-01-17 19:18:58,932 - attention.hidden_size = 256
INFO - 2021-01-17 19:18:58,932 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7287287287287287, 'roc_auc': 0.79244239187954, 'pr_auc': 0.7779902104336698, 'conicity_mean': '0.1652322', 'conicity_std': '0.038002856'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.736    0.721      0.729      0.729         0.728
precision    0.707    0.753      0.729      0.730         0.731
recall       0.766    0.692      0.729      0.729         0.729
support    492.000  507.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_de diversity_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:19:12,225 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:19:12,225 - type = vanillalstm
INFO - 2021-01-17 19:19:12,225 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:19:12,240 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:19:12,563 - vocab_size = 648
INFO - 2021-01-17 19:19:12,563 - embed_size = 200
INFO - 2021-01-17 19:19:12,563 - hidden_size = 128
INFO - 2021-01-17 19:19:12,563 - pre_embed = None
INFO - 2021-01-17 19:19:15,670 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:19:15,671 - hidden_size = 256
INFO - 2021-01-17 19:19:15,671 - output_size = 1
INFO - 2021-01-17 19:19:15,671 - use_attention = True
INFO - 2021-01-17 19:19:15,671 - regularizer_attention = None
INFO - 2021-01-17 19:19:15,672 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b534a6cc6d0> and extras set()
INFO - 2021-01-17 19:19:15,672 - attention.type = tanh
INFO - 2021-01-17 19:19:15,672 - type = tanh
INFO - 2021-01-17 19:19:15,672 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b534a6cc6d0> and extras set()
INFO - 2021-01-17 19:19:15,672 - attention.hidden_size = 256
INFO - 2021-01-17 19:19:15,672 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.974, BCE loss: 0.700, Diversity Loss: 0.548                     (Diversity_weight = 0.5)
Epoch: 0 Step: 1 Total Loss: 0.929, BCE loss: 0.697, Diversity Loss: 0.463                     (Diversity_weight = 0.5)
Epoch: 0 Step: 2 Total Loss: 0.931, BCE loss: 0.657, Diversity Loss: 0.548                     (Diversity_weight = 0.5)
Epoch: 0 Step: 3 Total Loss: 0.865, BCE loss: 0.677, Diversity Loss: 0.377                     (Diversity_weight = 0.5)
Epoch: 0 Step: 4 Total Loss: 0.908, BCE loss: 0.705, Diversity Loss: 0.404                     (Diversity_weight = 0.5)
Epoch: 0 Step: 5 Total Loss: 0.888, BCE loss: 0.683, Diversity Loss: 0.410                     (Diversity_weight = 0.5)
Epoch: 0 Step: 6 Total Loss: 0.836, BCE loss: 0.668, Diversity Loss: 0.337                     (Diversity_weight = 0.5)
Epoch: 0 Step: 7 Total Loss: 0.834, BCE loss: 0.692, Diversity Loss: 0.284                     (Diversity_weight = 0.5)
Epoch: 0 Step: 8 Total Loss: 0.806, BCE loss: 0.688, Diversity Loss: 0.236                     (Diversity_weight = 0.5)
Epoch: 0 Step: 9 Total Loss: 0.804, BCE loss: 0.691, Diversity Loss: 0.226                     (Diversity_weight = 0.5)
Epoch: 0 Step: 10 Total Loss: 0.803, BCE loss: 0.693, Diversity Loss: 0.221                     (Diversity_weight = 0.5)
Epoch: 0 Step: 11 Total Loss: 0.824, BCE loss: 0.692, Diversity Loss: 0.264                     (Diversity_weight = 0.5)
Epoch: 0 Step: 12 Total Loss: 0.822, BCE loss: 0.686, Diversity Loss: 0.272                     (Diversity_weight = 0.5)
Epoch: 0 Step: 13 Total Loss: 0.887, BCE loss: 0.699, Diversity Loss: 0.378                     (Diversity_weight = 0.5)
Epoch: 0 Step: 14 Total Loss: 0.812, BCE loss: 0.692, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 0 Step: 15 Total Loss: 0.843, BCE loss: 0.694, Diversity Loss: 0.299                     (Diversity_weight = 0.5)
Epoch: 0 Step: 16 Total Loss: 0.810, BCE loss: 0.690, Diversity Loss: 0.240                     (Diversity_weight = 0.5)
Epoch: 0 Step: 17 Total Loss: 0.790, BCE loss: 0.690, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 0 Step: 18 Total Loss: 0.786, BCE loss: 0.691, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 0 Step: 19 Total Loss: 0.780, BCE loss: 0.688, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 0 Step: 20 Total Loss: 0.780, BCE loss: 0.695, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 0 Step: 21 Total Loss: 0.769, BCE loss: 0.690, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 0 Step: 22 Total Loss: 0.766, BCE loss: 0.692, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 0 Step: 23 Total Loss: 0.811, BCE loss: 0.690, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 0 Step: 24 Total Loss: 0.774, BCE loss: 0.694, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 0 Step: 25 Total Loss: 0.824, BCE loss: 0.686, Diversity Loss: 0.275                     (Diversity_weight = 0.5)
Epoch: 0 Step: 26 Total Loss: 0.789, BCE loss: 0.687, Diversity Loss: 0.204                     (Diversity_weight = 0.5)
Epoch: 0 Step: 27 Total Loss: 0.786, BCE loss: 0.694, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 0 Step: 28 Total Loss: 0.793, BCE loss: 0.681, Diversity Loss: 0.225                     (Diversity_weight = 0.5)
Epoch: 0 Step: 29 Total Loss: 0.791, BCE loss: 0.688, Diversity Loss: 0.205                     (Diversity_weight = 0.5)
Epoch: 0 Step: 30 Total Loss: 0.803, BCE loss: 0.682, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 0 Step: 31 Total Loss: 0.773, BCE loss: 0.685, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 0 Step: 32 Total Loss: 0.889, BCE loss: 0.686, Diversity Loss: 0.405                     (Diversity_weight = 0.5)
Epoch: 0 Step: 33 Total Loss: 0.770, BCE loss: 0.686, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 0 Step: 34 Total Loss: 0.780, BCE loss: 0.685, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 0 Step: 35 Total Loss: 0.760, BCE loss: 0.683, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 0 Step: 36 Total Loss: 0.791, BCE loss: 0.669, Diversity Loss: 0.244                     (Diversity_weight = 0.5)
Epoch: 0 Step: 37 Total Loss: 0.829, BCE loss: 0.685, Diversity Loss: 0.288                     (Diversity_weight = 0.5)
Epoch: 0 Step: 38 Total Loss: 0.814, BCE loss: 0.685, Diversity Loss: 0.257                     (Diversity_weight = 0.5)
Epoch: 0 Step: 39 Total Loss: 0.796, BCE loss: 0.711, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 0 Step: 40 Total Loss: 0.803, BCE loss: 0.669, Diversity Loss: 0.270                     (Diversity_weight = 0.5)
Epoch: 0 Step: 41 Total Loss: 0.788, BCE loss: 0.685, Diversity Loss: 0.206                     (Diversity_weight = 0.5)
Epoch: 0 Step: 42 Total Loss: 0.754, BCE loss: 0.676, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 0 Step: 43 Total Loss: 0.769, BCE loss: 0.688, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 0 Step: 44 Total Loss: 0.772, BCE loss: 0.690, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 0 Step: 45 Total Loss: 0.763, BCE loss: 0.685, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 0 Step: 46 Total Loss: 0.866, BCE loss: 0.693, Diversity Loss: 0.345                     (Diversity_weight = 0.5)
Epoch: 0 Step: 47 Total Loss: 0.742, BCE loss: 0.672, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 0 Step: 48 Total Loss: 0.747, BCE loss: 0.682, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 0 Step: 49 Total Loss: 0.746, BCE loss: 0.651, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 0 Step: 50 Total Loss: 0.791, BCE loss: 0.681, Diversity Loss: 0.220                     (Diversity_weight = 0.5)
Epoch: 0 Step: 51 Total Loss: 0.771, BCE loss: 0.691, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 0 Step: 52 Total Loss: 0.734, BCE loss: 0.671, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 0 Step: 53 Total Loss: 0.739, BCE loss: 0.683, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 0 Step: 54 Total Loss: 0.753, BCE loss: 0.667, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 0 Step: 55 Total Loss: 0.735, BCE loss: 0.669, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 0 Step: 56 Total Loss: 0.703, BCE loss: 0.628, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 0 Step: 57 Total Loss: 0.845, BCE loss: 0.759, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 0 Step: 58 Total Loss: 0.740, BCE loss: 0.668, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 0 Step: 59 Total Loss: 0.704, BCE loss: 0.617, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 0 Step: 60 Total Loss: 0.732, BCE loss: 0.675, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 0 Step: 61 Total Loss: 0.759, BCE loss: 0.682, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 0 Step: 62 Total Loss: 0.784, BCE loss: 0.687, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
{'accuracy': 0.5336008024072216, 'roc_auc': 0.6642409215684696, 'pr_auc': 0.6334912254285062, 'conicity_mean': 0.16008164, 'conicity_std': 0.056134988}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.678    0.156      0.534      0.417         0.421
precision    0.522    0.717      0.534      0.619         0.618
recall       0.966    0.088      0.534      0.527         0.534
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.6642409215684696
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:19:15_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.728, BCE loss: 0.671, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 1 Step: 1 Total Loss: 0.753, BCE loss: 0.698, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 1 Step: 2 Total Loss: 0.740, BCE loss: 0.677, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 1 Step: 3 Total Loss: 0.796, BCE loss: 0.697, Diversity Loss: 0.198                     (Diversity_weight = 0.5)
Epoch: 1 Step: 4 Total Loss: 0.724, BCE loss: 0.661, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 1 Step: 5 Total Loss: 0.744, BCE loss: 0.668, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 1 Step: 6 Total Loss: 0.762, BCE loss: 0.677, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 1 Step: 7 Total Loss: 0.712, BCE loss: 0.626, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 1 Step: 8 Total Loss: 0.736, BCE loss: 0.670, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 1 Step: 9 Total Loss: 0.787, BCE loss: 0.704, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 1 Step: 10 Total Loss: 0.756, BCE loss: 0.691, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 1 Step: 11 Total Loss: 0.752, BCE loss: 0.656, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 1 Step: 12 Total Loss: 0.725, BCE loss: 0.669, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 1 Step: 13 Total Loss: 0.736, BCE loss: 0.668, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 1 Step: 14 Total Loss: 0.732, BCE loss: 0.628, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 1 Step: 15 Total Loss: 0.735, BCE loss: 0.680, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 1 Step: 16 Total Loss: 0.761, BCE loss: 0.696, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 1 Step: 17 Total Loss: 0.714, BCE loss: 0.656, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 1 Step: 18 Total Loss: 0.730, BCE loss: 0.665, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 1 Step: 19 Total Loss: 0.721, BCE loss: 0.660, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 20 Total Loss: 0.696, BCE loss: 0.614, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 1 Step: 21 Total Loss: 0.767, BCE loss: 0.695, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 1 Step: 22 Total Loss: 0.738, BCE loss: 0.682, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 1 Step: 23 Total Loss: 0.807, BCE loss: 0.702, Diversity Loss: 0.211                     (Diversity_weight = 0.5)
Epoch: 1 Step: 24 Total Loss: 0.661, BCE loss: 0.592, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 1 Step: 25 Total Loss: 0.703, BCE loss: 0.632, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 1 Step: 26 Total Loss: 0.731, BCE loss: 0.609, Diversity Loss: 0.245                     (Diversity_weight = 0.5)
Epoch: 1 Step: 27 Total Loss: 0.768, BCE loss: 0.665, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 1 Step: 28 Total Loss: 0.683, BCE loss: 0.563, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 1 Step: 29 Total Loss: 0.695, BCE loss: 0.633, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 1 Step: 30 Total Loss: 0.704, BCE loss: 0.627, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 1 Step: 31 Total Loss: 0.666, BCE loss: 0.573, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 1 Step: 32 Total Loss: 0.662, BCE loss: 0.583, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 1 Step: 33 Total Loss: 0.689, BCE loss: 0.629, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 1 Step: 34 Total Loss: 0.706, BCE loss: 0.629, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 1 Step: 35 Total Loss: 0.731, BCE loss: 0.673, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 1 Step: 36 Total Loss: 0.823, BCE loss: 0.773, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 1 Step: 37 Total Loss: 0.762, BCE loss: 0.691, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 1 Step: 38 Total Loss: 0.687, BCE loss: 0.594, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 1 Step: 39 Total Loss: 0.702, BCE loss: 0.639, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 1 Step: 40 Total Loss: 0.612, BCE loss: 0.509, Diversity Loss: 0.206                     (Diversity_weight = 0.5)
Epoch: 1 Step: 41 Total Loss: 0.727, BCE loss: 0.652, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 1 Step: 42 Total Loss: 0.708, BCE loss: 0.649, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 1 Step: 43 Total Loss: 0.666, BCE loss: 0.613, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 1 Step: 44 Total Loss: 0.705, BCE loss: 0.641, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 1 Step: 45 Total Loss: 0.713, BCE loss: 0.648, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 1 Step: 46 Total Loss: 0.656, BCE loss: 0.568, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 1 Step: 47 Total Loss: 0.739, BCE loss: 0.616, Diversity Loss: 0.245                     (Diversity_weight = 0.5)
Epoch: 1 Step: 48 Total Loss: 0.677, BCE loss: 0.616, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 1 Step: 49 Total Loss: 0.641, BCE loss: 0.564, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 1 Step: 50 Total Loss: 0.727, BCE loss: 0.675, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 1 Step: 51 Total Loss: 0.734, BCE loss: 0.666, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 1 Step: 52 Total Loss: 0.685, BCE loss: 0.614, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 1 Step: 53 Total Loss: 0.789, BCE loss: 0.719, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 1 Step: 54 Total Loss: 0.652, BCE loss: 0.596, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 1 Step: 55 Total Loss: 0.566, BCE loss: 0.454, Diversity Loss: 0.224                     (Diversity_weight = 0.5)
Epoch: 1 Step: 56 Total Loss: 0.683, BCE loss: 0.629, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 1 Step: 57 Total Loss: 0.671, BCE loss: 0.587, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 1 Step: 58 Total Loss: 0.693, BCE loss: 0.648, Diversity Loss: 0.090                     (Diversity_weight = 0.5)
Epoch: 1 Step: 59 Total Loss: 0.710, BCE loss: 0.631, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 1 Step: 60 Total Loss: 0.692, BCE loss: 0.528, Diversity Loss: 0.327                     (Diversity_weight = 0.5)
Epoch: 1 Step: 61 Total Loss: 0.688, BCE loss: 0.615, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 1 Step: 62 Total Loss: 0.952, BCE loss: 0.888, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
{'accuracy': 0.6439317953861585, 'roc_auc': 0.7029978345394975, 'pr_auc': 0.6699208044290679, 'conicity_mean': 0.15020949, 'conicity_std': 0.05023406}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.597    0.681      0.644      0.639         0.638
precision    0.701    0.609      0.644      0.655         0.656
recall       0.520    0.772      0.644      0.646         0.644
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7029978345394975
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:19:15_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.622, BCE loss: 0.559, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 2 Step: 1 Total Loss: 0.596, BCE loss: 0.541, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 2 Step: 2 Total Loss: 0.585, BCE loss: 0.518, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 2 Step: 3 Total Loss: 0.677, BCE loss: 0.612, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 2 Step: 4 Total Loss: 0.722, BCE loss: 0.665, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 2 Step: 5 Total Loss: 0.700, BCE loss: 0.604, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 2 Step: 6 Total Loss: 0.795, BCE loss: 0.739, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 2 Step: 7 Total Loss: 0.714, BCE loss: 0.635, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 2 Step: 8 Total Loss: 0.532, BCE loss: 0.415, Diversity Loss: 0.234                     (Diversity_weight = 0.5)
Epoch: 2 Step: 9 Total Loss: 0.679, BCE loss: 0.635, Diversity Loss: 0.090                     (Diversity_weight = 0.5)
Epoch: 2 Step: 10 Total Loss: 0.615, BCE loss: 0.462, Diversity Loss: 0.306                     (Diversity_weight = 0.5)
Epoch: 2 Step: 11 Total Loss: 0.713, BCE loss: 0.667, Diversity Loss: 0.091                     (Diversity_weight = 0.5)
Epoch: 2 Step: 12 Total Loss: 0.786, BCE loss: 0.727, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 2 Step: 13 Total Loss: 0.660, BCE loss: 0.570, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 2 Step: 14 Total Loss: 0.691, BCE loss: 0.631, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 2 Step: 15 Total Loss: 0.604, BCE loss: 0.539, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 2 Step: 16 Total Loss: 0.656, BCE loss: 0.602, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 2 Step: 17 Total Loss: 0.663, BCE loss: 0.588, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 2 Step: 18 Total Loss: 0.607, BCE loss: 0.557, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 2 Step: 19 Total Loss: 0.622, BCE loss: 0.552, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 2 Step: 20 Total Loss: 0.738, BCE loss: 0.676, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 2 Step: 21 Total Loss: 0.578, BCE loss: 0.502, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 2 Step: 22 Total Loss: 0.657, BCE loss: 0.606, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 2 Step: 23 Total Loss: 0.654, BCE loss: 0.582, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 2 Step: 24 Total Loss: 0.725, BCE loss: 0.664, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 2 Step: 25 Total Loss: 0.640, BCE loss: 0.576, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 2 Step: 26 Total Loss: 0.642, BCE loss: 0.573, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 2 Step: 27 Total Loss: 0.630, BCE loss: 0.572, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 2 Step: 28 Total Loss: 0.534, BCE loss: 0.464, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 2 Step: 29 Total Loss: 0.591, BCE loss: 0.527, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 2 Step: 30 Total Loss: 0.737, BCE loss: 0.686, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 2 Step: 31 Total Loss: 0.601, BCE loss: 0.480, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 2 Step: 32 Total Loss: 0.619, BCE loss: 0.529, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 2 Step: 33 Total Loss: 0.733, BCE loss: 0.644, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 2 Step: 34 Total Loss: 0.533, BCE loss: 0.475, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 2 Step: 35 Total Loss: 0.558, BCE loss: 0.492, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 2 Step: 36 Total Loss: 0.698, BCE loss: 0.626, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 2 Step: 37 Total Loss: 0.531, BCE loss: 0.474, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 2 Step: 38 Total Loss: 0.604, BCE loss: 0.521, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 2 Step: 39 Total Loss: 0.611, BCE loss: 0.509, Diversity Loss: 0.203                     (Diversity_weight = 0.5)
Epoch: 2 Step: 40 Total Loss: 0.654, BCE loss: 0.602, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 2 Step: 41 Total Loss: 0.619, BCE loss: 0.556, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 2 Step: 42 Total Loss: 0.580, BCE loss: 0.499, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 2 Step: 43 Total Loss: 0.494, BCE loss: 0.410, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 2 Step: 44 Total Loss: 0.651, BCE loss: 0.579, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 2 Step: 45 Total Loss: 0.604, BCE loss: 0.529, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 2 Step: 46 Total Loss: 0.563, BCE loss: 0.505, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 2 Step: 47 Total Loss: 0.516, BCE loss: 0.426, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 2 Step: 48 Total Loss: 0.497, BCE loss: 0.417, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 2 Step: 49 Total Loss: 0.664, BCE loss: 0.557, Diversity Loss: 0.214                     (Diversity_weight = 0.5)
Epoch: 2 Step: 50 Total Loss: 0.550, BCE loss: 0.491, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 2 Step: 51 Total Loss: 0.582, BCE loss: 0.506, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 2 Step: 52 Total Loss: 0.597, BCE loss: 0.492, Diversity Loss: 0.210                     (Diversity_weight = 0.5)
Epoch: 2 Step: 53 Total Loss: 0.562, BCE loss: 0.512, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 2 Step: 54 Total Loss: 0.653, BCE loss: 0.581, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 2 Step: 55 Total Loss: 0.577, BCE loss: 0.476, Diversity Loss: 0.202                     (Diversity_weight = 0.5)
Epoch: 2 Step: 56 Total Loss: 0.603, BCE loss: 0.533, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 2 Step: 57 Total Loss: 0.587, BCE loss: 0.498, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 2 Step: 58 Total Loss: 0.572, BCE loss: 0.510, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 2 Step: 59 Total Loss: 0.808, BCE loss: 0.765, Diversity Loss: 0.086                     (Diversity_weight = 0.5)
Epoch: 2 Step: 60 Total Loss: 0.719, BCE loss: 0.653, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 2 Step: 61 Total Loss: 0.680, BCE loss: 0.630, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 2 Step: 62 Total Loss: 0.673, BCE loss: 0.620, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
{'accuracy': 0.695085255767302, 'roc_auc': 0.7624956731040146, 'pr_auc': 0.7554904857985854, 'conicity_mean': 0.14168829, 'conicity_std': 0.048185624}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.709    0.680      0.695      0.694         0.695
precision    0.688    0.704      0.695      0.696         0.696
recall       0.731    0.658      0.695      0.695         0.695
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7624956731040146
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:19:15_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.515, BCE loss: 0.451, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 3 Step: 1 Total Loss: 0.586, BCE loss: 0.489, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 3 Step: 2 Total Loss: 0.488, BCE loss: 0.412, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 3 Step: 3 Total Loss: 0.550, BCE loss: 0.502, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 3 Step: 4 Total Loss: 0.593, BCE loss: 0.546, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 3 Step: 5 Total Loss: 0.447, BCE loss: 0.364, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 3 Step: 6 Total Loss: 0.418, BCE loss: 0.327, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 3 Step: 7 Total Loss: 0.557, BCE loss: 0.482, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 8 Total Loss: 0.686, BCE loss: 0.630, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 3 Step: 9 Total Loss: 0.603, BCE loss: 0.558, Diversity Loss: 0.091                     (Diversity_weight = 0.5)
Epoch: 3 Step: 10 Total Loss: 0.442, BCE loss: 0.383, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 3 Step: 11 Total Loss: 0.550, BCE loss: 0.501, Diversity Loss: 0.098                     (Diversity_weight = 0.5)
Epoch: 3 Step: 12 Total Loss: 0.515, BCE loss: 0.444, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 3 Step: 13 Total Loss: 0.385, BCE loss: 0.320, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 3 Step: 14 Total Loss: 0.502, BCE loss: 0.452, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 3 Step: 15 Total Loss: 0.396, BCE loss: 0.297, Diversity Loss: 0.198                     (Diversity_weight = 0.5)
Epoch: 3 Step: 16 Total Loss: 0.591, BCE loss: 0.494, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 3 Step: 17 Total Loss: 0.536, BCE loss: 0.481, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 3 Step: 18 Total Loss: 0.431, BCE loss: 0.370, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 3 Step: 19 Total Loss: 0.492, BCE loss: 0.440, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 3 Step: 20 Total Loss: 0.505, BCE loss: 0.444, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 3 Step: 21 Total Loss: 0.686, BCE loss: 0.624, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 3 Step: 22 Total Loss: 0.480, BCE loss: 0.434, Diversity Loss: 0.092                     (Diversity_weight = 0.5)
Epoch: 3 Step: 23 Total Loss: 0.419, BCE loss: 0.325, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 3 Step: 24 Total Loss: 0.352, BCE loss: 0.296, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 3 Step: 25 Total Loss: 0.557, BCE loss: 0.501, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 3 Step: 26 Total Loss: 0.538, BCE loss: 0.466, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 3 Step: 27 Total Loss: 0.452, BCE loss: 0.371, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 3 Step: 28 Total Loss: 0.615, BCE loss: 0.498, Diversity Loss: 0.235                     (Diversity_weight = 0.5)
Epoch: 3 Step: 29 Total Loss: 0.654, BCE loss: 0.558, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 3 Step: 30 Total Loss: 0.557, BCE loss: 0.492, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 3 Step: 31 Total Loss: 0.574, BCE loss: 0.515, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 3 Step: 32 Total Loss: 0.951, BCE loss: 0.908, Diversity Loss: 0.086                     (Diversity_weight = 0.5)
Epoch: 3 Step: 33 Total Loss: 0.462, BCE loss: 0.387, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 34 Total Loss: 0.318, BCE loss: 0.206, Diversity Loss: 0.225                     (Diversity_weight = 0.5)
Epoch: 3 Step: 35 Total Loss: 0.702, BCE loss: 0.647, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 3 Step: 36 Total Loss: 0.560, BCE loss: 0.496, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 3 Step: 37 Total Loss: 0.604, BCE loss: 0.521, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 3 Step: 38 Total Loss: 0.608, BCE loss: 0.458, Diversity Loss: 0.300                     (Diversity_weight = 0.5)
Epoch: 3 Step: 39 Total Loss: 0.517, BCE loss: 0.447, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 3 Step: 40 Total Loss: 0.480, BCE loss: 0.424, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 3 Step: 41 Total Loss: 0.469, BCE loss: 0.386, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 3 Step: 42 Total Loss: 0.645, BCE loss: 0.603, Diversity Loss: 0.084                     (Diversity_weight = 0.5)
Epoch: 3 Step: 43 Total Loss: 0.818, BCE loss: 0.759, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 3 Step: 44 Total Loss: 0.641, BCE loss: 0.572, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 3 Step: 45 Total Loss: 0.469, BCE loss: 0.392, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 3 Step: 46 Total Loss: 0.669, BCE loss: 0.618, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 3 Step: 47 Total Loss: 0.706, BCE loss: 0.640, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 3 Step: 48 Total Loss: 0.501, BCE loss: 0.431, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 3 Step: 49 Total Loss: 0.592, BCE loss: 0.542, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 3 Step: 50 Total Loss: 0.495, BCE loss: 0.427, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 3 Step: 51 Total Loss: 0.596, BCE loss: 0.511, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 3 Step: 52 Total Loss: 0.451, BCE loss: 0.383, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 3 Step: 53 Total Loss: 0.560, BCE loss: 0.502, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 3 Step: 54 Total Loss: 0.533, BCE loss: 0.482, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 3 Step: 55 Total Loss: 0.661, BCE loss: 0.605, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 3 Step: 56 Total Loss: 0.483, BCE loss: 0.421, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 3 Step: 57 Total Loss: 0.482, BCE loss: 0.417, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 3 Step: 58 Total Loss: 0.502, BCE loss: 0.427, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 59 Total Loss: 0.548, BCE loss: 0.485, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 3 Step: 60 Total Loss: 0.584, BCE loss: 0.480, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 3 Step: 61 Total Loss: 0.549, BCE loss: 0.483, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 3 Step: 62 Total Loss: 0.468, BCE loss: 0.391, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
{'accuracy': 0.7091273821464393, 'roc_auc': 0.7987570739718086, 'pr_auc': 0.7903608527716611, 'conicity_mean': 0.140398, 'conicity_std': 0.04918964}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.695    0.722      0.709      0.708         0.708
precision    0.743    0.682      0.709      0.712         0.713
recall       0.652    0.768      0.709      0.710         0.709
support    506.000  491.000    997.000    997.000       997.000
Model Saved on  roc_auc 0.7987570739718086
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:19:15_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.414, BCE loss: 0.351, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 4 Step: 1 Total Loss: 0.487, BCE loss: 0.409, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 4 Step: 2 Total Loss: 0.323, BCE loss: 0.242, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 4 Step: 3 Total Loss: 0.585, BCE loss: 0.545, Diversity Loss: 0.080                     (Diversity_weight = 0.5)
Epoch: 4 Step: 4 Total Loss: 0.495, BCE loss: 0.434, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 4 Step: 5 Total Loss: 0.435, BCE loss: 0.368, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 4 Step: 6 Total Loss: 0.448, BCE loss: 0.396, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 4 Step: 7 Total Loss: 0.456, BCE loss: 0.404, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 4 Step: 8 Total Loss: 0.498, BCE loss: 0.451, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 4 Step: 9 Total Loss: 0.350, BCE loss: 0.283, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 4 Step: 10 Total Loss: 0.444, BCE loss: 0.364, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 4 Step: 11 Total Loss: 0.437, BCE loss: 0.345, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 4 Step: 12 Total Loss: 0.476, BCE loss: 0.424, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 4 Step: 13 Total Loss: 0.580, BCE loss: 0.538, Diversity Loss: 0.086                     (Diversity_weight = 0.5)
Epoch: 4 Step: 14 Total Loss: 0.577, BCE loss: 0.521, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 4 Step: 15 Total Loss: 0.386, BCE loss: 0.312, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 4 Step: 16 Total Loss: 0.319, BCE loss: 0.243, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 4 Step: 17 Total Loss: 0.541, BCE loss: 0.461, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 4 Step: 18 Total Loss: 0.385, BCE loss: 0.324, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 4 Step: 19 Total Loss: 0.437, BCE loss: 0.350, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 4 Step: 20 Total Loss: 0.289, BCE loss: 0.182, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 4 Step: 21 Total Loss: 0.455, BCE loss: 0.382, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 4 Step: 22 Total Loss: 0.504, BCE loss: 0.358, Diversity Loss: 0.294                     (Diversity_weight = 0.5)
Epoch: 4 Step: 23 Total Loss: 0.298, BCE loss: 0.224, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 4 Step: 24 Total Loss: 0.635, BCE loss: 0.585, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 4 Step: 25 Total Loss: 0.482, BCE loss: 0.435, Diversity Loss: 0.094                     (Diversity_weight = 0.5)
Epoch: 4 Step: 26 Total Loss: 0.537, BCE loss: 0.477, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 4 Step: 27 Total Loss: 0.343, BCE loss: 0.286, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 4 Step: 28 Total Loss: 0.409, BCE loss: 0.351, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 4 Step: 29 Total Loss: 0.419, BCE loss: 0.332, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 4 Step: 30 Total Loss: 0.505, BCE loss: 0.401, Diversity Loss: 0.208                     (Diversity_weight = 0.5)
Epoch: 4 Step: 31 Total Loss: 0.409, BCE loss: 0.332, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 4 Step: 32 Total Loss: 0.561, BCE loss: 0.491, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 4 Step: 33 Total Loss: 0.559, BCE loss: 0.456, Diversity Loss: 0.206                     (Diversity_weight = 0.5)
Epoch: 4 Step: 34 Total Loss: 0.657, BCE loss: 0.592, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 4 Step: 35 Total Loss: 0.469, BCE loss: 0.400, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 4 Step: 36 Total Loss: 0.385, BCE loss: 0.332, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 4 Step: 37 Total Loss: 0.530, BCE loss: 0.412, Diversity Loss: 0.237                     (Diversity_weight = 0.5)
Epoch: 4 Step: 38 Total Loss: 0.379, BCE loss: 0.322, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 4 Step: 39 Total Loss: 0.456, BCE loss: 0.408, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 4 Step: 40 Total Loss: 0.618, BCE loss: 0.554, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 4 Step: 41 Total Loss: 0.407, BCE loss: 0.337, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 4 Step: 42 Total Loss: 0.406, BCE loss: 0.352, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 4 Step: 43 Total Loss: 0.266, BCE loss: 0.173, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 4 Step: 44 Total Loss: 0.459, BCE loss: 0.396, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 4 Step: 45 Total Loss: 0.501, BCE loss: 0.429, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 4 Step: 46 Total Loss: 0.387, BCE loss: 0.334, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 4 Step: 47 Total Loss: 0.366, BCE loss: 0.291, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 4 Step: 48 Total Loss: 0.353, BCE loss: 0.294, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 4 Step: 49 Total Loss: 0.419, BCE loss: 0.363, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 4 Step: 50 Total Loss: 0.622, BCE loss: 0.521, Diversity Loss: 0.202                     (Diversity_weight = 0.5)
Epoch: 4 Step: 51 Total Loss: 0.546, BCE loss: 0.472, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 4 Step: 52 Total Loss: 0.403, BCE loss: 0.344, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 4 Step: 53 Total Loss: 0.314, BCE loss: 0.252, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 4 Step: 54 Total Loss: 0.295, BCE loss: 0.228, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 4 Step: 55 Total Loss: 0.308, BCE loss: 0.229, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 4 Step: 56 Total Loss: 0.373, BCE loss: 0.302, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 4 Step: 57 Total Loss: 0.442, BCE loss: 0.379, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 4 Step: 58 Total Loss: 0.540, BCE loss: 0.491, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 4 Step: 59 Total Loss: 0.576, BCE loss: 0.485, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 4 Step: 60 Total Loss: 0.508, BCE loss: 0.440, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 4 Step: 61 Total Loss: 0.481, BCE loss: 0.426, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 4 Step: 62 Total Loss: 0.400, BCE loss: 0.291, Diversity Loss: 0.219                     (Diversity_weight = 0.5)
{'accuracy': 0.724172517552658, 'roc_auc': 0.788448998977645, 'pr_auc': 0.7740691931412256, 'conicity_mean': 0.14107719, 'conicity_std': 0.049197428}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.714    0.733      0.724      0.724         0.724
precision    0.753    0.700      0.724      0.726         0.727
recall       0.680    0.770      0.724      0.725         0.724
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.788448998977645
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:19:15_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.320, BCE loss: 0.263, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 5 Step: 1 Total Loss: 0.371, BCE loss: 0.287, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 5 Step: 2 Total Loss: 0.294, BCE loss: 0.240, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 5 Step: 3 Total Loss: 0.250, BCE loss: 0.171, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 5 Step: 4 Total Loss: 0.242, BCE loss: 0.165, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 5 Step: 5 Total Loss: 0.381, BCE loss: 0.336, Diversity Loss: 0.089                     (Diversity_weight = 0.5)
Epoch: 5 Step: 6 Total Loss: 0.235, BCE loss: 0.183, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 5 Step: 7 Total Loss: 0.225, BCE loss: 0.169, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 5 Step: 8 Total Loss: 0.229, BCE loss: 0.161, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 5 Step: 9 Total Loss: 0.267, BCE loss: 0.194, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 5 Step: 10 Total Loss: 0.428, BCE loss: 0.361, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 5 Step: 11 Total Loss: 0.429, BCE loss: 0.333, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 5 Step: 12 Total Loss: 0.241, BCE loss: 0.172, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 5 Step: 13 Total Loss: 0.208, BCE loss: 0.114, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 5 Step: 14 Total Loss: 0.385, BCE loss: 0.320, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 5 Step: 15 Total Loss: 0.331, BCE loss: 0.265, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 5 Step: 16 Total Loss: 0.422, BCE loss: 0.353, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 5 Step: 17 Total Loss: 0.372, BCE loss: 0.297, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 5 Step: 18 Total Loss: 0.342, BCE loss: 0.279, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 5 Step: 19 Total Loss: 0.613, BCE loss: 0.552, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 5 Step: 20 Total Loss: 0.324, BCE loss: 0.243, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 5 Step: 21 Total Loss: 0.241, BCE loss: 0.165, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 5 Step: 22 Total Loss: 0.289, BCE loss: 0.238, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 5 Step: 23 Total Loss: 0.423, BCE loss: 0.338, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 5 Step: 24 Total Loss: 0.367, BCE loss: 0.300, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 5 Step: 25 Total Loss: 0.347, BCE loss: 0.252, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 5 Step: 26 Total Loss: 0.416, BCE loss: 0.366, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 5 Step: 27 Total Loss: 0.325, BCE loss: 0.277, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 5 Step: 28 Total Loss: 0.197, BCE loss: 0.138, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 5 Step: 29 Total Loss: 0.259, BCE loss: 0.212, Diversity Loss: 0.094                     (Diversity_weight = 0.5)
Epoch: 5 Step: 30 Total Loss: 0.432, BCE loss: 0.284, Diversity Loss: 0.295                     (Diversity_weight = 0.5)
Epoch: 5 Step: 31 Total Loss: 0.343, BCE loss: 0.271, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 5 Step: 32 Total Loss: 0.433, BCE loss: 0.369, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 5 Step: 33 Total Loss: 0.309, BCE loss: 0.227, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 5 Step: 34 Total Loss: 0.470, BCE loss: 0.383, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 5 Step: 35 Total Loss: 0.397, BCE loss: 0.322, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 5 Step: 36 Total Loss: 0.185, BCE loss: 0.131, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 5 Step: 37 Total Loss: 0.567, BCE loss: 0.512, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 5 Step: 38 Total Loss: 0.338, BCE loss: 0.283, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 5 Step: 39 Total Loss: 0.393, BCE loss: 0.342, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 5 Step: 40 Total Loss: 0.406, BCE loss: 0.354, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 5 Step: 41 Total Loss: 0.413, BCE loss: 0.302, Diversity Loss: 0.223                     (Diversity_weight = 0.5)
Epoch: 5 Step: 42 Total Loss: 0.479, BCE loss: 0.428, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 5 Step: 43 Total Loss: 0.360, BCE loss: 0.265, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 5 Step: 44 Total Loss: 0.506, BCE loss: 0.444, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 5 Step: 45 Total Loss: 0.280, BCE loss: 0.211, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 5 Step: 46 Total Loss: 0.636, BCE loss: 0.540, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 5 Step: 47 Total Loss: 0.222, BCE loss: 0.156, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 5 Step: 48 Total Loss: 0.313, BCE loss: 0.204, Diversity Loss: 0.219                     (Diversity_weight = 0.5)
Epoch: 5 Step: 49 Total Loss: 0.344, BCE loss: 0.300, Diversity Loss: 0.087                     (Diversity_weight = 0.5)
Epoch: 5 Step: 50 Total Loss: 0.303, BCE loss: 0.244, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 5 Step: 51 Total Loss: 0.355, BCE loss: 0.281, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 5 Step: 52 Total Loss: 0.354, BCE loss: 0.313, Diversity Loss: 0.084                     (Diversity_weight = 0.5)
Epoch: 5 Step: 53 Total Loss: 0.380, BCE loss: 0.316, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 5 Step: 54 Total Loss: 0.450, BCE loss: 0.387, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 5 Step: 55 Total Loss: 0.260, BCE loss: 0.191, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 5 Step: 56 Total Loss: 0.369, BCE loss: 0.306, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 5 Step: 57 Total Loss: 0.584, BCE loss: 0.522, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 5 Step: 58 Total Loss: 0.517, BCE loss: 0.431, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 5 Step: 59 Total Loss: 0.240, BCE loss: 0.168, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 5 Step: 60 Total Loss: 0.206, BCE loss: 0.097, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 5 Step: 61 Total Loss: 0.504, BCE loss: 0.453, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 5 Step: 62 Total Loss: 0.355, BCE loss: 0.301, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
{'accuracy': 0.7151454363089268, 'roc_auc': 0.7811274884683191, 'pr_auc': 0.7805832027636375, 'conicity_mean': 0.13963343, 'conicity_std': 0.048389744}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.723    0.707      0.715      0.715         0.715
precision    0.713    0.717      0.715      0.715         0.715
recall       0.733    0.697      0.715      0.715         0.715
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7811274884683191
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:19:15_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.297, BCE loss: 0.201, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 6 Step: 1 Total Loss: 0.236, BCE loss: 0.165, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 6 Step: 2 Total Loss: 0.231, BCE loss: 0.154, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 6 Step: 3 Total Loss: 0.186, BCE loss: 0.098, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 6 Step: 4 Total Loss: 0.177, BCE loss: 0.111, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 6 Step: 5 Total Loss: 0.340, BCE loss: 0.297, Diversity Loss: 0.087                     (Diversity_weight = 0.5)
Epoch: 6 Step: 6 Total Loss: 0.209, BCE loss: 0.137, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 6 Step: 7 Total Loss: 0.238, BCE loss: 0.188, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 6 Step: 8 Total Loss: 0.347, BCE loss: 0.231, Diversity Loss: 0.232                     (Diversity_weight = 0.5)
Epoch: 6 Step: 9 Total Loss: 0.239, BCE loss: 0.200, Diversity Loss: 0.078                     (Diversity_weight = 0.5)
Epoch: 6 Step: 10 Total Loss: 0.342, BCE loss: 0.298, Diversity Loss: 0.086                     (Diversity_weight = 0.5)
Epoch: 6 Step: 11 Total Loss: 0.248, BCE loss: 0.167, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 6 Step: 12 Total Loss: 0.318, BCE loss: 0.258, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 6 Step: 13 Total Loss: 0.162, BCE loss: 0.102, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 6 Step: 14 Total Loss: 0.305, BCE loss: 0.247, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 6 Step: 15 Total Loss: 0.155, BCE loss: 0.077, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 6 Step: 16 Total Loss: 0.355, BCE loss: 0.265, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 6 Step: 17 Total Loss: 0.164, BCE loss: 0.112, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 6 Step: 18 Total Loss: 0.185, BCE loss: 0.109, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 6 Step: 19 Total Loss: 0.219, BCE loss: 0.138, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 6 Step: 20 Total Loss: 0.387, BCE loss: 0.322, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 6 Step: 21 Total Loss: 0.294, BCE loss: 0.242, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 6 Step: 22 Total Loss: 0.186, BCE loss: 0.094, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 6 Step: 23 Total Loss: 0.174, BCE loss: 0.123, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 6 Step: 24 Total Loss: 0.179, BCE loss: 0.124, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 6 Step: 25 Total Loss: 0.200, BCE loss: 0.150, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 6 Step: 26 Total Loss: 0.279, BCE loss: 0.211, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 6 Step: 27 Total Loss: 0.210, BCE loss: 0.138, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 6 Step: 28 Total Loss: 0.240, BCE loss: 0.192, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 6 Step: 29 Total Loss: 0.197, BCE loss: 0.122, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 6 Step: 30 Total Loss: 0.173, BCE loss: 0.090, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 6 Step: 31 Total Loss: 0.213, BCE loss: 0.128, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 6 Step: 32 Total Loss: 0.377, BCE loss: 0.317, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 6 Step: 33 Total Loss: 0.269, BCE loss: 0.212, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 6 Step: 34 Total Loss: 0.245, BCE loss: 0.189, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 6 Step: 35 Total Loss: 0.175, BCE loss: 0.121, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 6 Step: 36 Total Loss: 0.372, BCE loss: 0.227, Diversity Loss: 0.291                     (Diversity_weight = 0.5)
Epoch: 6 Step: 37 Total Loss: 0.138, BCE loss: 0.085, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 6 Step: 38 Total Loss: 0.137, BCE loss: 0.069, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 6 Step: 39 Total Loss: 0.280, BCE loss: 0.228, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 6 Step: 40 Total Loss: 0.253, BCE loss: 0.206, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 6 Step: 41 Total Loss: 0.253, BCE loss: 0.183, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 6 Step: 42 Total Loss: 0.146, BCE loss: 0.080, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 6 Step: 43 Total Loss: 0.175, BCE loss: 0.112, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 6 Step: 44 Total Loss: 0.139, BCE loss: 0.082, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 6 Step: 45 Total Loss: 0.237, BCE loss: 0.175, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 6 Step: 46 Total Loss: 0.421, BCE loss: 0.362, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 6 Step: 47 Total Loss: 0.344, BCE loss: 0.239, Diversity Loss: 0.210                     (Diversity_weight = 0.5)
Epoch: 6 Step: 48 Total Loss: 0.138, BCE loss: 0.070, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 6 Step: 49 Total Loss: 0.219, BCE loss: 0.111, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 6 Step: 50 Total Loss: 0.288, BCE loss: 0.203, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 6 Step: 51 Total Loss: 0.223, BCE loss: 0.114, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 6 Step: 52 Total Loss: 0.216, BCE loss: 0.146, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 6 Step: 53 Total Loss: 0.390, BCE loss: 0.332, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 6 Step: 54 Total Loss: 0.239, BCE loss: 0.147, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 6 Step: 55 Total Loss: 0.456, BCE loss: 0.387, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 6 Step: 56 Total Loss: 0.334, BCE loss: 0.280, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 6 Step: 57 Total Loss: 0.382, BCE loss: 0.314, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 6 Step: 58 Total Loss: 0.201, BCE loss: 0.139, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 6 Step: 59 Total Loss: 0.258, BCE loss: 0.191, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 6 Step: 60 Total Loss: 0.273, BCE loss: 0.200, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 6 Step: 61 Total Loss: 0.169, BCE loss: 0.103, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 6 Step: 62 Total Loss: 0.343, BCE loss: 0.248, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
{'accuracy': 0.7161484453360081, 'roc_auc': 0.7985678980543056, 'pr_auc': 0.7940813875697819, 'conicity_mean': 0.14119102, 'conicity_std': 0.04877186}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.692    0.737      0.716      0.714         0.714
precision    0.770    0.678      0.716      0.724         0.725
recall       0.628    0.807      0.716      0.717         0.716
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7985678980543056
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:19:15_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.242, BCE loss: 0.189, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 7 Step: 1 Total Loss: 0.263, BCE loss: 0.207, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 7 Step: 2 Total Loss: 0.145, BCE loss: 0.053, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 7 Step: 3 Total Loss: 0.155, BCE loss: 0.058, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 7 Step: 4 Total Loss: 0.155, BCE loss: 0.092, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 7 Step: 5 Total Loss: 0.255, BCE loss: 0.195, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 7 Step: 6 Total Loss: 0.325, BCE loss: 0.282, Diversity Loss: 0.086                     (Diversity_weight = 0.5)
Epoch: 7 Step: 7 Total Loss: 0.166, BCE loss: 0.115, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 7 Step: 8 Total Loss: 0.182, BCE loss: 0.105, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 7 Step: 9 Total Loss: 0.212, BCE loss: 0.149, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 7 Step: 10 Total Loss: 0.205, BCE loss: 0.094, Diversity Loss: 0.222                     (Diversity_weight = 0.5)
Epoch: 7 Step: 11 Total Loss: 0.164, BCE loss: 0.109, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 7 Step: 12 Total Loss: 0.220, BCE loss: 0.137, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 7 Step: 13 Total Loss: 0.169, BCE loss: 0.105, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 7 Step: 14 Total Loss: 0.185, BCE loss: 0.128, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 7 Step: 15 Total Loss: 0.157, BCE loss: 0.086, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 7 Step: 16 Total Loss: 0.171, BCE loss: 0.118, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 7 Step: 17 Total Loss: 0.341, BCE loss: 0.203, Diversity Loss: 0.277                     (Diversity_weight = 0.5)
Epoch: 7 Step: 18 Total Loss: 0.147, BCE loss: 0.062, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 7 Step: 19 Total Loss: 0.120, BCE loss: 0.073, Diversity Loss: 0.094                     (Diversity_weight = 0.5)
Epoch: 7 Step: 20 Total Loss: 0.236, BCE loss: 0.157, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 7 Step: 21 Total Loss: 0.364, BCE loss: 0.285, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 7 Step: 22 Total Loss: 0.145, BCE loss: 0.090, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 7 Step: 23 Total Loss: 0.189, BCE loss: 0.142, Diversity Loss: 0.095                     (Diversity_weight = 0.5)
Epoch: 7 Step: 24 Total Loss: 0.340, BCE loss: 0.282, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 7 Step: 25 Total Loss: 0.198, BCE loss: 0.104, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 7 Step: 26 Total Loss: 0.374, BCE loss: 0.325, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 7 Step: 27 Total Loss: 0.250, BCE loss: 0.154, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 7 Step: 28 Total Loss: 0.219, BCE loss: 0.161, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 7 Step: 29 Total Loss: 0.162, BCE loss: 0.079, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 7 Step: 30 Total Loss: 0.226, BCE loss: 0.158, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 7 Step: 31 Total Loss: 0.383, BCE loss: 0.342, Diversity Loss: 0.084                     (Diversity_weight = 0.5)
Epoch: 7 Step: 32 Total Loss: 0.165, BCE loss: 0.118, Diversity Loss: 0.095                     (Diversity_weight = 0.5)
Epoch: 7 Step: 33 Total Loss: 0.186, BCE loss: 0.122, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 7 Step: 34 Total Loss: 0.404, BCE loss: 0.356, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 7 Step: 35 Total Loss: 0.111, BCE loss: 0.041, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 7 Step: 36 Total Loss: 0.142, BCE loss: 0.069, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 7 Step: 37 Total Loss: 0.238, BCE loss: 0.179, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 7 Step: 38 Total Loss: 0.217, BCE loss: 0.151, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 7 Step: 39 Total Loss: 0.158, BCE loss: 0.086, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 7 Step: 40 Total Loss: 0.420, BCE loss: 0.326, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 7 Step: 41 Total Loss: 0.115, BCE loss: 0.050, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 7 Step: 42 Total Loss: 0.257, BCE loss: 0.135, Diversity Loss: 0.243                     (Diversity_weight = 0.5)
Epoch: 7 Step: 43 Total Loss: 0.250, BCE loss: 0.208, Diversity Loss: 0.084                     (Diversity_weight = 0.5)
Epoch: 7 Step: 44 Total Loss: 0.142, BCE loss: 0.067, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 7 Step: 45 Total Loss: 0.211, BCE loss: 0.139, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 7 Step: 46 Total Loss: 0.119, BCE loss: 0.046, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 7 Step: 47 Total Loss: 0.279, BCE loss: 0.199, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 7 Step: 48 Total Loss: 0.336, BCE loss: 0.258, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 7 Step: 49 Total Loss: 0.346, BCE loss: 0.291, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 7 Step: 50 Total Loss: 0.175, BCE loss: 0.121, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 7 Step: 51 Total Loss: 0.371, BCE loss: 0.310, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 7 Step: 52 Total Loss: 0.181, BCE loss: 0.124, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 7 Step: 53 Total Loss: 0.180, BCE loss: 0.107, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 7 Step: 54 Total Loss: 0.257, BCE loss: 0.191, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 7 Step: 55 Total Loss: 0.122, BCE loss: 0.064, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 7 Step: 56 Total Loss: 0.186, BCE loss: 0.083, Diversity Loss: 0.206                     (Diversity_weight = 0.5)
Epoch: 7 Step: 57 Total Loss: 0.305, BCE loss: 0.237, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 7 Step: 58 Total Loss: 0.210, BCE loss: 0.159, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 7 Step: 59 Total Loss: 0.142, BCE loss: 0.048, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 7 Step: 60 Total Loss: 0.166, BCE loss: 0.113, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 7 Step: 61 Total Loss: 0.110, BCE loss: 0.038, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 7 Step: 62 Total Loss: 0.166, BCE loss: 0.101, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
{'accuracy': 0.7161484453360081, 'roc_auc': 0.7816748911232219, 'pr_auc': 0.7816043617317097, 'conicity_mean': 0.13977417, 'conicity_std': 0.046826445}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.720    0.712      0.716      0.716         0.716
precision    0.721    0.711      0.716      0.716         0.716
recall       0.719    0.713      0.716      0.716         0.716
support    506.000  491.000    997.000    997.000       997.000
Model not saved on  roc_auc 0.7816748911232219
saved config  {'model': {'encoder': {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_de/cls_de/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:19:15_2021
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:20:13,636 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:20:13,636 - type = vanillalstm
INFO - 2021-01-17 19:20:13,636 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:20:13,636 - vocab_size = 648
INFO - 2021-01-17 19:20:13,636 - embed_size = 200
INFO - 2021-01-17 19:20:13,637 - hidden_size = 128
INFO - 2021-01-17 19:20:13,637 - pre_embed = None
INFO - 2021-01-17 19:20:13,651 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:20:13,652 - hidden_size = 256
INFO - 2021-01-17 19:20:13,652 - output_size = 1
INFO - 2021-01-17 19:20:13,652 - use_attention = True
INFO - 2021-01-17 19:20:13,652 - regularizer_attention = None
INFO - 2021-01-17 19:20:13,652 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b534ad3e050> and extras set()
INFO - 2021-01-17 19:20:13,652 - attention.type = tanh
INFO - 2021-01-17 19:20:13,652 - type = tanh
INFO - 2021-01-17 19:20:13,652 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b534ad3e050> and extras set()
INFO - 2021-01-17 19:20:13,652 - attention.hidden_size = 256
INFO - 2021-01-17 19:20:13,652 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7387387387387387, 'roc_auc': 0.8030379564150671, 'pr_auc': 0.7922277068598285, 'conicity_mean': '0.14017512', 'conicity_std': '0.04610775'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.720    0.755      0.739      0.738         0.738
precision    0.763    0.720      0.739      0.741         0.741
recall       0.681    0.795      0.739      0.738         0.739
support    492.000  507.000    999.000    999.000       999.000
encoder params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:20:15,073 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 648, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:20:15,073 - type = vanillalstm
INFO - 2021-01-17 19:20:15,073 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 648, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:20:15,073 - vocab_size = 648
INFO - 2021-01-17 19:20:15,073 - embed_size = 200
INFO - 2021-01-17 19:20:15,073 - hidden_size = 128
INFO - 2021-01-17 19:20:15,074 - pre_embed = None
INFO - 2021-01-17 19:20:15,088 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:20:15,088 - hidden_size = 256
INFO - 2021-01-17 19:20:15,088 - output_size = 1
INFO - 2021-01-17 19:20:15,088 - use_attention = True
INFO - 2021-01-17 19:20:15,088 - regularizer_attention = None
INFO - 2021-01-17 19:20:15,088 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b534a6cc550> and extras set()
INFO - 2021-01-17 19:20:15,088 - attention.type = tanh
INFO - 2021-01-17 19:20:15,088 - type = tanh
INFO - 2021-01-17 19:20:15,089 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b534a6cc550> and extras set()
INFO - 2021-01-17 19:20:15,089 - attention.hidden_size = 256
INFO - 2021-01-17 19:20:15,089 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.001001001001001], 'basepath': './experiments/cls_de', 'exp_dirname': 'cls_de/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7387387387387387, 'roc_auc': 0.8030379564150671, 'pr_auc': 0.7922277068598285, 'conicity_mean': '0.14017512', 'conicity_std': '0.04610775'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.720    0.755      0.739      0.738         0.738
precision    0.763    0.720      0.739      0.741         0.741
recall       0.681    0.795      0.739      0.738         0.739
support    492.000  507.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_fr vanilla_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:20:25,450 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:20:25,450 - type = vanillalstm
INFO - 2021-01-17 19:20:25,450 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:20:25,466 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:20:25,789 - vocab_size = 604
INFO - 2021-01-17 19:20:25,789 - embed_size = 200
INFO - 2021-01-17 19:20:25,789 - hidden_size = 128
INFO - 2021-01-17 19:20:25,790 - pre_embed = None
INFO - 2021-01-17 19:20:28,872 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:20:28,873 - hidden_size = 256
INFO - 2021-01-17 19:20:28,873 - output_size = 1
INFO - 2021-01-17 19:20:28,873 - use_attention = True
INFO - 2021-01-17 19:20:28,873 - regularizer_attention = None
INFO - 2021-01-17 19:20:28,873 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2aded00c02d0> and extras set()
INFO - 2021-01-17 19:20:28,874 - attention.type = tanh
INFO - 2021-01-17 19:20:28,874 - type = tanh
INFO - 2021-01-17 19:20:28,874 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2aded00c02d0> and extras set()
INFO - 2021-01-17 19:20:28,874 - attention.hidden_size = 256
INFO - 2021-01-17 19:20:28,874 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.496                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.708, BCE loss: 0.708, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.728, BCE loss: 0.728, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.726, BCE loss: 0.726, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.594                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.609                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.701, BCE loss: 0.701, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.712, BCE loss: 0.712, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.715, BCE loss: 0.715, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.704, BCE loss: 0.704, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.593                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.693                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.666, BCE loss: 0.666, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.697                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.732, BCE loss: 0.732, Diversity Loss: 0.617                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.704, BCE loss: 0.704, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.681, BCE loss: 0.681, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.668                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.666, BCE loss: 0.666, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.664, BCE loss: 0.664, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.584                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.644                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.784                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.669, BCE loss: 0.669, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.620, BCE loss: 0.620, Diversity Loss: 0.659                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.617                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.634, BCE loss: 0.634, Diversity Loss: 0.708                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.702                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.727                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.636, BCE loss: 0.636, Diversity Loss: 0.657                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.651                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.638, BCE loss: 0.638, Diversity Loss: 0.728                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.664, BCE loss: 0.664, Diversity Loss: 0.757                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.703, BCE loss: 0.703, Diversity Loss: 0.731                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.849                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.784, BCE loss: 0.784, Diversity Loss: 0.791                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.818                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.723                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.584, BCE loss: 0.584, Diversity Loss: 0.715                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.583, BCE loss: 0.583, Diversity Loss: 0.670                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.712                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.556, BCE loss: 0.556, Diversity Loss: 0.677                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.656                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.709, BCE loss: 0.709, Diversity Loss: 0.664                     (Diversity_weight = 0)
{'accuracy': 0.6342685370741483, 'roc_auc': 0.6856637950077314, 'pr_auc': 0.6781319739449341, 'conicity_mean': 0.6964566, 'conicity_std': 0.13545394}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.649    0.618      0.634      0.634         0.634
precision    0.628    0.641      0.634      0.635         0.635
recall       0.672    0.596      0.634      0.634         0.634
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.6856637950077314
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:20:29_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.607, BCE loss: 0.607, Diversity Loss: 0.664                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.718                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.731                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.776, BCE loss: 0.776, Diversity Loss: 0.714                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.745                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.479, BCE loss: 0.479, Diversity Loss: 0.773                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.598, BCE loss: 0.598, Diversity Loss: 0.808                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.702, BCE loss: 0.702, Diversity Loss: 0.794                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.639, BCE loss: 0.639, Diversity Loss: 0.724                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.710, BCE loss: 0.710, Diversity Loss: 0.806                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.774                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.492, BCE loss: 0.492, Diversity Loss: 0.716                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.589, BCE loss: 0.589, Diversity Loss: 0.793                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.597, BCE loss: 0.597, Diversity Loss: 0.729                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.612, BCE loss: 0.612, Diversity Loss: 0.822                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.750                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.612, BCE loss: 0.612, Diversity Loss: 0.752                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.600, BCE loss: 0.600, Diversity Loss: 0.790                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.579, BCE loss: 0.579, Diversity Loss: 0.718                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.541, BCE loss: 0.541, Diversity Loss: 0.704                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.634, BCE loss: 0.634, Diversity Loss: 0.689                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.590, BCE loss: 0.590, Diversity Loss: 0.860                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.627, BCE loss: 0.627, Diversity Loss: 0.684                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.580, BCE loss: 0.580, Diversity Loss: 0.758                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.587, BCE loss: 0.587, Diversity Loss: 0.703                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.747                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.569, BCE loss: 0.569, Diversity Loss: 0.712                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.550, BCE loss: 0.550, Diversity Loss: 0.720                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.646, BCE loss: 0.646, Diversity Loss: 0.691                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.489, BCE loss: 0.489, Diversity Loss: 0.639                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.724                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.613                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.760                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.743                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.626, BCE loss: 0.626, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.716                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.639, BCE loss: 0.639, Diversity Loss: 0.806                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.742, BCE loss: 0.742, Diversity Loss: 0.732                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.639, BCE loss: 0.639, Diversity Loss: 0.659                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.598, BCE loss: 0.598, Diversity Loss: 0.685                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.720                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.537, BCE loss: 0.537, Diversity Loss: 0.640                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.523, BCE loss: 0.523, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.643, BCE loss: 0.643, Diversity Loss: 0.656                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.747, BCE loss: 0.747, Diversity Loss: 0.595                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.529, BCE loss: 0.529, Diversity Loss: 0.700                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.708                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.720                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.567, BCE loss: 0.567, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.562, BCE loss: 0.562, Diversity Loss: 0.602                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.573, BCE loss: 0.573, Diversity Loss: 0.722                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.639                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.708                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.554, BCE loss: 0.554, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.558, BCE loss: 0.558, Diversity Loss: 0.766                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.624                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.615, BCE loss: 0.615, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.615, BCE loss: 0.615, Diversity Loss: 0.660                     (Diversity_weight = 0)
{'accuracy': 0.685370741482966, 'roc_auc': 0.7420808482438701, 'pr_auc': 0.6989008735133453, 'conicity_mean': 0.66196066, 'conicity_std': 0.10842822}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.631    0.726      0.685      0.678         0.678
precision    0.772    0.639      0.685      0.706         0.706
recall       0.533    0.840      0.685      0.687         0.685
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7420808482438701
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:20:29_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.416, BCE loss: 0.416, Diversity Loss: 0.812                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.630                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.623                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.647                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.545, BCE loss: 0.545, Diversity Loss: 0.780                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.584, BCE loss: 0.584, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.296, BCE loss: 0.296, Diversity Loss: 0.730                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.513, BCE loss: 0.513, Diversity Loss: 0.699                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.539, BCE loss: 0.539, Diversity Loss: 0.664                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.699                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.485, BCE loss: 0.485, Diversity Loss: 0.704                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.578, BCE loss: 0.578, Diversity Loss: 0.695                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.605, BCE loss: 0.605, Diversity Loss: 0.637                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.561, BCE loss: 0.561, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.687                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.655                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.667                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.484, BCE loss: 0.484, Diversity Loss: 0.679                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.684                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.410, BCE loss: 0.410, Diversity Loss: 0.739                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.530, BCE loss: 0.530, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.695                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.722                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.487, BCE loss: 0.487, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.463, BCE loss: 0.463, Diversity Loss: 0.613                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.528, BCE loss: 0.528, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.749                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.402, BCE loss: 0.402, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.606, BCE loss: 0.606, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.658, BCE loss: 0.658, Diversity Loss: 0.655                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.542, BCE loss: 0.542, Diversity Loss: 0.655                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.676                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.542, BCE loss: 0.542, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.633                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.443, BCE loss: 0.443, Diversity Loss: 0.662                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.514, BCE loss: 0.514, Diversity Loss: 0.697                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.454, BCE loss: 0.454, Diversity Loss: 0.637                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.557, BCE loss: 0.557, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.733                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.612, BCE loss: 0.612, Diversity Loss: 0.683                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.750, BCE loss: 0.750, Diversity Loss: 0.662                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.682                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.478, BCE loss: 0.478, Diversity Loss: 0.665                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.474, BCE loss: 0.474, Diversity Loss: 0.712                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.458, BCE loss: 0.458, Diversity Loss: 0.647                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.428, BCE loss: 0.428, Diversity Loss: 0.683                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.458, BCE loss: 0.458, Diversity Loss: 0.703                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.706, BCE loss: 0.706, Diversity Loss: 0.748                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.500, BCE loss: 0.500, Diversity Loss: 0.633                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.590, BCE loss: 0.590, Diversity Loss: 0.656                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.497, BCE loss: 0.497, Diversity Loss: 0.649                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.579, BCE loss: 0.579, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.699                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.579, BCE loss: 0.579, Diversity Loss: 0.691                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.499, BCE loss: 0.499, Diversity Loss: 0.634                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.570, BCE loss: 0.570, Diversity Loss: 0.696                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.566                     (Diversity_weight = 0)
{'accuracy': 0.6983967935871743, 'roc_auc': 0.7732353354619756, 'pr_auc': 0.7358933395914814, 'conicity_mean': 0.5922155, 'conicity_std': 0.1082893}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.723    0.670      0.698      0.696         0.696
precision    0.674    0.733      0.698      0.703         0.703
recall       0.779    0.616      0.698      0.698         0.698
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7732353354619756
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:20:29_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.432, BCE loss: 0.432, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.773                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.399, BCE loss: 0.399, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.387, BCE loss: 0.387, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.447, BCE loss: 0.447, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.682                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.444, BCE loss: 0.444, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.742                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.450, BCE loss: 0.450, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.333, BCE loss: 0.333, Diversity Loss: 0.730                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.348, BCE loss: 0.348, Diversity Loss: 0.701                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.456, BCE loss: 0.456, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.414, BCE loss: 0.414, Diversity Loss: 0.633                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.534, BCE loss: 0.534, Diversity Loss: 0.683                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.406, BCE loss: 0.406, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.360, BCE loss: 0.360, Diversity Loss: 0.617                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.345, BCE loss: 0.345, Diversity Loss: 0.675                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.605, BCE loss: 0.605, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.477, BCE loss: 0.477, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.593                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.516, BCE loss: 0.516, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.384, BCE loss: 0.384, Diversity Loss: 0.561                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.658                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.401, BCE loss: 0.401, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.320, BCE loss: 0.320, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.387, BCE loss: 0.387, Diversity Loss: 0.637                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.328, BCE loss: 0.328, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.404, BCE loss: 0.404, Diversity Loss: 0.629                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.445, BCE loss: 0.445, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.391, BCE loss: 0.391, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.631                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.710                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.721                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.410, BCE loss: 0.410, Diversity Loss: 0.728                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.666                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.522, BCE loss: 0.522, Diversity Loss: 0.694                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.659                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.439, BCE loss: 0.439, Diversity Loss: 0.633                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.412, BCE loss: 0.412, Diversity Loss: 0.656                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.386, BCE loss: 0.386, Diversity Loss: 0.645                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.390, BCE loss: 0.390, Diversity Loss: 0.595                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.640                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.429, BCE loss: 0.429, Diversity Loss: 0.629                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.381, BCE loss: 0.381, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.290, BCE loss: 0.290, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.412, BCE loss: 0.412, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.457, BCE loss: 0.457, Diversity Loss: 0.593                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.388, BCE loss: 0.388, Diversity Loss: 0.631                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.517, BCE loss: 0.517, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.356, BCE loss: 0.356, Diversity Loss: 0.682                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.485, BCE loss: 0.485, Diversity Loss: 0.521                     (Diversity_weight = 0)
{'accuracy': 0.7134268537074149, 'roc_auc': 0.7828584051248066, 'pr_auc': 0.7466812059177402, 'conicity_mean': 0.56429976, 'conicity_std': 0.10433037}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.724    0.702      0.713      0.713         0.713
precision    0.704    0.725      0.713      0.714         0.714
recall       0.746    0.681      0.713      0.713         0.713
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7828584051248066
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:20:29_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.302, BCE loss: 0.302, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.652                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.319, BCE loss: 0.319, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.308, BCE loss: 0.308, Diversity Loss: 0.586                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.249, BCE loss: 0.249, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.650                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.255, BCE loss: 0.255, Diversity Loss: 0.594                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.343, BCE loss: 0.343, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.422, BCE loss: 0.422, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.336, BCE loss: 0.336, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.415, BCE loss: 0.415, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.649                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.379, BCE loss: 0.379, Diversity Loss: 0.591                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.248, BCE loss: 0.248, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.251, BCE loss: 0.251, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.328, BCE loss: 0.328, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.317, BCE loss: 0.317, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.343, BCE loss: 0.343, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.678                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.254, BCE loss: 0.254, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.263, BCE loss: 0.263, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.268, BCE loss: 0.268, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.599                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.320, BCE loss: 0.320, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.231, BCE loss: 0.231, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.233, BCE loss: 0.233, Diversity Loss: 0.636                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.377, BCE loss: 0.377, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.423, BCE loss: 0.423, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.755                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.361, BCE loss: 0.361, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.303, BCE loss: 0.303, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.341, BCE loss: 0.341, Diversity Loss: 0.680                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.252, BCE loss: 0.252, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.289, BCE loss: 0.289, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.339, BCE loss: 0.339, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.623                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.514, BCE loss: 0.514, Diversity Loss: 0.653                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.367, BCE loss: 0.367, Diversity Loss: 0.582                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.658                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.546, BCE loss: 0.546, Diversity Loss: 0.676                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.655                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.363, BCE loss: 0.363, Diversity Loss: 0.639                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.336, BCE loss: 0.336, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.593                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.373, BCE loss: 0.373, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.373, BCE loss: 0.373, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.287, BCE loss: 0.287, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.353, BCE loss: 0.353, Diversity Loss: 0.586                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.337, BCE loss: 0.337, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.268, BCE loss: 0.268, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.322, BCE loss: 0.322, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.458, BCE loss: 0.458, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.264, BCE loss: 0.264, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.425, BCE loss: 0.425, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.446, BCE loss: 0.446, Diversity Loss: 0.683                     (Diversity_weight = 0)
{'accuracy': 0.7354709418837675, 'roc_auc': 0.7961604112697552, 'pr_auc': 0.7501672956274777, 'conicity_mean': 0.5587608, 'conicity_std': 0.1025246}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.736    0.735      0.735      0.735         0.735
precision    0.740    0.731      0.735      0.735         0.736
recall       0.732    0.739      0.735      0.736         0.735
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7961604112697552
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:20:29_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.283, BCE loss: 0.283, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.248, BCE loss: 0.248, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.195, BCE loss: 0.195, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.229, BCE loss: 0.229, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.220, BCE loss: 0.220, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.285, BCE loss: 0.285, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.291, BCE loss: 0.291, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.249, BCE loss: 0.249, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.231, BCE loss: 0.231, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.126, BCE loss: 0.126, Diversity Loss: 0.585                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.335, BCE loss: 0.335, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.210, BCE loss: 0.210, Diversity Loss: 0.632                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.399, BCE loss: 0.399, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.125, BCE loss: 0.125, Diversity Loss: 0.670                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.750                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.645                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.189, BCE loss: 0.189, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.174, BCE loss: 0.174, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.306, BCE loss: 0.306, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.388, BCE loss: 0.388, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.155, BCE loss: 0.155, Diversity Loss: 0.578                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.374, BCE loss: 0.374, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.517, BCE loss: 0.517, Diversity Loss: 0.624                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.160, BCE loss: 0.160, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.585                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.254, BCE loss: 0.254, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.415, BCE loss: 0.415, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.625                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.574                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.313, BCE loss: 0.313, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.629                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.281, BCE loss: 0.281, Diversity Loss: 0.582                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.631                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.216, BCE loss: 0.216, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.295, BCE loss: 0.295, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.285, BCE loss: 0.285, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.250, BCE loss: 0.250, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.305, BCE loss: 0.305, Diversity Loss: 0.668                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.242, BCE loss: 0.242, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.182, BCE loss: 0.182, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.606                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.634                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.270, BCE loss: 0.270, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.316, BCE loss: 0.316, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.556                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.248, BCE loss: 0.248, Diversity Loss: 0.535                     (Diversity_weight = 0)
{'accuracy': 0.7224448897795591, 'roc_auc': 0.7761230596220656, 'pr_auc': 0.7334375410312637, 'conicity_mean': 0.5805444, 'conicity_std': 0.09424121}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.718    0.727      0.722      0.722         0.722
precision    0.736    0.710      0.722      0.723         0.723
recall       0.700    0.745      0.722      0.723         0.722
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7761230596220656
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:20:29_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.651                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.118, BCE loss: 0.118, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.653                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.248, BCE loss: 0.248, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.559                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.098, BCE loss: 0.098, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.702                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.114, BCE loss: 0.114, Diversity Loss: 0.670                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.614                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.613                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.302, BCE loss: 0.302, Diversity Loss: 0.544                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.124, BCE loss: 0.124, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.050, BCE loss: 0.050, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.279, BCE loss: 0.279, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.310, BCE loss: 0.310, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.185, BCE loss: 0.185, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.383, BCE loss: 0.383, Diversity Loss: 0.541                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.545                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.746                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.791, BCE loss: 0.791, Diversity Loss: 0.646                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.210, BCE loss: 0.210, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.151, BCE loss: 0.151, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.104, BCE loss: 0.104, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.114, BCE loss: 0.114, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.240, BCE loss: 0.240, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.322, BCE loss: 0.322, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.279, BCE loss: 0.279, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.188, BCE loss: 0.188, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.182, BCE loss: 0.182, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.159, BCE loss: 0.159, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.602                     (Diversity_weight = 0)
{'accuracy': 0.7254509018036072, 'roc_auc': 0.7982770046388337, 'pr_auc': 0.7696795850512932, 'conicity_mean': 0.54867166, 'conicity_std': 0.10410044}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.752    0.693      0.725      0.722         0.723
precision    0.691    0.778      0.725      0.734         0.734
recall       0.825    0.624      0.725      0.725         0.725
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7982770046388337
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:20:29_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.102, BCE loss: 0.102, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.318, BCE loss: 0.318, Diversity Loss: 0.598                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.191, BCE loss: 0.191, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.130, BCE loss: 0.130, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.574                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.150, BCE loss: 0.150, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.213, BCE loss: 0.213, Diversity Loss: 0.589                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.187, BCE loss: 0.187, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.549                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.610                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.165, BCE loss: 0.165, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.130, BCE loss: 0.130, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.174, BCE loss: 0.174, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.460                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.105, BCE loss: 0.105, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.611                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.349, BCE loss: 0.349, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.134, BCE loss: 0.134, Diversity Loss: 0.699                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.072, BCE loss: 0.072, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.643                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.474                     (Diversity_weight = 0)
{'accuracy': 0.7294589178356713, 'roc_auc': 0.7950679759824889, 'pr_auc': 0.7736481367473254, 'conicity_mean': 0.52122754, 'conicity_std': 0.09571364}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.745    0.712      0.729      0.728         0.729
precision    0.710    0.754      0.729      0.732         0.732
recall       0.783    0.675      0.729      0.729         0.729
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7950679759824889
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
./experiments/cls_fr/cls_fr/lstm+tanh/Sun_Jan_17_19:20:29_2021
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:21:23,760 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:21:23,761 - type = vanillalstm
INFO - 2021-01-17 19:21:23,761 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:21:23,761 - vocab_size = 604
INFO - 2021-01-17 19:21:23,761 - embed_size = 200
INFO - 2021-01-17 19:21:23,761 - hidden_size = 128
INFO - 2021-01-17 19:21:23,761 - pre_embed = None
INFO - 2021-01-17 19:21:23,775 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:21:23,776 - hidden_size = 256
INFO - 2021-01-17 19:21:23,776 - output_size = 1
INFO - 2021-01-17 19:21:23,776 - use_attention = True
INFO - 2021-01-17 19:21:23,776 - regularizer_attention = None
INFO - 2021-01-17 19:21:23,776 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2aded00f0610> and extras set()
INFO - 2021-01-17 19:21:23,776 - attention.type = tanh
INFO - 2021-01-17 19:21:23,776 - type = tanh
INFO - 2021-01-17 19:21:23,776 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2aded00f0610> and extras set()
INFO - 2021-01-17 19:21:23,776 - attention.hidden_size = 256
INFO - 2021-01-17 19:21:23,776 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.6783567134268537, 'roc_auc': 0.7737494226559832, 'pr_auc': 0.7562780008355373, 'conicity_mean': '0.5397995', 'conicity_std': '0.105024226'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.715    0.631      0.678      0.673         0.673
precision    0.638    0.747      0.678      0.693         0.693
recall       0.812    0.547      0.678      0.679         0.678
support    495.000  503.000    998.000    998.000       998.000
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:21:25,120 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:21:25,120 - type = vanillalstm
INFO - 2021-01-17 19:21:25,120 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:21:25,120 - vocab_size = 604
INFO - 2021-01-17 19:21:25,121 - embed_size = 200
INFO - 2021-01-17 19:21:25,121 - hidden_size = 128
INFO - 2021-01-17 19:21:25,121 - pre_embed = None
INFO - 2021-01-17 19:21:25,134 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:21:25,135 - hidden_size = 256
INFO - 2021-01-17 19:21:25,135 - output_size = 1
INFO - 2021-01-17 19:21:25,135 - use_attention = True
INFO - 2021-01-17 19:21:25,135 - regularizer_attention = None
INFO - 2021-01-17 19:21:25,135 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2adebb577e10> and extras set()
INFO - 2021-01-17 19:21:25,135 - attention.type = tanh
INFO - 2021-01-17 19:21:25,135 - type = tanh
INFO - 2021-01-17 19:21:25,135 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2adebb577e10> and extras set()
INFO - 2021-01-17 19:21:25,135 - attention.hidden_size = 256
INFO - 2021-01-17 19:21:25,135 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.6783567134268537, 'roc_auc': 0.7737494226559832, 'pr_auc': 0.7562780008355373, 'conicity_mean': '0.5397995', 'conicity_std': '0.105024226'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.715    0.631      0.678      0.673         0.673
precision    0.638    0.747      0.678      0.693         0.693
recall       0.812    0.547      0.678      0.679         0.678
support    495.000  503.000    998.000    998.000       998.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_fr ortho_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:21:35,434 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:21:35,434 - type = ortholstm
INFO - 2021-01-17 19:21:35,434 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:21:35,449 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:21:35,772 - vocab_size = 604
INFO - 2021-01-17 19:21:35,772 - embed_size = 200
INFO - 2021-01-17 19:21:35,772 - hidden_size = 128
INFO - 2021-01-17 19:21:35,772 - pre_embed = None
INFO - 2021-01-17 19:21:38,841 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:21:38,842 - hidden_size = 256
INFO - 2021-01-17 19:21:38,842 - output_size = 1
INFO - 2021-01-17 19:21:38,842 - use_attention = True
INFO - 2021-01-17 19:21:38,842 - regularizer_attention = None
INFO - 2021-01-17 19:21:38,842 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ac7039ab4d0> and extras set()
INFO - 2021-01-17 19:21:38,842 - attention.type = tanh
INFO - 2021-01-17 19:21:38,843 - type = tanh
INFO - 2021-01-17 19:21:38,843 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ac7039ab4d0> and extras set()
INFO - 2021-01-17 19:21:38,843 - attention.hidden_size = 256
INFO - 2021-01-17 19:21:38,843 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.228                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.111                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.243                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.681, BCE loss: 0.681, Diversity Loss: 0.296                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.104                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.642, BCE loss: 0.642, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.669, BCE loss: 0.669, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.628, BCE loss: 0.628, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.630, BCE loss: 0.630, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.748, BCE loss: 0.748, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.644, BCE loss: 0.644, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.703, BCE loss: 0.703, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.262                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.636, BCE loss: 0.636, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.634, BCE loss: 0.634, Diversity Loss: 0.186                     (Diversity_weight = 0)
{'accuracy': 0.6623246492985972, 'roc_auc': 0.7057091792678274, 'pr_auc': 0.6763708540819027, 'conicity_mean': 0.18015651, 'conicity_std': 0.045785088}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.651    0.672      0.662      0.662         0.662
precision    0.679    0.648      0.662      0.663         0.664
recall       0.626    0.699      0.662      0.663         0.662
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7057091792678274
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:21:39_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.629, BCE loss: 0.629, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.651, BCE loss: 0.651, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.657, BCE loss: 0.657, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.572, BCE loss: 0.572, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.635, BCE loss: 0.635, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.599, BCE loss: 0.599, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.664, BCE loss: 0.664, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.519, BCE loss: 0.519, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.564, BCE loss: 0.564, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.756, BCE loss: 0.756, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.515, BCE loss: 0.515, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.521, BCE loss: 0.521, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.603, BCE loss: 0.603, Diversity Loss: 0.228                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.643, BCE loss: 0.643, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.567, BCE loss: 0.567, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.591, BCE loss: 0.591, Diversity Loss: 0.123                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.508, BCE loss: 0.508, Diversity Loss: 0.256                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.582, BCE loss: 0.582, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.555, BCE loss: 0.555, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.522, BCE loss: 0.522, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.099                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.513, BCE loss: 0.513, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.526, BCE loss: 0.526, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.530, BCE loss: 0.530, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.510, BCE loss: 0.510, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.614, BCE loss: 0.614, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.487, BCE loss: 0.487, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.627, BCE loss: 0.627, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.562, BCE loss: 0.562, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.504, BCE loss: 0.504, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.569, BCE loss: 0.569, Diversity Loss: 0.241                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.603, BCE loss: 0.603, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.626, BCE loss: 0.626, Diversity Loss: 0.256                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.424, BCE loss: 0.424, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.536, BCE loss: 0.536, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.555, BCE loss: 0.555, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.295                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.400, BCE loss: 0.400, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.651, BCE loss: 0.651, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.652, BCE loss: 0.652, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.646, BCE loss: 0.646, Diversity Loss: 0.266                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.487, BCE loss: 0.487, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.461, BCE loss: 0.461, Diversity Loss: 0.157                     (Diversity_weight = 0)
{'accuracy': 0.7094188376753507, 'roc_auc': 0.7563467678775829, 'pr_auc': 0.7019939632830212, 'conicity_mean': 0.1733896, 'conicity_std': 0.044225194}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.669    0.741      0.709      0.705         0.705
precision    0.786    0.664      0.709      0.725         0.725
recall       0.583    0.838      0.709      0.710         0.709
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7563467678775829
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:21:39_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.125                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.370, BCE loss: 0.370, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.427, BCE loss: 0.427, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.537, BCE loss: 0.537, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.389, BCE loss: 0.389, Diversity Loss: 0.257                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.422, BCE loss: 0.422, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.440, BCE loss: 0.440, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.410, BCE loss: 0.410, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.516, BCE loss: 0.516, Diversity Loss: 0.234                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.453, BCE loss: 0.453, Diversity Loss: 0.245                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.267, BCE loss: 0.267, Diversity Loss: 0.095                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.614, BCE loss: 0.614, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.463, BCE loss: 0.463, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.354, BCE loss: 0.354, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.538, BCE loss: 0.538, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.360, BCE loss: 0.360, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.523, BCE loss: 0.523, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.480, BCE loss: 0.480, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.322, BCE loss: 0.322, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.473, BCE loss: 0.473, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.350, BCE loss: 0.350, Diversity Loss: 0.228                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.480, BCE loss: 0.480, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.449, BCE loss: 0.449, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.541, BCE loss: 0.541, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.443, BCE loss: 0.443, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.400, BCE loss: 0.400, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.544, BCE loss: 0.544, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.456, BCE loss: 0.456, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.486, BCE loss: 0.486, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.583, BCE loss: 0.583, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.438, BCE loss: 0.438, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.430, BCE loss: 0.430, Diversity Loss: 0.269                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.460, BCE loss: 0.460, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.485, BCE loss: 0.485, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.515, BCE loss: 0.515, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.373, BCE loss: 0.373, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.425, BCE loss: 0.425, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.589, BCE loss: 0.589, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.422, BCE loss: 0.422, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.372, BCE loss: 0.372, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.442, BCE loss: 0.442, Diversity Loss: 0.110                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.492, BCE loss: 0.492, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.497, BCE loss: 0.497, Diversity Loss: 0.292                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.438, BCE loss: 0.438, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.120                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.357, BCE loss: 0.357, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.328, BCE loss: 0.328, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.291, BCE loss: 0.291, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.537, BCE loss: 0.537, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.424, BCE loss: 0.424, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.509, BCE loss: 0.509, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.586, BCE loss: 0.586, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.539, BCE loss: 0.539, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.484, BCE loss: 0.484, Diversity Loss: 0.161                     (Diversity_weight = 0)
{'accuracy': 0.7344689378757515, 'roc_auc': 0.7916460830973753, 'pr_auc': 0.7349023421902146, 'conicity_mean': 0.17370386, 'conicity_std': 0.04279328}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.725    0.743      0.734      0.734         0.734
precision    0.759    0.714      0.734      0.736         0.736
recall       0.694    0.776      0.734      0.735         0.734
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7916460830973753
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:21:39_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.351, BCE loss: 0.351, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.312, BCE loss: 0.312, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.413, BCE loss: 0.413, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.290, BCE loss: 0.290, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.247                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.383, BCE loss: 0.383, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.290, BCE loss: 0.290, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.328, BCE loss: 0.328, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.244                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.339, BCE loss: 0.339, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.358, BCE loss: 0.358, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.228, BCE loss: 0.228, Diversity Loss: 0.290                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.241                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.310, BCE loss: 0.310, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.164, BCE loss: 0.164, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.496, BCE loss: 0.496, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.260, BCE loss: 0.260, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.333, BCE loss: 0.333, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.369, BCE loss: 0.369, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.234, BCE loss: 0.234, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.370, BCE loss: 0.370, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.425, BCE loss: 0.425, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.303, BCE loss: 0.303, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.372, BCE loss: 0.372, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.402, BCE loss: 0.402, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.233, BCE loss: 0.233, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.459, BCE loss: 0.459, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.231, BCE loss: 0.231, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.457, BCE loss: 0.457, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.581, BCE loss: 0.581, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.236, BCE loss: 0.236, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.459, BCE loss: 0.459, Diversity Loss: 0.264                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.383, BCE loss: 0.383, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.303, BCE loss: 0.303, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.366, BCE loss: 0.366, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.233, BCE loss: 0.233, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.273, BCE loss: 0.273, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.358, BCE loss: 0.358, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.326, BCE loss: 0.326, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.508, BCE loss: 0.508, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.356, BCE loss: 0.356, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.380, BCE loss: 0.380, Diversity Loss: 0.108                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.374, BCE loss: 0.374, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.497, BCE loss: 0.497, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.389, BCE loss: 0.389, Diversity Loss: 0.191                     (Diversity_weight = 0)
{'accuracy': 0.7234468937875751, 'roc_auc': 0.7734321344659316, 'pr_auc': 0.7315790521140573, 'conicity_mean': 0.1744757, 'conicity_std': 0.041984424}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.714    0.733      0.723      0.723         0.723
precision    0.746    0.704      0.723      0.725         0.725
recall       0.684    0.764      0.723      0.724         0.723
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7734321344659316
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:21:39_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.257, BCE loss: 0.257, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.384, BCE loss: 0.384, Diversity Loss: 0.119                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.272, BCE loss: 0.272, Diversity Loss: 0.107                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.275, BCE loss: 0.275, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.258, BCE loss: 0.258, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.125, BCE loss: 0.125, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.249, BCE loss: 0.249, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.104, BCE loss: 0.104, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.229                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.340, BCE loss: 0.340, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.174, BCE loss: 0.174, Diversity Loss: 0.249                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.288, BCE loss: 0.288, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.290, BCE loss: 0.290, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.173, BCE loss: 0.173, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.174, BCE loss: 0.174, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.172, BCE loss: 0.172, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.134, BCE loss: 0.134, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.270, BCE loss: 0.270, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.311, BCE loss: 0.311, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.253, BCE loss: 0.253, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.373, BCE loss: 0.373, Diversity Loss: 0.266                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.274, BCE loss: 0.274, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.102, BCE loss: 0.102, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.151, BCE loss: 0.151, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.215, BCE loss: 0.215, Diversity Loss: 0.247                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.191, BCE loss: 0.191, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.357, BCE loss: 0.357, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.136, BCE loss: 0.136, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.247, BCE loss: 0.247, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.280, BCE loss: 0.280, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.217, BCE loss: 0.217, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.331, BCE loss: 0.331, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.288                     (Diversity_weight = 0)
{'accuracy': 0.7274549098196392, 'roc_auc': 0.789360804867763, 'pr_auc': 0.7562768941708937, 'conicity_mean': 0.1812831, 'conicity_std': 0.043045055}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.709    0.743      0.727      0.726         0.726
precision    0.767    0.697      0.727      0.732         0.732
recall       0.660    0.796      0.727      0.728         0.727
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.789360804867763
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:21:39_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.133, BCE loss: 0.133, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.130, BCE loss: 0.130, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.140, BCE loss: 0.140, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.126, BCE loss: 0.126, Diversity Loss: 0.118                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.234                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.150, BCE loss: 0.150, Diversity Loss: 0.260                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.092, BCE loss: 0.092, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.160, BCE loss: 0.160, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.288                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.244, BCE loss: 0.244, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.172, BCE loss: 0.172, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.345, BCE loss: 0.345, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.110, BCE loss: 0.110, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.156, BCE loss: 0.156, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.241                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.133, BCE loss: 0.133, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.179, BCE loss: 0.179, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.097                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.173, BCE loss: 0.173, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.187, BCE loss: 0.187, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.155, BCE loss: 0.155, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.218, BCE loss: 0.218, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.158, BCE loss: 0.158, Diversity Loss: 0.252                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.089, BCE loss: 0.089, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.061, BCE loss: 0.061, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.048, BCE loss: 0.048, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.167, BCE loss: 0.167, Diversity Loss: 0.182                     (Diversity_weight = 0)
{'accuracy': 0.7164328657314629, 'roc_auc': 0.7855372813623311, 'pr_auc': 0.7446061132537328, 'conicity_mean': 0.17905872, 'conicity_std': 0.0433102}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.703    0.729      0.716      0.716         0.716
precision    0.744    0.693      0.716      0.719         0.719
recall       0.666    0.768      0.716      0.717         0.716
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7855372813623311
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:21:39_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.282                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.026, BCE loss: 0.026, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.114                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.226                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.165, BCE loss: 0.165, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.098, BCE loss: 0.098, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.263                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.086, BCE loss: 0.086, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.115, BCE loss: 0.115, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.249                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.050, BCE loss: 0.050, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.208, BCE loss: 0.208, Diversity Loss: 0.124                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.240                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.133, BCE loss: 0.133, Diversity Loss: 0.234                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.127, BCE loss: 0.127, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.097                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.063, BCE loss: 0.063, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.131                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.167                     (Diversity_weight = 0)
{'accuracy': 0.720440881763527, 'roc_auc': 0.7692591923208225, 'pr_auc': 0.7248736533468502, 'conicity_mean': 0.17632176, 'conicity_std': 0.042725436}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.699    0.739       0.72      0.719         0.719
precision    0.764    0.688       0.72      0.726         0.726
recall       0.644    0.798       0.72      0.721         0.720
support    503.000  495.000     998.00    998.000       998.000
Model not saved on  roc_auc 0.7692591923208225
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:21:39_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.014, BCE loss: 0.014, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.129, BCE loss: 0.129, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.234                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.113                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.008, BCE loss: 0.008, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.280                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.038, BCE loss: 0.038, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.059, BCE loss: 0.059, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.116, BCE loss: 0.116, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.008, BCE loss: 0.008, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.104, BCE loss: 0.104, Diversity Loss: 0.098                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.045, BCE loss: 0.045, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.008, BCE loss: 0.008, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.014, BCE loss: 0.014, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.007, BCE loss: 0.007, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.256                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.258                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.007, BCE loss: 0.007, Diversity Loss: 0.242                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.169                     (Diversity_weight = 0)
{'accuracy': 0.7314629258517034, 'roc_auc': 0.7869229070024298, 'pr_auc': 0.7505383456292933, 'conicity_mean': 0.17928156, 'conicity_std': 0.042649027}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.734    0.729      0.731      0.731         0.731
precision    0.734    0.729      0.731      0.731         0.731
recall       0.734    0.729      0.731      0.731         0.731
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7869229070024298
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
./experiments/cls_fr/cls_fr/ortho_lstm+tanh/Sun_Jan_17_19:21:39_2021
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:85: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:86: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:23:11,076 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:23:11,077 - type = ortholstm
INFO - 2021-01-17 19:23:11,077 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:23:11,077 - vocab_size = 604
INFO - 2021-01-17 19:23:11,077 - embed_size = 200
INFO - 2021-01-17 19:23:11,077 - hidden_size = 128
INFO - 2021-01-17 19:23:11,077 - pre_embed = None
INFO - 2021-01-17 19:23:11,092 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:23:11,092 - hidden_size = 256
INFO - 2021-01-17 19:23:11,092 - output_size = 1
INFO - 2021-01-17 19:23:11,092 - use_attention = True
INFO - 2021-01-17 19:23:11,092 - regularizer_attention = None
INFO - 2021-01-17 19:23:11,092 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ac7039d6190> and extras set()
INFO - 2021-01-17 19:23:11,092 - attention.type = tanh
INFO - 2021-01-17 19:23:11,093 - type = tanh
INFO - 2021-01-17 19:23:11,093 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ac7039d6190> and extras set()
INFO - 2021-01-17 19:23:11,093 - attention.hidden_size = 256
INFO - 2021-01-17 19:23:11,093 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7384769539078156, 'roc_auc': 0.7979637327549851, 'pr_auc': 0.765070343206189, 'conicity_mean': '0.17320225', 'conicity_std': '0.04022508'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.726    0.750      0.738      0.738         0.738
precision    0.757    0.723      0.738      0.740         0.740
recall       0.697    0.779      0.738      0.738         0.738
support    495.000  503.000    998.000    998.000       998.000
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:23:13,190 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:23:13,190 - type = ortholstm
INFO - 2021-01-17 19:23:13,190 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:23:13,191 - vocab_size = 604
INFO - 2021-01-17 19:23:13,191 - embed_size = 200
INFO - 2021-01-17 19:23:13,191 - hidden_size = 128
INFO - 2021-01-17 19:23:13,191 - pre_embed = None
INFO - 2021-01-17 19:23:13,205 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:23:13,205 - hidden_size = 256
INFO - 2021-01-17 19:23:13,205 - output_size = 1
INFO - 2021-01-17 19:23:13,205 - use_attention = True
INFO - 2021-01-17 19:23:13,206 - regularizer_attention = None
INFO - 2021-01-17 19:23:13,206 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ac6eff38250> and extras set()
INFO - 2021-01-17 19:23:13,206 - attention.type = tanh
INFO - 2021-01-17 19:23:13,206 - type = tanh
INFO - 2021-01-17 19:23:13,206 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ac6eff38250> and extras set()
INFO - 2021-01-17 19:23:13,206 - attention.hidden_size = 256
INFO - 2021-01-17 19:23:13,206 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7384769539078156, 'roc_auc': 0.7979637327549851, 'pr_auc': 0.765070343206189, 'conicity_mean': '0.17320225', 'conicity_std': '0.04022508'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.726    0.750      0.738      0.738         0.738
precision    0.757    0.723      0.738      0.740         0.740
recall       0.697    0.779      0.738      0.738         0.738
support    495.000  503.000    998.000    998.000       998.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_fr diversity_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:23:26,207 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:23:26,207 - type = vanillalstm
INFO - 2021-01-17 19:23:26,207 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:23:26,223 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:23:26,544 - vocab_size = 604
INFO - 2021-01-17 19:23:26,544 - embed_size = 200
INFO - 2021-01-17 19:23:26,544 - hidden_size = 128
INFO - 2021-01-17 19:23:26,544 - pre_embed = None
INFO - 2021-01-17 19:23:29,608 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:23:29,608 - hidden_size = 256
INFO - 2021-01-17 19:23:29,608 - output_size = 1
INFO - 2021-01-17 19:23:29,609 - use_attention = True
INFO - 2021-01-17 19:23:29,609 - regularizer_attention = None
INFO - 2021-01-17 19:23:29,609 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b7b43ea8410> and extras set()
INFO - 2021-01-17 19:23:29,609 - attention.type = tanh
INFO - 2021-01-17 19:23:29,609 - type = tanh
INFO - 2021-01-17 19:23:29,609 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b7b43ea8410> and extras set()
INFO - 2021-01-17 19:23:29,609 - attention.hidden_size = 256
INFO - 2021-01-17 19:23:29,609 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.943, BCE loss: 0.697, Diversity Loss: 0.493                     (Diversity_weight = 0.5)
Epoch: 0 Step: 1 Total Loss: 0.958, BCE loss: 0.705, Diversity Loss: 0.507                     (Diversity_weight = 0.5)
Epoch: 0 Step: 2 Total Loss: 0.908, BCE loss: 0.697, Diversity Loss: 0.422                     (Diversity_weight = 0.5)
Epoch: 0 Step: 3 Total Loss: 0.879, BCE loss: 0.690, Diversity Loss: 0.377                     (Diversity_weight = 0.5)
Epoch: 0 Step: 4 Total Loss: 0.868, BCE loss: 0.691, Diversity Loss: 0.355                     (Diversity_weight = 0.5)
Epoch: 0 Step: 5 Total Loss: 0.852, BCE loss: 0.683, Diversity Loss: 0.338                     (Diversity_weight = 0.5)
Epoch: 0 Step: 6 Total Loss: 0.877, BCE loss: 0.697, Diversity Loss: 0.361                     (Diversity_weight = 0.5)
Epoch: 0 Step: 7 Total Loss: 0.817, BCE loss: 0.691, Diversity Loss: 0.252                     (Diversity_weight = 0.5)
Epoch: 0 Step: 8 Total Loss: 0.808, BCE loss: 0.688, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 0 Step: 9 Total Loss: 0.836, BCE loss: 0.688, Diversity Loss: 0.296                     (Diversity_weight = 0.5)
Epoch: 0 Step: 10 Total Loss: 0.807, BCE loss: 0.690, Diversity Loss: 0.233                     (Diversity_weight = 0.5)
Epoch: 0 Step: 11 Total Loss: 0.823, BCE loss: 0.685, Diversity Loss: 0.276                     (Diversity_weight = 0.5)
Epoch: 0 Step: 12 Total Loss: 0.790, BCE loss: 0.692, Diversity Loss: 0.196                     (Diversity_weight = 0.5)
Epoch: 0 Step: 13 Total Loss: 0.797, BCE loss: 0.688, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 0 Step: 14 Total Loss: 0.791, BCE loss: 0.690, Diversity Loss: 0.203                     (Diversity_weight = 0.5)
Epoch: 0 Step: 15 Total Loss: 0.790, BCE loss: 0.693, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 0 Step: 16 Total Loss: 0.788, BCE loss: 0.684, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 0 Step: 17 Total Loss: 0.781, BCE loss: 0.685, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 0 Step: 18 Total Loss: 0.900, BCE loss: 0.706, Diversity Loss: 0.387                     (Diversity_weight = 0.5)
Epoch: 0 Step: 19 Total Loss: 0.801, BCE loss: 0.683, Diversity Loss: 0.235                     (Diversity_weight = 0.5)
Epoch: 0 Step: 20 Total Loss: 0.768, BCE loss: 0.685, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 0 Step: 21 Total Loss: 0.851, BCE loss: 0.669, Diversity Loss: 0.364                     (Diversity_weight = 0.5)
Epoch: 0 Step: 22 Total Loss: 0.795, BCE loss: 0.691, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 0 Step: 23 Total Loss: 0.770, BCE loss: 0.690, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 0 Step: 24 Total Loss: 0.882, BCE loss: 0.680, Diversity Loss: 0.404                     (Diversity_weight = 0.5)
Epoch: 0 Step: 25 Total Loss: 0.811, BCE loss: 0.691, Diversity Loss: 0.240                     (Diversity_weight = 0.5)
Epoch: 0 Step: 26 Total Loss: 0.761, BCE loss: 0.686, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 0 Step: 27 Total Loss: 0.811, BCE loss: 0.676, Diversity Loss: 0.270                     (Diversity_weight = 0.5)
Epoch: 0 Step: 28 Total Loss: 0.765, BCE loss: 0.697, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 0 Step: 29 Total Loss: 0.755, BCE loss: 0.684, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 0 Step: 30 Total Loss: 0.759, BCE loss: 0.686, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 0 Step: 31 Total Loss: 0.800, BCE loss: 0.646, Diversity Loss: 0.309                     (Diversity_weight = 0.5)
Epoch: 0 Step: 32 Total Loss: 0.772, BCE loss: 0.685, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 0 Step: 33 Total Loss: 0.757, BCE loss: 0.677, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 0 Step: 34 Total Loss: 0.752, BCE loss: 0.662, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 0 Step: 35 Total Loss: 0.758, BCE loss: 0.684, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 0 Step: 36 Total Loss: 0.777, BCE loss: 0.678, Diversity Loss: 0.198                     (Diversity_weight = 0.5)
Epoch: 0 Step: 37 Total Loss: 0.795, BCE loss: 0.681, Diversity Loss: 0.228                     (Diversity_weight = 0.5)
Epoch: 0 Step: 38 Total Loss: 0.827, BCE loss: 0.719, Diversity Loss: 0.217                     (Diversity_weight = 0.5)
Epoch: 0 Step: 39 Total Loss: 0.752, BCE loss: 0.684, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 0 Step: 40 Total Loss: 0.827, BCE loss: 0.719, Diversity Loss: 0.217                     (Diversity_weight = 0.5)
Epoch: 0 Step: 41 Total Loss: 0.748, BCE loss: 0.685, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 0 Step: 42 Total Loss: 0.752, BCE loss: 0.686, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 0 Step: 43 Total Loss: 0.743, BCE loss: 0.668, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 0 Step: 44 Total Loss: 0.749, BCE loss: 0.674, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 0 Step: 45 Total Loss: 0.747, BCE loss: 0.678, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 0 Step: 46 Total Loss: 0.805, BCE loss: 0.677, Diversity Loss: 0.255                     (Diversity_weight = 0.5)
Epoch: 0 Step: 47 Total Loss: 0.751, BCE loss: 0.661, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 0 Step: 48 Total Loss: 0.747, BCE loss: 0.658, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 0 Step: 49 Total Loss: 0.764, BCE loss: 0.695, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 0 Step: 50 Total Loss: 0.750, BCE loss: 0.683, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 0 Step: 51 Total Loss: 0.750, BCE loss: 0.667, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 0 Step: 52 Total Loss: 0.778, BCE loss: 0.677, Diversity Loss: 0.202                     (Diversity_weight = 0.5)
Epoch: 0 Step: 53 Total Loss: 0.730, BCE loss: 0.665, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 0 Step: 54 Total Loss: 0.731, BCE loss: 0.673, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 0 Step: 55 Total Loss: 0.766, BCE loss: 0.674, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 0 Step: 56 Total Loss: 0.726, BCE loss: 0.663, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 0 Step: 57 Total Loss: 0.808, BCE loss: 0.719, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 0 Step: 58 Total Loss: 0.742, BCE loss: 0.670, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 0 Step: 59 Total Loss: 0.769, BCE loss: 0.676, Diversity Loss: 0.186                     (Diversity_weight = 0.5)
Epoch: 0 Step: 60 Total Loss: 0.753, BCE loss: 0.690, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 0 Step: 61 Total Loss: 0.761, BCE loss: 0.679, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 0 Step: 62 Total Loss: 0.754, BCE loss: 0.667, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
{'accuracy': 0.6432865731462926, 'roc_auc': 0.7055123802638712, 'pr_auc': 0.674340135581385, 'conicity_mean': 0.16280364, 'conicity_std': 0.052282445}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.678    0.600      0.643      0.639         0.639
precision    0.622    0.676      0.643      0.649         0.649
recall       0.746    0.539      0.643      0.642         0.643
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7055123802638712
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:23:29_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.703, BCE loss: 0.648, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 1 Step: 1 Total Loss: 0.713, BCE loss: 0.624, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 1 Step: 2 Total Loss: 0.726, BCE loss: 0.647, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 1 Step: 3 Total Loss: 0.786, BCE loss: 0.626, Diversity Loss: 0.320                     (Diversity_weight = 0.5)
Epoch: 1 Step: 4 Total Loss: 0.787, BCE loss: 0.650, Diversity Loss: 0.273                     (Diversity_weight = 0.5)
Epoch: 1 Step: 5 Total Loss: 0.708, BCE loss: 0.597, Diversity Loss: 0.222                     (Diversity_weight = 0.5)
Epoch: 1 Step: 6 Total Loss: 0.685, BCE loss: 0.595, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 1 Step: 7 Total Loss: 0.695, BCE loss: 0.598, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 1 Step: 8 Total Loss: 0.834, BCE loss: 0.768, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 1 Step: 9 Total Loss: 0.719, BCE loss: 0.586, Diversity Loss: 0.267                     (Diversity_weight = 0.5)
Epoch: 1 Step: 10 Total Loss: 0.742, BCE loss: 0.680, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 11 Total Loss: 0.684, BCE loss: 0.614, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 1 Step: 12 Total Loss: 0.726, BCE loss: 0.635, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 1 Step: 13 Total Loss: 0.765, BCE loss: 0.656, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 1 Step: 14 Total Loss: 0.723, BCE loss: 0.650, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 1 Step: 15 Total Loss: 0.847, BCE loss: 0.764, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 1 Step: 16 Total Loss: 0.725, BCE loss: 0.640, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 1 Step: 17 Total Loss: 0.707, BCE loss: 0.618, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 1 Step: 18 Total Loss: 0.675, BCE loss: 0.612, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 1 Step: 19 Total Loss: 0.669, BCE loss: 0.601, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 1 Step: 20 Total Loss: 0.691, BCE loss: 0.623, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 1 Step: 21 Total Loss: 0.745, BCE loss: 0.682, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 1 Step: 22 Total Loss: 0.781, BCE loss: 0.707, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 1 Step: 23 Total Loss: 0.635, BCE loss: 0.555, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 1 Step: 24 Total Loss: 0.702, BCE loss: 0.640, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 1 Step: 25 Total Loss: 0.709, BCE loss: 0.621, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 1 Step: 26 Total Loss: 0.707, BCE loss: 0.644, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 1 Step: 27 Total Loss: 0.693, BCE loss: 0.638, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 1 Step: 28 Total Loss: 0.804, BCE loss: 0.715, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 1 Step: 29 Total Loss: 0.737, BCE loss: 0.670, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 1 Step: 30 Total Loss: 0.761, BCE loss: 0.694, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 1 Step: 31 Total Loss: 0.744, BCE loss: 0.690, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 1 Step: 32 Total Loss: 0.720, BCE loss: 0.636, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 1 Step: 33 Total Loss: 0.713, BCE loss: 0.654, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 1 Step: 34 Total Loss: 0.757, BCE loss: 0.673, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 1 Step: 35 Total Loss: 0.650, BCE loss: 0.577, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 1 Step: 36 Total Loss: 0.635, BCE loss: 0.571, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 1 Step: 37 Total Loss: 0.651, BCE loss: 0.570, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 1 Step: 38 Total Loss: 0.766, BCE loss: 0.697, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 1 Step: 39 Total Loss: 0.687, BCE loss: 0.629, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 1 Step: 40 Total Loss: 0.724, BCE loss: 0.630, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 1 Step: 41 Total Loss: 0.698, BCE loss: 0.619, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 1 Step: 42 Total Loss: 0.669, BCE loss: 0.616, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 1 Step: 43 Total Loss: 0.663, BCE loss: 0.595, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 1 Step: 44 Total Loss: 0.758, BCE loss: 0.667, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 1 Step: 45 Total Loss: 0.636, BCE loss: 0.578, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 1 Step: 46 Total Loss: 0.666, BCE loss: 0.553, Diversity Loss: 0.227                     (Diversity_weight = 0.5)
Epoch: 1 Step: 47 Total Loss: 0.661, BCE loss: 0.605, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 1 Step: 48 Total Loss: 0.719, BCE loss: 0.655, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 1 Step: 49 Total Loss: 0.657, BCE loss: 0.597, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 1 Step: 50 Total Loss: 0.628, BCE loss: 0.558, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 1 Step: 51 Total Loss: 0.702, BCE loss: 0.652, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 1 Step: 52 Total Loss: 0.810, BCE loss: 0.745, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 1 Step: 53 Total Loss: 0.552, BCE loss: 0.495, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 1 Step: 54 Total Loss: 0.814, BCE loss: 0.722, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 1 Step: 55 Total Loss: 0.668, BCE loss: 0.569, Diversity Loss: 0.197                     (Diversity_weight = 0.5)
Epoch: 1 Step: 56 Total Loss: 0.701, BCE loss: 0.626, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 1 Step: 57 Total Loss: 0.866, BCE loss: 0.769, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 1 Step: 58 Total Loss: 0.803, BCE loss: 0.742, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 1 Step: 59 Total Loss: 0.656, BCE loss: 0.580, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 1 Step: 60 Total Loss: 0.847, BCE loss: 0.778, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 1 Step: 61 Total Loss: 0.731, BCE loss: 0.659, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 1 Step: 62 Total Loss: 0.710, BCE loss: 0.583, Diversity Loss: 0.253                     (Diversity_weight = 0.5)
{'accuracy': 0.6302605210420842, 'roc_auc': 0.6880494808924232, 'pr_auc': 0.6398617678032567, 'conicity_mean': 0.16260952, 'conicity_std': 0.05119436}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.658    0.598       0.63      0.628         0.628
precision    0.616    0.649       0.63      0.633         0.633
recall       0.706    0.554       0.63      0.630         0.630
support    503.000  495.000     998.00    998.000       998.000
Model not saved on  roc_auc 0.6880494808924232
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:23:29_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.583, BCE loss: 0.524, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 2 Step: 1 Total Loss: 0.688, BCE loss: 0.613, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 2 Step: 2 Total Loss: 0.600, BCE loss: 0.535, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 2 Step: 3 Total Loss: 0.544, BCE loss: 0.481, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 2 Step: 4 Total Loss: 0.676, BCE loss: 0.604, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 2 Step: 5 Total Loss: 0.660, BCE loss: 0.576, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 2 Step: 6 Total Loss: 0.672, BCE loss: 0.622, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 2 Step: 7 Total Loss: 0.552, BCE loss: 0.483, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 2 Step: 8 Total Loss: 0.763, BCE loss: 0.694, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 2 Step: 9 Total Loss: 0.733, BCE loss: 0.638, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 2 Step: 10 Total Loss: 0.520, BCE loss: 0.463, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 2 Step: 11 Total Loss: 0.653, BCE loss: 0.532, Diversity Loss: 0.244                     (Diversity_weight = 0.5)
Epoch: 2 Step: 12 Total Loss: 0.615, BCE loss: 0.529, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 2 Step: 13 Total Loss: 0.648, BCE loss: 0.506, Diversity Loss: 0.285                     (Diversity_weight = 0.5)
Epoch: 2 Step: 14 Total Loss: 0.619, BCE loss: 0.537, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 2 Step: 15 Total Loss: 0.746, BCE loss: 0.651, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 2 Step: 16 Total Loss: 0.541, BCE loss: 0.462, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 2 Step: 17 Total Loss: 0.578, BCE loss: 0.515, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 2 Step: 18 Total Loss: 0.600, BCE loss: 0.513, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 2 Step: 19 Total Loss: 0.534, BCE loss: 0.466, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 2 Step: 20 Total Loss: 0.611, BCE loss: 0.551, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 2 Step: 21 Total Loss: 0.558, BCE loss: 0.492, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 2 Step: 22 Total Loss: 0.580, BCE loss: 0.513, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 2 Step: 23 Total Loss: 0.603, BCE loss: 0.520, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 2 Step: 24 Total Loss: 0.650, BCE loss: 0.596, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 2 Step: 25 Total Loss: 0.654, BCE loss: 0.578, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 2 Step: 26 Total Loss: 0.727, BCE loss: 0.631, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 2 Step: 27 Total Loss: 0.548, BCE loss: 0.439, Diversity Loss: 0.219                     (Diversity_weight = 0.5)
Epoch: 2 Step: 28 Total Loss: 0.635, BCE loss: 0.557, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 2 Step: 29 Total Loss: 0.604, BCE loss: 0.526, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 2 Step: 30 Total Loss: 0.569, BCE loss: 0.510, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 2 Step: 31 Total Loss: 0.604, BCE loss: 0.533, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 2 Step: 32 Total Loss: 0.575, BCE loss: 0.513, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 2 Step: 33 Total Loss: 0.755, BCE loss: 0.654, Diversity Loss: 0.202                     (Diversity_weight = 0.5)
Epoch: 2 Step: 34 Total Loss: 0.712, BCE loss: 0.657, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 2 Step: 35 Total Loss: 0.592, BCE loss: 0.513, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 2 Step: 36 Total Loss: 0.623, BCE loss: 0.562, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 2 Step: 37 Total Loss: 0.725, BCE loss: 0.610, Diversity Loss: 0.229                     (Diversity_weight = 0.5)
Epoch: 2 Step: 38 Total Loss: 0.509, BCE loss: 0.430, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 2 Step: 39 Total Loss: 0.729, BCE loss: 0.642, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 2 Step: 40 Total Loss: 0.595, BCE loss: 0.538, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 2 Step: 41 Total Loss: 0.534, BCE loss: 0.461, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 2 Step: 42 Total Loss: 0.698, BCE loss: 0.611, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 2 Step: 43 Total Loss: 0.570, BCE loss: 0.449, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 2 Step: 44 Total Loss: 0.608, BCE loss: 0.550, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 2 Step: 45 Total Loss: 0.627, BCE loss: 0.551, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 2 Step: 46 Total Loss: 0.634, BCE loss: 0.564, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 2 Step: 47 Total Loss: 0.777, BCE loss: 0.703, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 2 Step: 48 Total Loss: 0.604, BCE loss: 0.541, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 2 Step: 49 Total Loss: 0.643, BCE loss: 0.594, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 2 Step: 50 Total Loss: 0.655, BCE loss: 0.590, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 2 Step: 51 Total Loss: 0.723, BCE loss: 0.608, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 2 Step: 52 Total Loss: 0.589, BCE loss: 0.531, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 2 Step: 53 Total Loss: 0.569, BCE loss: 0.514, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 2 Step: 54 Total Loss: 0.604, BCE loss: 0.514, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 2 Step: 55 Total Loss: 0.702, BCE loss: 0.633, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 2 Step: 56 Total Loss: 0.812, BCE loss: 0.727, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 2 Step: 57 Total Loss: 0.645, BCE loss: 0.587, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 2 Step: 58 Total Loss: 0.517, BCE loss: 0.464, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 2 Step: 59 Total Loss: 0.519, BCE loss: 0.439, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 2 Step: 60 Total Loss: 0.513, BCE loss: 0.453, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 2 Step: 61 Total Loss: 0.707, BCE loss: 0.643, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 2 Step: 62 Total Loss: 0.507, BCE loss: 0.442, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
{'accuracy': 0.7004008016032064, 'roc_auc': 0.7564632407574753, 'pr_auc': 0.6990319576549622, 'conicity_mean': 0.14777724, 'conicity_std': 0.04800373}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.684    0.716        0.7      0.700         0.699
precision    0.731    0.676        0.7      0.704         0.704
recall       0.642    0.760        0.7      0.701         0.700
support    503.000  495.000      998.0    998.000       998.000
Model Saved on  roc_auc 0.7564632407574753
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:23:29_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.609, BCE loss: 0.536, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 3 Step: 1 Total Loss: 0.565, BCE loss: 0.427, Diversity Loss: 0.276                     (Diversity_weight = 0.5)
Epoch: 3 Step: 2 Total Loss: 0.499, BCE loss: 0.431, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 3 Step: 3 Total Loss: 0.432, BCE loss: 0.378, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 3 Step: 4 Total Loss: 0.684, BCE loss: 0.592, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 3 Step: 5 Total Loss: 0.434, BCE loss: 0.374, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 3 Step: 6 Total Loss: 0.570, BCE loss: 0.517, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 3 Step: 7 Total Loss: 0.389, BCE loss: 0.315, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 3 Step: 8 Total Loss: 0.466, BCE loss: 0.355, Diversity Loss: 0.223                     (Diversity_weight = 0.5)
Epoch: 3 Step: 9 Total Loss: 0.624, BCE loss: 0.560, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 3 Step: 10 Total Loss: 0.482, BCE loss: 0.420, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 3 Step: 11 Total Loss: 0.446, BCE loss: 0.386, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 3 Step: 12 Total Loss: 0.603, BCE loss: 0.540, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 3 Step: 13 Total Loss: 0.556, BCE loss: 0.486, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 3 Step: 14 Total Loss: 0.378, BCE loss: 0.325, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 3 Step: 15 Total Loss: 0.584, BCE loss: 0.498, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 3 Step: 16 Total Loss: 0.392, BCE loss: 0.318, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 17 Total Loss: 0.449, BCE loss: 0.324, Diversity Loss: 0.250                     (Diversity_weight = 0.5)
Epoch: 3 Step: 18 Total Loss: 0.378, BCE loss: 0.324, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 3 Step: 19 Total Loss: 0.416, BCE loss: 0.334, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 3 Step: 20 Total Loss: 0.557, BCE loss: 0.503, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 3 Step: 21 Total Loss: 0.461, BCE loss: 0.373, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 3 Step: 22 Total Loss: 0.755, BCE loss: 0.652, Diversity Loss: 0.206                     (Diversity_weight = 0.5)
Epoch: 3 Step: 23 Total Loss: 0.867, BCE loss: 0.779, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 3 Step: 24 Total Loss: 0.468, BCE loss: 0.391, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 3 Step: 25 Total Loss: 0.546, BCE loss: 0.447, Diversity Loss: 0.198                     (Diversity_weight = 0.5)
Epoch: 3 Step: 26 Total Loss: 0.537, BCE loss: 0.439, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 3 Step: 27 Total Loss: 0.524, BCE loss: 0.437, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 3 Step: 28 Total Loss: 0.578, BCE loss: 0.505, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 3 Step: 29 Total Loss: 0.606, BCE loss: 0.539, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 3 Step: 30 Total Loss: 0.496, BCE loss: 0.408, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 3 Step: 31 Total Loss: 0.557, BCE loss: 0.490, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 3 Step: 32 Total Loss: 0.564, BCE loss: 0.497, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 3 Step: 33 Total Loss: 0.567, BCE loss: 0.511, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 3 Step: 34 Total Loss: 0.520, BCE loss: 0.459, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 3 Step: 35 Total Loss: 0.611, BCE loss: 0.530, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 3 Step: 36 Total Loss: 0.489, BCE loss: 0.439, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 3 Step: 37 Total Loss: 0.456, BCE loss: 0.393, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 3 Step: 38 Total Loss: 0.326, BCE loss: 0.273, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 3 Step: 39 Total Loss: 0.493, BCE loss: 0.439, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 3 Step: 40 Total Loss: 0.489, BCE loss: 0.370, Diversity Loss: 0.238                     (Diversity_weight = 0.5)
Epoch: 3 Step: 41 Total Loss: 0.584, BCE loss: 0.508, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 3 Step: 42 Total Loss: 0.589, BCE loss: 0.499, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 3 Step: 43 Total Loss: 0.486, BCE loss: 0.413, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 3 Step: 44 Total Loss: 0.495, BCE loss: 0.396, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 3 Step: 45 Total Loss: 0.664, BCE loss: 0.578, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 3 Step: 46 Total Loss: 0.553, BCE loss: 0.483, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 3 Step: 47 Total Loss: 0.479, BCE loss: 0.425, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 3 Step: 48 Total Loss: 0.397, BCE loss: 0.335, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 3 Step: 49 Total Loss: 0.731, BCE loss: 0.618, Diversity Loss: 0.226                     (Diversity_weight = 0.5)
Epoch: 3 Step: 50 Total Loss: 0.493, BCE loss: 0.444, Diversity Loss: 0.098                     (Diversity_weight = 0.5)
Epoch: 3 Step: 51 Total Loss: 0.460, BCE loss: 0.405, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 3 Step: 52 Total Loss: 0.476, BCE loss: 0.419, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 3 Step: 53 Total Loss: 0.481, BCE loss: 0.431, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 3 Step: 54 Total Loss: 0.533, BCE loss: 0.479, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 3 Step: 55 Total Loss: 0.492, BCE loss: 0.420, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 3 Step: 56 Total Loss: 0.423, BCE loss: 0.348, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 57 Total Loss: 0.462, BCE loss: 0.387, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 3 Step: 58 Total Loss: 0.400, BCE loss: 0.343, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 3 Step: 59 Total Loss: 0.445, BCE loss: 0.391, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 3 Step: 60 Total Loss: 0.449, BCE loss: 0.400, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 3 Step: 61 Total Loss: 0.467, BCE loss: 0.405, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 3 Step: 62 Total Loss: 0.484, BCE loss: 0.401, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
{'accuracy': 0.7004008016032064, 'roc_auc': 0.7604474165110348, 'pr_auc': 0.7216632985386716, 'conicity_mean': 0.14494447, 'conicity_std': 0.04785497}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.708    0.693        0.7      0.700         0.700
precision    0.696    0.705        0.7      0.701         0.701
recall       0.720    0.681        0.7      0.700         0.700
support    503.000  495.000      998.0    998.000       998.000
Model Saved on  roc_auc 0.7604474165110348
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:23:29_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.346, BCE loss: 0.290, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 4 Step: 1 Total Loss: 0.490, BCE loss: 0.404, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 4 Step: 2 Total Loss: 0.396, BCE loss: 0.322, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 4 Step: 3 Total Loss: 0.413, BCE loss: 0.354, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 4 Step: 4 Total Loss: 0.388, BCE loss: 0.312, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 4 Step: 5 Total Loss: 0.557, BCE loss: 0.487, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 4 Step: 6 Total Loss: 0.322, BCE loss: 0.247, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 4 Step: 7 Total Loss: 0.360, BCE loss: 0.287, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 4 Step: 8 Total Loss: 0.475, BCE loss: 0.342, Diversity Loss: 0.265                     (Diversity_weight = 0.5)
Epoch: 4 Step: 9 Total Loss: 0.362, BCE loss: 0.307, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 4 Step: 10 Total Loss: 0.415, BCE loss: 0.348, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 4 Step: 11 Total Loss: 0.473, BCE loss: 0.414, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 4 Step: 12 Total Loss: 0.423, BCE loss: 0.359, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 4 Step: 13 Total Loss: 0.340, BCE loss: 0.262, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 4 Step: 14 Total Loss: 0.473, BCE loss: 0.391, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 4 Step: 15 Total Loss: 0.270, BCE loss: 0.220, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 4 Step: 16 Total Loss: 0.446, BCE loss: 0.392, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 4 Step: 17 Total Loss: 0.374, BCE loss: 0.312, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 4 Step: 18 Total Loss: 0.267, BCE loss: 0.202, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 4 Step: 19 Total Loss: 0.435, BCE loss: 0.318, Diversity Loss: 0.235                     (Diversity_weight = 0.5)
Epoch: 4 Step: 20 Total Loss: 0.496, BCE loss: 0.408, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 4 Step: 21 Total Loss: 0.305, BCE loss: 0.249, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 4 Step: 22 Total Loss: 0.501, BCE loss: 0.431, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 4 Step: 23 Total Loss: 0.276, BCE loss: 0.218, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 4 Step: 24 Total Loss: 0.370, BCE loss: 0.307, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 4 Step: 25 Total Loss: 0.581, BCE loss: 0.518, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 4 Step: 26 Total Loss: 0.468, BCE loss: 0.346, Diversity Loss: 0.243                     (Diversity_weight = 0.5)
Epoch: 4 Step: 27 Total Loss: 0.515, BCE loss: 0.468, Diversity Loss: 0.095                     (Diversity_weight = 0.5)
Epoch: 4 Step: 28 Total Loss: 0.475, BCE loss: 0.423, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 4 Step: 29 Total Loss: 0.436, BCE loss: 0.356, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 4 Step: 30 Total Loss: 0.497, BCE loss: 0.377, Diversity Loss: 0.242                     (Diversity_weight = 0.5)
Epoch: 4 Step: 31 Total Loss: 0.477, BCE loss: 0.381, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 4 Step: 32 Total Loss: 0.273, BCE loss: 0.184, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 4 Step: 33 Total Loss: 0.329, BCE loss: 0.250, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 4 Step: 34 Total Loss: 0.279, BCE loss: 0.229, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 4 Step: 35 Total Loss: 0.668, BCE loss: 0.599, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 4 Step: 36 Total Loss: 0.337, BCE loss: 0.237, Diversity Loss: 0.201                     (Diversity_weight = 0.5)
Epoch: 4 Step: 37 Total Loss: 0.419, BCE loss: 0.369, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 4 Step: 38 Total Loss: 0.419, BCE loss: 0.345, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 4 Step: 39 Total Loss: 0.341, BCE loss: 0.280, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 4 Step: 40 Total Loss: 0.605, BCE loss: 0.534, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 4 Step: 41 Total Loss: 0.285, BCE loss: 0.232, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 4 Step: 42 Total Loss: 0.502, BCE loss: 0.414, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 4 Step: 43 Total Loss: 0.391, BCE loss: 0.334, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 4 Step: 44 Total Loss: 0.447, BCE loss: 0.385, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 4 Step: 45 Total Loss: 0.265, BCE loss: 0.201, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 4 Step: 46 Total Loss: 0.322, BCE loss: 0.272, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 4 Step: 47 Total Loss: 0.310, BCE loss: 0.257, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 4 Step: 48 Total Loss: 0.534, BCE loss: 0.482, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 4 Step: 49 Total Loss: 0.454, BCE loss: 0.404, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 4 Step: 50 Total Loss: 0.477, BCE loss: 0.400, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 4 Step: 51 Total Loss: 0.487, BCE loss: 0.405, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 4 Step: 52 Total Loss: 0.392, BCE loss: 0.291, Diversity Loss: 0.203                     (Diversity_weight = 0.5)
Epoch: 4 Step: 53 Total Loss: 0.355, BCE loss: 0.297, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 4 Step: 54 Total Loss: 0.413, BCE loss: 0.349, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 4 Step: 55 Total Loss: 0.306, BCE loss: 0.229, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 4 Step: 56 Total Loss: 0.450, BCE loss: 0.384, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 4 Step: 57 Total Loss: 0.407, BCE loss: 0.330, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 4 Step: 58 Total Loss: 0.458, BCE loss: 0.360, Diversity Loss: 0.195                     (Diversity_weight = 0.5)
Epoch: 4 Step: 59 Total Loss: 0.689, BCE loss: 0.599, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 4 Step: 60 Total Loss: 0.444, BCE loss: 0.385, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 4 Step: 61 Total Loss: 0.329, BCE loss: 0.247, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 4 Step: 62 Total Loss: 0.549, BCE loss: 0.490, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
{'accuracy': 0.7114228456913828, 'roc_auc': 0.7650621523384943, 'pr_auc': 0.7205137331211866, 'conicity_mean': 0.14399791, 'conicity_std': 0.047823448}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.707    0.716      0.711      0.711         0.711
precision    0.724    0.699      0.711      0.712         0.712
recall       0.690    0.733      0.711      0.712         0.711
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7650621523384943
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:23:29_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.313, BCE loss: 0.229, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 5 Step: 1 Total Loss: 0.243, BCE loss: 0.177, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 5 Step: 2 Total Loss: 0.412, BCE loss: 0.331, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 5 Step: 3 Total Loss: 0.216, BCE loss: 0.145, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 5 Step: 4 Total Loss: 0.323, BCE loss: 0.265, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 5 Step: 5 Total Loss: 0.457, BCE loss: 0.400, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 5 Step: 6 Total Loss: 0.371, BCE loss: 0.268, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 5 Step: 7 Total Loss: 0.273, BCE loss: 0.197, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 5 Step: 8 Total Loss: 0.356, BCE loss: 0.280, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 5 Step: 9 Total Loss: 0.305, BCE loss: 0.227, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 5 Step: 10 Total Loss: 0.257, BCE loss: 0.184, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 5 Step: 11 Total Loss: 0.286, BCE loss: 0.232, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 5 Step: 12 Total Loss: 0.321, BCE loss: 0.265, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 5 Step: 13 Total Loss: 0.338, BCE loss: 0.247, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 5 Step: 14 Total Loss: 0.240, BCE loss: 0.152, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 5 Step: 15 Total Loss: 0.554, BCE loss: 0.436, Diversity Loss: 0.237                     (Diversity_weight = 0.5)
Epoch: 5 Step: 16 Total Loss: 0.272, BCE loss: 0.216, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 5 Step: 17 Total Loss: 0.420, BCE loss: 0.324, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 5 Step: 18 Total Loss: 0.337, BCE loss: 0.270, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 5 Step: 19 Total Loss: 0.254, BCE loss: 0.184, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 5 Step: 20 Total Loss: 0.275, BCE loss: 0.211, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 5 Step: 21 Total Loss: 0.152, BCE loss: 0.096, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 5 Step: 22 Total Loss: 0.364, BCE loss: 0.288, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 5 Step: 23 Total Loss: 0.186, BCE loss: 0.118, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 5 Step: 24 Total Loss: 0.398, BCE loss: 0.325, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 5 Step: 25 Total Loss: 0.462, BCE loss: 0.405, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 5 Step: 26 Total Loss: 0.333, BCE loss: 0.281, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 5 Step: 27 Total Loss: 0.291, BCE loss: 0.199, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 5 Step: 28 Total Loss: 0.185, BCE loss: 0.137, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 5 Step: 29 Total Loss: 0.466, BCE loss: 0.378, Diversity Loss: 0.175                     (Diversity_weight = 0.5)
Epoch: 5 Step: 30 Total Loss: 0.298, BCE loss: 0.237, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 5 Step: 31 Total Loss: 0.222, BCE loss: 0.169, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 5 Step: 32 Total Loss: 0.338, BCE loss: 0.221, Diversity Loss: 0.233                     (Diversity_weight = 0.5)
Epoch: 5 Step: 33 Total Loss: 0.471, BCE loss: 0.399, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 5 Step: 34 Total Loss: 0.216, BCE loss: 0.166, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 5 Step: 35 Total Loss: 0.272, BCE loss: 0.211, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 5 Step: 36 Total Loss: 0.289, BCE loss: 0.223, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 5 Step: 37 Total Loss: 0.122, BCE loss: 0.064, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 5 Step: 38 Total Loss: 0.227, BCE loss: 0.164, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 5 Step: 39 Total Loss: 0.274, BCE loss: 0.210, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 5 Step: 40 Total Loss: 0.240, BCE loss: 0.164, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 5 Step: 41 Total Loss: 0.228, BCE loss: 0.177, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 5 Step: 42 Total Loss: 0.350, BCE loss: 0.296, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 5 Step: 43 Total Loss: 0.271, BCE loss: 0.221, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 5 Step: 44 Total Loss: 0.221, BCE loss: 0.170, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 5 Step: 45 Total Loss: 0.317, BCE loss: 0.264, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 5 Step: 46 Total Loss: 0.244, BCE loss: 0.185, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 5 Step: 47 Total Loss: 0.421, BCE loss: 0.285, Diversity Loss: 0.271                     (Diversity_weight = 0.5)
Epoch: 5 Step: 48 Total Loss: 0.290, BCE loss: 0.211, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 5 Step: 49 Total Loss: 0.509, BCE loss: 0.433, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 5 Step: 50 Total Loss: 0.579, BCE loss: 0.511, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 5 Step: 51 Total Loss: 0.412, BCE loss: 0.341, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 5 Step: 52 Total Loss: 0.514, BCE loss: 0.450, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 5 Step: 53 Total Loss: 0.440, BCE loss: 0.336, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 5 Step: 54 Total Loss: 0.193, BCE loss: 0.135, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 5 Step: 55 Total Loss: 0.301, BCE loss: 0.188, Diversity Loss: 0.226                     (Diversity_weight = 0.5)
Epoch: 5 Step: 56 Total Loss: 0.284, BCE loss: 0.234, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 5 Step: 57 Total Loss: 0.391, BCE loss: 0.301, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 5 Step: 58 Total Loss: 0.351, BCE loss: 0.272, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 5 Step: 59 Total Loss: 0.334, BCE loss: 0.287, Diversity Loss: 0.094                     (Diversity_weight = 0.5)
Epoch: 5 Step: 60 Total Loss: 0.442, BCE loss: 0.354, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 5 Step: 61 Total Loss: 0.281, BCE loss: 0.207, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 5 Step: 62 Total Loss: 0.553, BCE loss: 0.466, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
{'accuracy': 0.717434869739479, 'roc_auc': 0.7751069341526597, 'pr_auc': 0.7296853308414671, 'conicity_mean': 0.1401282, 'conicity_std': 0.04628976}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.706    0.728      0.717      0.717         0.717
precision    0.742    0.697      0.717      0.719         0.720
recall       0.674    0.762      0.717      0.718         0.717
support    503.000  495.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7751069341526597
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:23:29_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.174, BCE loss: 0.123, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 6 Step: 1 Total Loss: 0.131, BCE loss: 0.090, Diversity Loss: 0.082                     (Diversity_weight = 0.5)
Epoch: 6 Step: 2 Total Loss: 0.262, BCE loss: 0.180, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 6 Step: 3 Total Loss: 0.333, BCE loss: 0.250, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 6 Step: 4 Total Loss: 0.269, BCE loss: 0.218, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 6 Step: 5 Total Loss: 0.263, BCE loss: 0.204, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 6 Step: 6 Total Loss: 0.331, BCE loss: 0.244, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 6 Step: 7 Total Loss: 0.436, BCE loss: 0.355, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 6 Step: 8 Total Loss: 0.237, BCE loss: 0.159, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 6 Step: 9 Total Loss: 0.176, BCE loss: 0.097, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 6 Step: 10 Total Loss: 0.192, BCE loss: 0.118, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 6 Step: 11 Total Loss: 0.187, BCE loss: 0.122, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 6 Step: 12 Total Loss: 0.254, BCE loss: 0.188, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 6 Step: 13 Total Loss: 0.229, BCE loss: 0.183, Diversity Loss: 0.092                     (Diversity_weight = 0.5)
Epoch: 6 Step: 14 Total Loss: 0.356, BCE loss: 0.247, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 6 Step: 15 Total Loss: 0.244, BCE loss: 0.195, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 6 Step: 16 Total Loss: 0.317, BCE loss: 0.258, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 6 Step: 17 Total Loss: 0.202, BCE loss: 0.148, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 6 Step: 18 Total Loss: 0.168, BCE loss: 0.119, Diversity Loss: 0.098                     (Diversity_weight = 0.5)
Epoch: 6 Step: 19 Total Loss: 0.189, BCE loss: 0.140, Diversity Loss: 0.098                     (Diversity_weight = 0.5)
Epoch: 6 Step: 20 Total Loss: 0.262, BCE loss: 0.202, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 6 Step: 21 Total Loss: 0.235, BCE loss: 0.179, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 6 Step: 22 Total Loss: 0.148, BCE loss: 0.081, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 6 Step: 23 Total Loss: 0.178, BCE loss: 0.104, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 6 Step: 24 Total Loss: 0.248, BCE loss: 0.149, Diversity Loss: 0.197                     (Diversity_weight = 0.5)
Epoch: 6 Step: 25 Total Loss: 0.280, BCE loss: 0.209, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 6 Step: 26 Total Loss: 0.166, BCE loss: 0.106, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 6 Step: 27 Total Loss: 0.306, BCE loss: 0.247, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 6 Step: 28 Total Loss: 0.257, BCE loss: 0.202, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 6 Step: 29 Total Loss: 0.336, BCE loss: 0.276, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 6 Step: 30 Total Loss: 0.132, BCE loss: 0.070, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 6 Step: 31 Total Loss: 0.202, BCE loss: 0.139, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 6 Step: 32 Total Loss: 0.160, BCE loss: 0.109, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 6 Step: 33 Total Loss: 0.321, BCE loss: 0.252, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 6 Step: 34 Total Loss: 0.131, BCE loss: 0.056, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 6 Step: 35 Total Loss: 0.139, BCE loss: 0.065, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 6 Step: 36 Total Loss: 0.157, BCE loss: 0.104, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 6 Step: 37 Total Loss: 0.164, BCE loss: 0.097, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 6 Step: 38 Total Loss: 0.355, BCE loss: 0.262, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 6 Step: 39 Total Loss: 0.309, BCE loss: 0.176, Diversity Loss: 0.266                     (Diversity_weight = 0.5)
Epoch: 6 Step: 40 Total Loss: 0.198, BCE loss: 0.119, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 6 Step: 41 Total Loss: 0.339, BCE loss: 0.232, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 6 Step: 42 Total Loss: 0.316, BCE loss: 0.249, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 6 Step: 43 Total Loss: 0.384, BCE loss: 0.315, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 6 Step: 44 Total Loss: 0.213, BCE loss: 0.131, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 6 Step: 45 Total Loss: 0.225, BCE loss: 0.111, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 6 Step: 46 Total Loss: 0.236, BCE loss: 0.146, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 6 Step: 47 Total Loss: 0.239, BCE loss: 0.165, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 6 Step: 48 Total Loss: 0.475, BCE loss: 0.355, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 6 Step: 49 Total Loss: 0.178, BCE loss: 0.130, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 6 Step: 50 Total Loss: 0.133, BCE loss: 0.084, Diversity Loss: 0.098                     (Diversity_weight = 0.5)
Epoch: 6 Step: 51 Total Loss: 0.150, BCE loss: 0.098, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 6 Step: 52 Total Loss: 0.249, BCE loss: 0.155, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 6 Step: 53 Total Loss: 0.121, BCE loss: 0.067, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 6 Step: 54 Total Loss: 0.537, BCE loss: 0.480, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 6 Step: 55 Total Loss: 0.275, BCE loss: 0.215, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 6 Step: 56 Total Loss: 0.129, BCE loss: 0.074, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 6 Step: 57 Total Loss: 0.122, BCE loss: 0.058, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 6 Step: 58 Total Loss: 0.193, BCE loss: 0.130, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 6 Step: 59 Total Loss: 0.428, BCE loss: 0.354, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 6 Step: 60 Total Loss: 0.211, BCE loss: 0.123, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 6 Step: 61 Total Loss: 0.188, BCE loss: 0.117, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 6 Step: 62 Total Loss: 0.210, BCE loss: 0.125, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
{'accuracy': 0.7054108216432866, 'roc_auc': 0.7592806795590097, 'pr_auc': 0.7088271539674621, 'conicity_mean': 0.14402708, 'conicity_std': 0.046896946}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.692    0.717      0.705      0.705         0.705
precision    0.731    0.684      0.705      0.708         0.708
recall       0.658    0.754      0.705      0.706         0.705
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7592806795590097
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:23:29_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.275, BCE loss: 0.188, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 7 Step: 1 Total Loss: 0.272, BCE loss: 0.138, Diversity Loss: 0.268                     (Diversity_weight = 0.5)
Epoch: 7 Step: 2 Total Loss: 0.226, BCE loss: 0.137, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 7 Step: 3 Total Loss: 0.077, BCE loss: 0.023, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 7 Step: 4 Total Loss: 0.335, BCE loss: 0.225, Diversity Loss: 0.220                     (Diversity_weight = 0.5)
Epoch: 7 Step: 5 Total Loss: 0.129, BCE loss: 0.083, Diversity Loss: 0.092                     (Diversity_weight = 0.5)
Epoch: 7 Step: 6 Total Loss: 0.090, BCE loss: 0.041, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 7 Step: 7 Total Loss: 0.187, BCE loss: 0.123, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 7 Step: 8 Total Loss: 0.178, BCE loss: 0.112, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 7 Step: 9 Total Loss: 0.162, BCE loss: 0.078, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 7 Step: 10 Total Loss: 0.133, BCE loss: 0.062, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 7 Step: 11 Total Loss: 0.178, BCE loss: 0.106, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 7 Step: 12 Total Loss: 0.122, BCE loss: 0.068, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 7 Step: 13 Total Loss: 0.227, BCE loss: 0.103, Diversity Loss: 0.250                     (Diversity_weight = 0.5)
Epoch: 7 Step: 14 Total Loss: 0.108, BCE loss: 0.055, Diversity Loss: 0.104                     (Diversity_weight = 0.5)
Epoch: 7 Step: 15 Total Loss: 0.066, BCE loss: 0.016, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 7 Step: 16 Total Loss: 0.148, BCE loss: 0.073, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 7 Step: 17 Total Loss: 0.128, BCE loss: 0.051, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 7 Step: 18 Total Loss: 0.189, BCE loss: 0.127, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 7 Step: 19 Total Loss: 0.099, BCE loss: 0.037, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 7 Step: 20 Total Loss: 0.135, BCE loss: 0.085, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 7 Step: 21 Total Loss: 0.147, BCE loss: 0.050, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 7 Step: 22 Total Loss: 0.104, BCE loss: 0.048, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 7 Step: 23 Total Loss: 0.229, BCE loss: 0.143, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 7 Step: 24 Total Loss: 0.128, BCE loss: 0.043, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 7 Step: 25 Total Loss: 0.116, BCE loss: 0.045, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 7 Step: 26 Total Loss: 0.187, BCE loss: 0.136, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 7 Step: 27 Total Loss: 0.268, BCE loss: 0.193, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 7 Step: 28 Total Loss: 0.261, BCE loss: 0.195, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 7 Step: 29 Total Loss: 0.142, BCE loss: 0.085, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 7 Step: 30 Total Loss: 0.436, BCE loss: 0.342, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 7 Step: 31 Total Loss: 0.211, BCE loss: 0.124, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 7 Step: 32 Total Loss: 0.147, BCE loss: 0.094, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 7 Step: 33 Total Loss: 0.104, BCE loss: 0.054, Diversity Loss: 0.099                     (Diversity_weight = 0.5)
Epoch: 7 Step: 34 Total Loss: 0.172, BCE loss: 0.091, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 7 Step: 35 Total Loss: 0.169, BCE loss: 0.091, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 7 Step: 36 Total Loss: 0.153, BCE loss: 0.075, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 7 Step: 37 Total Loss: 0.343, BCE loss: 0.229, Diversity Loss: 0.226                     (Diversity_weight = 0.5)
Epoch: 7 Step: 38 Total Loss: 0.200, BCE loss: 0.123, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 7 Step: 39 Total Loss: 0.135, BCE loss: 0.065, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 7 Step: 40 Total Loss: 0.152, BCE loss: 0.097, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 7 Step: 41 Total Loss: 0.324, BCE loss: 0.260, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 7 Step: 42 Total Loss: 0.279, BCE loss: 0.191, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 7 Step: 43 Total Loss: 0.247, BCE loss: 0.187, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 7 Step: 44 Total Loss: 0.121, BCE loss: 0.063, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 7 Step: 45 Total Loss: 0.190, BCE loss: 0.126, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 7 Step: 46 Total Loss: 0.266, BCE loss: 0.189, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 7 Step: 47 Total Loss: 0.143, BCE loss: 0.093, Diversity Loss: 0.101                     (Diversity_weight = 0.5)
Epoch: 7 Step: 48 Total Loss: 0.311, BCE loss: 0.251, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 7 Step: 49 Total Loss: 0.129, BCE loss: 0.066, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 7 Step: 50 Total Loss: 0.228, BCE loss: 0.144, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 7 Step: 51 Total Loss: 0.251, BCE loss: 0.198, Diversity Loss: 0.106                     (Diversity_weight = 0.5)
Epoch: 7 Step: 52 Total Loss: 0.164, BCE loss: 0.116, Diversity Loss: 0.096                     (Diversity_weight = 0.5)
Epoch: 7 Step: 53 Total Loss: 0.190, BCE loss: 0.126, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 7 Step: 54 Total Loss: 0.088, BCE loss: 0.036, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 7 Step: 55 Total Loss: 0.106, BCE loss: 0.044, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 7 Step: 56 Total Loss: 0.270, BCE loss: 0.169, Diversity Loss: 0.202                     (Diversity_weight = 0.5)
Epoch: 7 Step: 57 Total Loss: 0.212, BCE loss: 0.145, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 7 Step: 58 Total Loss: 0.220, BCE loss: 0.147, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 7 Step: 59 Total Loss: 0.338, BCE loss: 0.276, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 7 Step: 60 Total Loss: 0.346, BCE loss: 0.242, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 7 Step: 61 Total Loss: 0.151, BCE loss: 0.080, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 7 Step: 62 Total Loss: 0.292, BCE loss: 0.234, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
{'accuracy': 0.6813627254509018, 'roc_auc': 0.7474345844127156, 'pr_auc': 0.6941555042783083, 'conicity_mean': 0.14398544, 'conicity_std': 0.046151783}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.647    0.709      0.681      0.678         0.678
precision    0.732    0.648      0.681      0.690         0.690
recall       0.581    0.784      0.681      0.682         0.681
support    503.000  495.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7474345844127156
saved config  {'model': {'encoder': {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_fr/cls_fr/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:23:29_2021
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:24:24,027 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:24:24,027 - type = vanillalstm
INFO - 2021-01-17 19:24:24,028 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:24:24,028 - vocab_size = 604
INFO - 2021-01-17 19:24:24,028 - embed_size = 200
INFO - 2021-01-17 19:24:24,028 - hidden_size = 128
INFO - 2021-01-17 19:24:24,028 - pre_embed = None
INFO - 2021-01-17 19:24:24,043 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:24:24,043 - hidden_size = 256
INFO - 2021-01-17 19:24:24,043 - output_size = 1
INFO - 2021-01-17 19:24:24,043 - use_attention = True
INFO - 2021-01-17 19:24:24,043 - regularizer_attention = None
INFO - 2021-01-17 19:24:24,043 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b7b444ff050> and extras set()
INFO - 2021-01-17 19:24:24,043 - attention.type = tanh
INFO - 2021-01-17 19:24:24,043 - type = tanh
INFO - 2021-01-17 19:24:24,043 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b7b444ff050> and extras set()
INFO - 2021-01-17 19:24:24,044 - attention.hidden_size = 256
INFO - 2021-01-17 19:24:24,044 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.720440881763527, 'roc_auc': 0.7747494829005763, 'pr_auc': 0.7320276784510565, 'conicity_mean': '0.1414524', 'conicity_std': '0.047994677'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.701    0.738       0.72      0.719         0.719
precision    0.747    0.700       0.72      0.723         0.723
recall       0.661    0.779       0.72      0.720         0.720
support    495.000  503.000     998.00    998.000       998.000
encoder params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:24:25,372 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 604, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:24:25,372 - type = vanillalstm
INFO - 2021-01-17 19:24:25,372 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 604, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:24:25,372 - vocab_size = 604
INFO - 2021-01-17 19:24:25,372 - embed_size = 200
INFO - 2021-01-17 19:24:25,373 - hidden_size = 128
INFO - 2021-01-17 19:24:25,373 - pre_embed = None
INFO - 2021-01-17 19:24:25,387 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:24:25,387 - hidden_size = 256
INFO - 2021-01-17 19:24:25,387 - output_size = 1
INFO - 2021-01-17 19:24:25,387 - use_attention = True
INFO - 2021-01-17 19:24:25,387 - regularizer_attention = None
INFO - 2021-01-17 19:24:25,387 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b7b43ea8610> and extras set()
INFO - 2021-01-17 19:24:25,387 - attention.type = tanh
INFO - 2021-01-17 19:24:25,387 - type = tanh
INFO - 2021-01-17 19:24:25,387 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b7b43ea8610> and extras set()
INFO - 2021-01-17 19:24:25,388 - attention.hidden_size = 256
INFO - 2021-01-17 19:24:25,388 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [0.998], 'basepath': './experiments/cls_fr', 'exp_dirname': 'cls_fr/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.720440881763527, 'roc_auc': 0.7747494829005763, 'pr_auc': 0.7320276784510565, 'conicity_mean': '0.1414524', 'conicity_std': '0.047994677'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.701    0.738       0.72      0.719         0.719
precision    0.747    0.700       0.72      0.723         0.723
recall       0.661    0.779       0.72      0.720         0.720
support    495.000  503.000     998.00    998.000       998.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_jp vanilla_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:24:35,716 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:24:35,716 - type = vanillalstm
INFO - 2021-01-17 19:24:35,716 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:24:35,732 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:24:36,055 - vocab_size = 548
INFO - 2021-01-17 19:24:36,055 - embed_size = 200
INFO - 2021-01-17 19:24:36,055 - hidden_size = 128
INFO - 2021-01-17 19:24:36,055 - pre_embed = None
INFO - 2021-01-17 19:24:39,118 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:24:39,119 - hidden_size = 256
INFO - 2021-01-17 19:24:39,119 - output_size = 1
INFO - 2021-01-17 19:24:39,119 - use_attention = True
INFO - 2021-01-17 19:24:39,119 - regularizer_attention = None
INFO - 2021-01-17 19:24:39,119 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ac88556f4d0> and extras set()
INFO - 2021-01-17 19:24:39,119 - attention.type = tanh
INFO - 2021-01-17 19:24:39,119 - type = tanh
INFO - 2021-01-17 19:24:39,119 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ac88556f4d0> and extras set()
INFO - 2021-01-17 19:24:39,119 - attention.hidden_size = 256
INFO - 2021-01-17 19:24:39,120 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.712, BCE loss: 0.712, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.719, BCE loss: 0.719, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.457                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.462                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.677, BCE loss: 0.677, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.664, BCE loss: 0.664, Diversity Loss: 0.583                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.427                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.674, BCE loss: 0.674, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.658, BCE loss: 0.658, Diversity Loss: 0.669                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.672, BCE loss: 0.672, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.626, BCE loss: 0.626, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.624, BCE loss: 0.624, Diversity Loss: 0.791                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.659, BCE loss: 0.659, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.614, BCE loss: 0.614, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.673, BCE loss: 0.673, Diversity Loss: 0.630                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.590, BCE loss: 0.590, Diversity Loss: 0.697                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.574, BCE loss: 0.574, Diversity Loss: 0.755                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.625, BCE loss: 0.625, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.621                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.541, BCE loss: 0.541, Diversity Loss: 0.596                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.594, BCE loss: 0.594, Diversity Loss: 0.630                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.716, BCE loss: 0.716, Diversity Loss: 0.603                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.603, BCE loss: 0.603, Diversity Loss: 0.661                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.529, BCE loss: 0.529, Diversity Loss: 0.765                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.441, BCE loss: 0.441, Diversity Loss: 0.576                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.573, BCE loss: 0.573, Diversity Loss: 0.641                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.606, BCE loss: 0.606, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.577, BCE loss: 0.577, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.614, BCE loss: 0.614, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.601, BCE loss: 0.601, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.606, BCE loss: 0.606, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.703, BCE loss: 0.703, Diversity Loss: 0.586                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.635, BCE loss: 0.635, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.643, BCE loss: 0.643, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.598, BCE loss: 0.598, Diversity Loss: 0.622                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.653, BCE loss: 0.653, Diversity Loss: 0.561                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.636, BCE loss: 0.636, Diversity Loss: 0.685                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.551, BCE loss: 0.551, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.624, BCE loss: 0.624, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.628, BCE loss: 0.628, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.658, BCE loss: 0.658, Diversity Loss: 0.470                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.637, BCE loss: 0.637, Diversity Loss: 0.482                     (Diversity_weight = 0)
{'accuracy': 0.720440881763527, 'roc_auc': 0.8093066317802153, 'pr_auc': 0.7742188248514098, 'conicity_mean': 0.50146234, 'conicity_std': 0.102705576}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.765    0.654       0.72      0.710         0.712
precision    0.683    0.795       0.72      0.739         0.736
recall       0.870    0.556       0.72      0.713         0.720
support    523.000  475.000     998.00    998.000       998.000
Model Saved on  roc_auc 0.8093066317802153
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:24:39_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.587, BCE loss: 0.587, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.555, BCE loss: 0.555, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.615, BCE loss: 0.615, Diversity Loss: 0.461                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.599, BCE loss: 0.599, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.616, BCE loss: 0.616, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.545, BCE loss: 0.545, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.556, BCE loss: 0.556, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.519, BCE loss: 0.519, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.533, BCE loss: 0.533, Diversity Loss: 0.579                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.449, BCE loss: 0.449, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.499, BCE loss: 0.499, Diversity Loss: 0.604                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.763                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.539, BCE loss: 0.539, Diversity Loss: 0.708                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.712, BCE loss: 0.712, Diversity Loss: 0.717                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.713, BCE loss: 0.713, Diversity Loss: 0.675                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.465, BCE loss: 0.465, Diversity Loss: 0.627                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.511, BCE loss: 0.511, Diversity Loss: 0.575                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.649, BCE loss: 0.649, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.648                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.461, BCE loss: 0.461, Diversity Loss: 0.699                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.523, BCE loss: 0.523, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.535, BCE loss: 0.535, Diversity Loss: 0.617                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.512, BCE loss: 0.512, Diversity Loss: 0.722                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.496, BCE loss: 0.496, Diversity Loss: 0.607                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.618, BCE loss: 0.618, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.667                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.561, BCE loss: 0.561, Diversity Loss: 0.616                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.501, BCE loss: 0.501, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.356, BCE loss: 0.356, Diversity Loss: 0.647                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.793                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.559, BCE loss: 0.559, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.464                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.539, BCE loss: 0.539, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.423, BCE loss: 0.423, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.630, BCE loss: 0.630, Diversity Loss: 0.432                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.639, BCE loss: 0.639, Diversity Loss: 0.437                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.427                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.451, BCE loss: 0.451, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.509, BCE loss: 0.509, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.558, BCE loss: 0.558, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.431, BCE loss: 0.431, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.553, BCE loss: 0.553, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.456, BCE loss: 0.456, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.525, BCE loss: 0.525, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.573                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.612, BCE loss: 0.612, Diversity Loss: 0.609                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.671, BCE loss: 0.671, Diversity Loss: 0.555                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.363, BCE loss: 0.363, Diversity Loss: 0.587                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.604, BCE loss: 0.604, Diversity Loss: 0.685                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.585, BCE loss: 0.585, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.481, BCE loss: 0.481, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.340, BCE loss: 0.340, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.500, BCE loss: 0.500, Diversity Loss: 0.546                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.532, BCE loss: 0.532, Diversity Loss: 0.591                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.447, BCE loss: 0.447, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.488, BCE loss: 0.488, Diversity Loss: 0.650                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.607, BCE loss: 0.607, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.498                     (Diversity_weight = 0)
{'accuracy': 0.7635270541082164, 'roc_auc': 0.8433732514843513, 'pr_auc': 0.8274957418188316, 'conicity_mean': 0.5431676, 'conicity_std': 0.13831218}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.754    0.773      0.764      0.763         0.763
precision    0.830    0.712      0.764      0.771         0.774
recall       0.690    0.844      0.764      0.767         0.764
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.8433732514843513
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:24:39_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.459, BCE loss: 0.459, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.346, BCE loss: 0.346, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.548, BCE loss: 0.548, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.423, BCE loss: 0.423, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.430, BCE loss: 0.430, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.371, BCE loss: 0.371, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.537                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.416, BCE loss: 0.416, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.440, BCE loss: 0.440, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.514, BCE loss: 0.514, Diversity Loss: 0.479                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.470, BCE loss: 0.470, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.470, BCE loss: 0.470, Diversity Loss: 0.568                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.450, BCE loss: 0.450, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.460, BCE loss: 0.460, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.375, BCE loss: 0.375, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.461, BCE loss: 0.461, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.534                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.567, BCE loss: 0.567, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.646                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.585, BCE loss: 0.585, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.279, BCE loss: 0.279, Diversity Loss: 0.484                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.435, BCE loss: 0.435, Diversity Loss: 0.569                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.389, BCE loss: 0.389, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.447, BCE loss: 0.447, Diversity Loss: 0.594                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.475, BCE loss: 0.475, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.415, BCE loss: 0.415, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.397, BCE loss: 0.397, Diversity Loss: 0.685                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.468, BCE loss: 0.468, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.428, BCE loss: 0.428, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.321, BCE loss: 0.321, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.392, BCE loss: 0.392, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.360, BCE loss: 0.360, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.457, BCE loss: 0.457, Diversity Loss: 0.601                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.346, BCE loss: 0.346, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.482, BCE loss: 0.482, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.370, BCE loss: 0.370, Diversity Loss: 0.676                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.443, BCE loss: 0.443, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.386, BCE loss: 0.386, Diversity Loss: 0.582                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.568, BCE loss: 0.568, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.390, BCE loss: 0.390, Diversity Loss: 0.552                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.437, BCE loss: 0.437, Diversity Loss: 0.615                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.348, BCE loss: 0.348, Diversity Loss: 0.588                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.610, BCE loss: 0.610, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.350, BCE loss: 0.350, Diversity Loss: 0.626                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.572                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.449, BCE loss: 0.449, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.271, BCE loss: 0.271, Diversity Loss: 0.574                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.536, BCE loss: 0.536, Diversity Loss: 0.594                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.292, BCE loss: 0.292, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.599                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.377, BCE loss: 0.377, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.489, BCE loss: 0.489, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.512, BCE loss: 0.512, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.427, BCE loss: 0.427, Diversity Loss: 0.454                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.388, BCE loss: 0.388, Diversity Loss: 0.517                     (Diversity_weight = 0)
{'accuracy': 0.782565130260521, 'roc_auc': 0.8475797524403743, 'pr_auc': 0.8300361331518316, 'conicity_mean': 0.51693946, 'conicity_std': 0.12797213}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.785    0.780      0.783      0.783         0.783
precision    0.815    0.752      0.783      0.783         0.785
recall       0.757    0.811      0.783      0.784         0.783
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.8475797524403743
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:24:39_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.268, BCE loss: 0.268, Diversity Loss: 0.453                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.641                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.297, BCE loss: 0.297, Diversity Loss: 0.540                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.470, BCE loss: 0.470, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.228, BCE loss: 0.228, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.343, BCE loss: 0.343, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.525                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.681                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.571                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.312, BCE loss: 0.312, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.273, BCE loss: 0.273, Diversity Loss: 0.509                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.543, BCE loss: 0.543, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.540, BCE loss: 0.540, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.595                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.377, BCE loss: 0.377, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.366, BCE loss: 0.366, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.245, BCE loss: 0.245, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.261, BCE loss: 0.261, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.548                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.368, BCE loss: 0.368, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.459, BCE loss: 0.459, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.191, BCE loss: 0.191, Diversity Loss: 0.521                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.291, BCE loss: 0.291, Diversity Loss: 0.538                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.408, BCE loss: 0.408, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.390, BCE loss: 0.390, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.478, BCE loss: 0.478, Diversity Loss: 0.662                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.509, BCE loss: 0.509, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.335, BCE loss: 0.335, Diversity Loss: 0.605                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.236, BCE loss: 0.236, Diversity Loss: 0.471                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.474, BCE loss: 0.474, Diversity Loss: 0.531                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.305, BCE loss: 0.305, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.318, BCE loss: 0.318, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.405, BCE loss: 0.405, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.362, BCE loss: 0.362, Diversity Loss: 0.560                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.343, BCE loss: 0.343, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.277, BCE loss: 0.277, Diversity Loss: 0.527                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.355, BCE loss: 0.355, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.502, BCE loss: 0.502, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.354, BCE loss: 0.354, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.359, BCE loss: 0.359, Diversity Loss: 0.535                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.236, BCE loss: 0.236, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.373, BCE loss: 0.373, Diversity Loss: 0.536                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.396, BCE loss: 0.396, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.382, BCE loss: 0.382, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.210, BCE loss: 0.210, Diversity Loss: 0.539                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.349, BCE loss: 0.349, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.286, BCE loss: 0.286, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.589, BCE loss: 0.589, Diversity Loss: 0.618                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.266, BCE loss: 0.266, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.511, BCE loss: 0.511, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.442, BCE loss: 0.442, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.426, BCE loss: 0.426, Diversity Loss: 0.563                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.231, BCE loss: 0.231, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.382, BCE loss: 0.382, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.551, BCE loss: 0.551, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.278, BCE loss: 0.278, Diversity Loss: 0.496                     (Diversity_weight = 0)
{'accuracy': 0.7705410821643287, 'roc_auc': 0.8475495622421254, 'pr_auc': 0.835874448532772, 'conicity_mean': 0.4827442, 'conicity_std': 0.12135652}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.796    0.738      0.771      0.767         0.768
precision    0.746    0.808      0.771      0.777         0.775
recall       0.853    0.680      0.771      0.766         0.771
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.8475495622421254
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:24:39_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.349, BCE loss: 0.349, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.278, BCE loss: 0.278, Diversity Loss: 0.451                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.255, BCE loss: 0.255, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.238, BCE loss: 0.238, Diversity Loss: 0.458                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.207, BCE loss: 0.207, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.495, BCE loss: 0.495, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.254, BCE loss: 0.254, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.265, BCE loss: 0.265, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.354, BCE loss: 0.354, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.509, BCE loss: 0.509, Diversity Loss: 0.566                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.530                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.298, BCE loss: 0.298, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.315, BCE loss: 0.315, Diversity Loss: 0.519                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.159, BCE loss: 0.159, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.400, BCE loss: 0.400, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.264, BCE loss: 0.264, Diversity Loss: 0.590                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.341, BCE loss: 0.341, Diversity Loss: 0.501                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.306, BCE loss: 0.306, Diversity Loss: 0.553                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.303, BCE loss: 0.303, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.236, BCE loss: 0.236, Diversity Loss: 0.494                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.207, BCE loss: 0.207, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.240, BCE loss: 0.240, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.233, BCE loss: 0.233, Diversity Loss: 0.551                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.331, BCE loss: 0.331, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.534, BCE loss: 0.534, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.570                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.270, BCE loss: 0.270, Diversity Loss: 0.635                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.525, BCE loss: 0.525, Diversity Loss: 0.650                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.322, BCE loss: 0.322, Diversity Loss: 0.593                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.365, BCE loss: 0.365, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.520                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.195, BCE loss: 0.195, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.469                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.209, BCE loss: 0.209, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.260, BCE loss: 0.260, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.283, BCE loss: 0.283, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.515                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.136, BCE loss: 0.136, Diversity Loss: 0.619                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.262, BCE loss: 0.262, Diversity Loss: 0.586                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.189, BCE loss: 0.189, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.221, BCE loss: 0.221, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.126, BCE loss: 0.126, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.432, BCE loss: 0.432, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.594                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.584                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.263, BCE loss: 0.263, Diversity Loss: 0.449                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.197, BCE loss: 0.197, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.275, BCE loss: 0.275, Diversity Loss: 0.452                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.242, BCE loss: 0.242, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.289, BCE loss: 0.289, Diversity Loss: 0.506                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.226, BCE loss: 0.226, Diversity Loss: 0.489                     (Diversity_weight = 0)
{'accuracy': 0.7525050100200401, 'roc_auc': 0.8320297876622723, 'pr_auc': 0.8124764181126856, 'conicity_mean': 0.5019053, 'conicity_std': 0.12714973}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.747    0.758      0.753      0.752         0.752
precision    0.804    0.710      0.753      0.757         0.759
recall       0.698    0.813      0.753      0.755         0.753
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.8320297876622723
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:24:39_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.444                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.410, BCE loss: 0.410, Diversity Loss: 0.558                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.088, BCE loss: 0.088, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.502                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.133, BCE loss: 0.133, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.206, BCE loss: 0.206, Diversity Loss: 0.458                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.252, BCE loss: 0.252, Diversity Loss: 0.592                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.565                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.366, BCE loss: 0.366, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.146, BCE loss: 0.146, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.388, BCE loss: 0.388, Diversity Loss: 0.607                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.118, BCE loss: 0.118, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.218, BCE loss: 0.218, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.292, BCE loss: 0.292, Diversity Loss: 0.554                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.436, BCE loss: 0.436, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.466                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.537, BCE loss: 0.537, Diversity Loss: 0.405                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.167, BCE loss: 0.167, Diversity Loss: 0.432                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.216, BCE loss: 0.216, Diversity Loss: 0.504                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.145, BCE loss: 0.145, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.387, BCE loss: 0.387, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.134, BCE loss: 0.134, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.168, BCE loss: 0.168, Diversity Loss: 0.453                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.294, BCE loss: 0.294, Diversity Loss: 0.456                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.212, BCE loss: 0.212, Diversity Loss: 0.434                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.459                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.265, BCE loss: 0.265, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.482                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.164, BCE loss: 0.164, Diversity Loss: 0.547                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.182, BCE loss: 0.182, Diversity Loss: 0.543                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.196, BCE loss: 0.196, Diversity Loss: 0.437                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.580                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.207, BCE loss: 0.207, Diversity Loss: 0.450                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.256, BCE loss: 0.256, Diversity Loss: 0.512                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.274, BCE loss: 0.274, Diversity Loss: 0.439                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.505                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.223, BCE loss: 0.223, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.161, BCE loss: 0.161, Diversity Loss: 0.471                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.209, BCE loss: 0.209, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.450                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.144, BCE loss: 0.144, Diversity Loss: 0.489                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.133, BCE loss: 0.133, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.491                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.262, BCE loss: 0.262, Diversity Loss: 0.529                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.214, BCE loss: 0.214, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.597                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.267, BCE loss: 0.267, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.596, BCE loss: 0.596, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.454                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.217, BCE loss: 0.217, Diversity Loss: 0.453                     (Diversity_weight = 0)
{'accuracy': 0.7334669338677354, 'roc_auc': 0.814515447318104, 'pr_auc': 0.7877265075595598, 'conicity_mean': 0.47387758, 'conicity_std': 0.120407395}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.716    0.749      0.733      0.733         0.732
precision    0.810    0.679      0.733      0.744         0.748
recall       0.642    0.834      0.733      0.738         0.733
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.814515447318104
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:24:39_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.449                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.193, BCE loss: 0.193, Diversity Loss: 0.452                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.343, BCE loss: 0.343, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.130, BCE loss: 0.130, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.439                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.164, BCE loss: 0.164, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.447                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.452                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.260, BCE loss: 0.260, Diversity Loss: 0.557                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.429                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.526                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.524                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.492                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.091, BCE loss: 0.091, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.298, BCE loss: 0.298, Diversity Loss: 0.499                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.673                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.445                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.273, BCE loss: 0.273, Diversity Loss: 0.457                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.436                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.423                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.449                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.136, BCE loss: 0.136, Diversity Loss: 0.486                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.223, BCE loss: 0.223, Diversity Loss: 0.455                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.201, BCE loss: 0.201, Diversity Loss: 0.453                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.434                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.424                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.550                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.187, BCE loss: 0.187, Diversity Loss: 0.440                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.438                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.174, BCE loss: 0.174, Diversity Loss: 0.495                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.186, BCE loss: 0.186, Diversity Loss: 0.412                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.143, BCE loss: 0.143, Diversity Loss: 0.516                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.125, BCE loss: 0.125, Diversity Loss: 0.403                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.190, BCE loss: 0.190, Diversity Loss: 0.403                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.443                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.189, BCE loss: 0.189, Diversity Loss: 0.418                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.349, BCE loss: 0.349, Diversity Loss: 0.600                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.437                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.169, BCE loss: 0.169, Diversity Loss: 0.440                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.142, BCE loss: 0.142, Diversity Loss: 0.523                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.160, BCE loss: 0.160, Diversity Loss: 0.517                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.094, BCE loss: 0.094, Diversity Loss: 0.453                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.480                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.434                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.420                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.166, BCE loss: 0.166, Diversity Loss: 0.483                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.096, BCE loss: 0.096, Diversity Loss: 0.418                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.055, BCE loss: 0.055, Diversity Loss: 0.435                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.123, BCE loss: 0.123, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.441                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.085, BCE loss: 0.085, Diversity Loss: 0.449                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.468                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.098, BCE loss: 0.098, Diversity Loss: 0.508                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.064, BCE loss: 0.064, Diversity Loss: 0.446                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.543                     (Diversity_weight = 0)
{'accuracy': 0.7424849699398798, 'roc_auc': 0.8216041058669619, 'pr_auc': 0.7960263199636608, 'conicity_mean': 0.47097877, 'conicity_std': 0.12182654}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.766    0.714      0.742      0.740         0.741
precision    0.732    0.757      0.742      0.744         0.744
recall       0.803    0.676      0.742      0.739         0.742
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.8216041058669619
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:24:39_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.226, BCE loss: 0.226, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.257, BCE loss: 0.257, Diversity Loss: 0.466                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.419                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.470                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.493                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.101, BCE loss: 0.101, Diversity Loss: 0.478                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.564                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.485                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.472                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.533                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.487                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.513                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.457                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.463                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.207, BCE loss: 0.207, Diversity Loss: 0.474                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.062, BCE loss: 0.062, Diversity Loss: 0.500                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.407                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.058, BCE loss: 0.058, Diversity Loss: 0.532                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.577                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.008, BCE loss: 0.008, Diversity Loss: 0.518                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.503                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.083, BCE loss: 0.083, Diversity Loss: 0.528                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.490                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.420                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.475                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.473                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.477                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.027, BCE loss: 0.027, Diversity Loss: 0.446                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.542                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.012, BCE loss: 0.012, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.457                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.007, BCE loss: 0.007, Diversity Loss: 0.460                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.014, BCE loss: 0.014, Diversity Loss: 0.522                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.047, BCE loss: 0.047, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.465                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.113, BCE loss: 0.113, Diversity Loss: 0.457                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.275, BCE loss: 0.275, Diversity Loss: 0.511                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.433                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.044, BCE loss: 0.044, Diversity Loss: 0.476                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.450                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.481                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.094, BCE loss: 0.094, Diversity Loss: 0.441                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.448                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.003, BCE loss: 0.003, Diversity Loss: 0.562                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.467                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.382                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.189, BCE loss: 0.189, Diversity Loss: 0.394                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.608                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.488                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.510                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.066, BCE loss: 0.066, Diversity Loss: 0.441                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.498                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.112, BCE loss: 0.112, Diversity Loss: 0.497                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.507                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.229, BCE loss: 0.229, Diversity Loss: 0.431                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.167, BCE loss: 0.167, Diversity Loss: 0.514                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.422                     (Diversity_weight = 0)
{'accuracy': 0.7254509018036072, 'roc_auc': 0.8135372848948375, 'pr_auc': 0.7919138581587175, 'conicity_mean': 0.4581185, 'conicity_std': 0.11897377}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.730    0.721      0.725      0.725         0.726
precision    0.754    0.698      0.725      0.726         0.727
recall       0.707    0.745      0.725      0.726         0.725
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.8135372848948375
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
./experiments/cls_jp/cls_jp/lstm+tanh/Sun_Jan_17_19:24:39_2021
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:25:26,280 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:25:26,280 - type = vanillalstm
INFO - 2021-01-17 19:25:26,280 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:25:26,281 - vocab_size = 548
INFO - 2021-01-17 19:25:26,281 - embed_size = 200
INFO - 2021-01-17 19:25:26,281 - hidden_size = 128
INFO - 2021-01-17 19:25:26,281 - pre_embed = None
INFO - 2021-01-17 19:25:26,295 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:25:26,295 - hidden_size = 256
INFO - 2021-01-17 19:25:26,295 - output_size = 1
INFO - 2021-01-17 19:25:26,295 - use_attention = True
INFO - 2021-01-17 19:25:26,295 - regularizer_attention = None
INFO - 2021-01-17 19:25:26,296 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ac8857b0250> and extras set()
INFO - 2021-01-17 19:25:26,296 - attention.type = tanh
INFO - 2021-01-17 19:25:26,296 - type = tanh
INFO - 2021-01-17 19:25:26,296 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ac8857b0250> and extras set()
INFO - 2021-01-17 19:25:26,296 - attention.hidden_size = 256
INFO - 2021-01-17 19:25:26,296 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7637637637637638, 'roc_auc': 0.8597549216552833, 'pr_auc': 0.8793555926281909, 'conicity_mean': '0.51132435', 'conicity_std': '0.120395884'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.756    0.771      0.764      0.764         0.764
precision    0.742    0.785      0.764      0.763         0.765
recall       0.771    0.758      0.764      0.764         0.764
support    475.000  524.000    999.000    999.000       999.000
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:25:27,445 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:25:27,445 - type = vanillalstm
INFO - 2021-01-17 19:25:27,445 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:25:27,445 - vocab_size = 548
INFO - 2021-01-17 19:25:27,445 - embed_size = 200
INFO - 2021-01-17 19:25:27,445 - hidden_size = 128
INFO - 2021-01-17 19:25:27,445 - pre_embed = None
INFO - 2021-01-17 19:25:27,459 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:25:27,459 - hidden_size = 256
INFO - 2021-01-17 19:25:27,459 - output_size = 1
INFO - 2021-01-17 19:25:27,459 - use_attention = True
INFO - 2021-01-17 19:25:27,459 - regularizer_attention = None
INFO - 2021-01-17 19:25:27,459 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2ac88556f890> and extras set()
INFO - 2021-01-17 19:25:27,459 - attention.type = tanh
INFO - 2021-01-17 19:25:27,459 - type = tanh
INFO - 2021-01-17 19:25:27,459 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2ac88556f890> and extras set()
INFO - 2021-01-17 19:25:27,459 - attention.hidden_size = 256
INFO - 2021-01-17 19:25:27,459 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 256, 'sparsity_lambda': 0.2}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7637637637637638, 'roc_auc': 0.8597549216552833, 'pr_auc': 0.8793555926281909, 'conicity_mean': '0.51132435', 'conicity_std': '0.120395884'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.756    0.771      0.764      0.764         0.764
precision    0.742    0.785      0.764      0.763         0.765
recall       0.771    0.758      0.764      0.764         0.764
support    475.000  524.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_jp ortho_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:25:37,174 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:25:37,175 - type = ortholstm
INFO - 2021-01-17 19:25:37,175 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:25:37,190 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:25:37,513 - vocab_size = 548
INFO - 2021-01-17 19:25:37,513 - embed_size = 200
INFO - 2021-01-17 19:25:37,513 - hidden_size = 128
INFO - 2021-01-17 19:25:37,513 - pre_embed = None
INFO - 2021-01-17 19:25:40,587 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:25:40,588 - hidden_size = 256
INFO - 2021-01-17 19:25:40,588 - output_size = 1
INFO - 2021-01-17 19:25:40,588 - use_attention = True
INFO - 2021-01-17 19:25:40,588 - regularizer_attention = None
INFO - 2021-01-17 19:25:40,588 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b7a51362590> and extras set()
INFO - 2021-01-17 19:25:40,589 - attention.type = tanh
INFO - 2021-01-17 19:25:40,589 - type = tanh
INFO - 2021-01-17 19:25:40,589 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b7a51362590> and extras set()
INFO - 2021-01-17 19:25:40,589 - attention.hidden_size = 256
INFO - 2021-01-17 19:25:40,589 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 0 Step: 1 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 0 Step: 2 Total Loss: 0.676, BCE loss: 0.676, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 0 Step: 3 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.107                     (Diversity_weight = 0)
Epoch: 0 Step: 4 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 0 Step: 5 Total Loss: 0.697, BCE loss: 0.697, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 0 Step: 6 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 0 Step: 7 Total Loss: 0.699, BCE loss: 0.699, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 0 Step: 8 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 0 Step: 9 Total Loss: 0.701, BCE loss: 0.701, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 0 Step: 10 Total Loss: 0.705, BCE loss: 0.705, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 0 Step: 11 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.110                     (Diversity_weight = 0)
Epoch: 0 Step: 12 Total Loss: 0.698, BCE loss: 0.698, Diversity Loss: 0.142                     (Diversity_weight = 0)
Epoch: 0 Step: 13 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 0 Step: 14 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 0 Step: 15 Total Loss: 0.694, BCE loss: 0.694, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 0 Step: 16 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 0 Step: 17 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 0 Step: 18 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 0 Step: 19 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.123                     (Diversity_weight = 0)
Epoch: 0 Step: 20 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 0 Step: 21 Total Loss: 0.685, BCE loss: 0.685, Diversity Loss: 0.262                     (Diversity_weight = 0)
Epoch: 0 Step: 22 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 23 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 0 Step: 24 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 0 Step: 25 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 0 Step: 26 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 0 Step: 27 Total Loss: 0.680, BCE loss: 0.680, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 0 Step: 28 Total Loss: 0.684, BCE loss: 0.684, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 0 Step: 29 Total Loss: 0.703, BCE loss: 0.703, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 0 Step: 30 Total Loss: 0.709, BCE loss: 0.709, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 0 Step: 31 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.237                     (Diversity_weight = 0)
Epoch: 0 Step: 32 Total Loss: 0.696, BCE loss: 0.696, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 0 Step: 33 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 0 Step: 34 Total Loss: 0.690, BCE loss: 0.690, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 0 Step: 35 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 0 Step: 36 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 0 Step: 37 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 0 Step: 38 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 0 Step: 39 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 0 Step: 40 Total Loss: 0.692, BCE loss: 0.692, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 0 Step: 41 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 0 Step: 42 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.151                     (Diversity_weight = 0)
Epoch: 0 Step: 43 Total Loss: 0.689, BCE loss: 0.689, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 0 Step: 44 Total Loss: 0.670, BCE loss: 0.670, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 0 Step: 45 Total Loss: 0.678, BCE loss: 0.678, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 0 Step: 46 Total Loss: 0.683, BCE loss: 0.683, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 0 Step: 47 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 0 Step: 48 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 0 Step: 49 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 0 Step: 50 Total Loss: 0.686, BCE loss: 0.686, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 0 Step: 51 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.115                     (Diversity_weight = 0)
Epoch: 0 Step: 52 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 0 Step: 53 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 0 Step: 54 Total Loss: 0.688, BCE loss: 0.688, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 0 Step: 55 Total Loss: 0.716, BCE loss: 0.716, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 0 Step: 56 Total Loss: 0.651, BCE loss: 0.651, Diversity Loss: 0.133                     (Diversity_weight = 0)
Epoch: 0 Step: 57 Total Loss: 0.666, BCE loss: 0.666, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 0 Step: 58 Total Loss: 0.712, BCE loss: 0.712, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 0 Step: 59 Total Loss: 0.693, BCE loss: 0.693, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 0 Step: 60 Total Loss: 0.682, BCE loss: 0.682, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 0 Step: 61 Total Loss: 0.679, BCE loss: 0.679, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 0 Step: 62 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.122                     (Diversity_weight = 0)
{'accuracy': 0.625250501002004, 'roc_auc': 0.6826969910435745, 'pr_auc': 0.6318749524450589, 'conicity_mean': 0.16994701, 'conicity_std': 0.041377895}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.600    0.647      0.625      0.624         0.623
precision    0.680    0.586      0.625      0.633         0.636
recall       0.537    0.722      0.625      0.630         0.625
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.6826969910435745
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:25:40_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.660, BCE loss: 0.660, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 1 Step: 1 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.154                     (Diversity_weight = 0)
Epoch: 1 Step: 2 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.268                     (Diversity_weight = 0)
Epoch: 1 Step: 3 Total Loss: 0.610, BCE loss: 0.610, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 1 Step: 4 Total Loss: 0.675, BCE loss: 0.675, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 1 Step: 5 Total Loss: 0.658, BCE loss: 0.658, Diversity Loss: 0.116                     (Diversity_weight = 0)
Epoch: 1 Step: 6 Total Loss: 0.691, BCE loss: 0.691, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 1 Step: 7 Total Loss: 0.564, BCE loss: 0.564, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 1 Step: 8 Total Loss: 0.629, BCE loss: 0.629, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 1 Step: 9 Total Loss: 0.594, BCE loss: 0.594, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 1 Step: 10 Total Loss: 0.663, BCE loss: 0.663, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 1 Step: 11 Total Loss: 0.600, BCE loss: 0.600, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 1 Step: 12 Total Loss: 0.639, BCE loss: 0.639, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 1 Step: 13 Total Loss: 0.633, BCE loss: 0.633, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 1 Step: 14 Total Loss: 0.566, BCE loss: 0.566, Diversity Loss: 0.322                     (Diversity_weight = 0)
Epoch: 1 Step: 15 Total Loss: 0.735, BCE loss: 0.735, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 1 Step: 16 Total Loss: 0.667, BCE loss: 0.667, Diversity Loss: 0.135                     (Diversity_weight = 0)
Epoch: 1 Step: 17 Total Loss: 0.565, BCE loss: 0.565, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 1 Step: 18 Total Loss: 0.662, BCE loss: 0.662, Diversity Loss: 0.130                     (Diversity_weight = 0)
Epoch: 1 Step: 19 Total Loss: 0.565, BCE loss: 0.565, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 1 Step: 20 Total Loss: 0.495, BCE loss: 0.495, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 1 Step: 21 Total Loss: 0.608, BCE loss: 0.608, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 1 Step: 22 Total Loss: 0.658, BCE loss: 0.658, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 1 Step: 23 Total Loss: 0.533, BCE loss: 0.533, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 1 Step: 24 Total Loss: 0.656, BCE loss: 0.656, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 1 Step: 25 Total Loss: 0.620, BCE loss: 0.620, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 1 Step: 26 Total Loss: 0.605, BCE loss: 0.605, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 1 Step: 27 Total Loss: 0.571, BCE loss: 0.571, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 1 Step: 28 Total Loss: 0.604, BCE loss: 0.604, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 1 Step: 29 Total Loss: 0.647, BCE loss: 0.647, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 1 Step: 30 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 1 Step: 31 Total Loss: 0.630, BCE loss: 0.630, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 1 Step: 32 Total Loss: 0.496, BCE loss: 0.496, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 1 Step: 33 Total Loss: 0.568, BCE loss: 0.568, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 1 Step: 34 Total Loss: 0.473, BCE loss: 0.473, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 1 Step: 35 Total Loss: 0.507, BCE loss: 0.507, Diversity Loss: 0.263                     (Diversity_weight = 0)
Epoch: 1 Step: 36 Total Loss: 0.595, BCE loss: 0.595, Diversity Loss: 0.243                     (Diversity_weight = 0)
Epoch: 1 Step: 37 Total Loss: 0.695, BCE loss: 0.695, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 1 Step: 38 Total Loss: 0.485, BCE loss: 0.485, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 1 Step: 39 Total Loss: 0.668, BCE loss: 0.668, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 1 Step: 40 Total Loss: 0.807, BCE loss: 0.807, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 1 Step: 41 Total Loss: 0.700, BCE loss: 0.700, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 1 Step: 42 Total Loss: 0.545, BCE loss: 0.545, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 1 Step: 43 Total Loss: 0.488, BCE loss: 0.488, Diversity Loss: 0.247                     (Diversity_weight = 0)
Epoch: 1 Step: 44 Total Loss: 0.521, BCE loss: 0.521, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 1 Step: 45 Total Loss: 0.558, BCE loss: 0.558, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 1 Step: 46 Total Loss: 0.583, BCE loss: 0.583, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 1 Step: 47 Total Loss: 0.561, BCE loss: 0.561, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 1 Step: 48 Total Loss: 0.576, BCE loss: 0.576, Diversity Loss: 0.120                     (Diversity_weight = 0)
Epoch: 1 Step: 49 Total Loss: 0.515, BCE loss: 0.515, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 1 Step: 50 Total Loss: 0.542, BCE loss: 0.542, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 1 Step: 51 Total Loss: 0.539, BCE loss: 0.539, Diversity Loss: 0.180                     (Diversity_weight = 0)
Epoch: 1 Step: 52 Total Loss: 0.519, BCE loss: 0.519, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 1 Step: 53 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 1 Step: 54 Total Loss: 0.511, BCE loss: 0.511, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 1 Step: 55 Total Loss: 0.611, BCE loss: 0.611, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 1 Step: 56 Total Loss: 0.632, BCE loss: 0.632, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 1 Step: 57 Total Loss: 0.582, BCE loss: 0.582, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 1 Step: 58 Total Loss: 0.687, BCE loss: 0.687, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 1 Step: 59 Total Loss: 0.558, BCE loss: 0.558, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 1 Step: 60 Total Loss: 0.661, BCE loss: 0.661, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 1 Step: 61 Total Loss: 0.585, BCE loss: 0.585, Diversity Loss: 0.147                     (Diversity_weight = 0)
Epoch: 1 Step: 62 Total Loss: 0.635, BCE loss: 0.635, Diversity Loss: 0.180                     (Diversity_weight = 0)
{'accuracy': 0.7304609218436874, 'roc_auc': 0.7836610647076583, 'pr_auc': 0.7493638936050799, 'conicity_mean': 0.19155777, 'conicity_std': 0.045079663}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.747    0.711       0.73      0.729          0.73
precision    0.734    0.726       0.73      0.730          0.73
recall       0.761    0.697       0.73      0.729          0.73
support    523.000  475.000     998.00    998.000        998.00
Model Saved on  roc_auc 0.7836610647076583
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:25:40_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.428, BCE loss: 0.428, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 2 Step: 1 Total Loss: 0.432, BCE loss: 0.432, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 2 Step: 2 Total Loss: 0.527, BCE loss: 0.527, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 2 Step: 3 Total Loss: 0.484, BCE loss: 0.484, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 2 Step: 4 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 2 Step: 5 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 2 Step: 6 Total Loss: 0.409, BCE loss: 0.409, Diversity Loss: 0.255                     (Diversity_weight = 0)
Epoch: 2 Step: 7 Total Loss: 0.410, BCE loss: 0.410, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 2 Step: 8 Total Loss: 0.545, BCE loss: 0.545, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 2 Step: 9 Total Loss: 0.503, BCE loss: 0.503, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 2 Step: 10 Total Loss: 0.371, BCE loss: 0.371, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 11 Total Loss: 0.368, BCE loss: 0.368, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 2 Step: 12 Total Loss: 0.408, BCE loss: 0.408, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 2 Step: 13 Total Loss: 0.423, BCE loss: 0.423, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 2 Step: 14 Total Loss: 0.495, BCE loss: 0.495, Diversity Loss: 0.238                     (Diversity_weight = 0)
Epoch: 2 Step: 15 Total Loss: 0.494, BCE loss: 0.494, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 2 Step: 16 Total Loss: 0.441, BCE loss: 0.441, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 2 Step: 17 Total Loss: 0.524, BCE loss: 0.524, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 2 Step: 18 Total Loss: 0.534, BCE loss: 0.534, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 2 Step: 19 Total Loss: 0.335, BCE loss: 0.335, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 2 Step: 20 Total Loss: 0.422, BCE loss: 0.422, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 21 Total Loss: 0.357, BCE loss: 0.357, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 2 Step: 22 Total Loss: 0.497, BCE loss: 0.497, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 2 Step: 23 Total Loss: 0.429, BCE loss: 0.429, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 2 Step: 24 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 2 Step: 25 Total Loss: 0.558, BCE loss: 0.558, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 2 Step: 26 Total Loss: 0.429, BCE loss: 0.429, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 2 Step: 27 Total Loss: 0.313, BCE loss: 0.313, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 2 Step: 28 Total Loss: 0.448, BCE loss: 0.448, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 2 Step: 29 Total Loss: 0.338, BCE loss: 0.338, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 2 Step: 30 Total Loss: 0.435, BCE loss: 0.435, Diversity Loss: 0.258                     (Diversity_weight = 0)
Epoch: 2 Step: 31 Total Loss: 0.397, BCE loss: 0.397, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 2 Step: 32 Total Loss: 0.579, BCE loss: 0.579, Diversity Loss: 0.338                     (Diversity_weight = 0)
Epoch: 2 Step: 33 Total Loss: 0.490, BCE loss: 0.490, Diversity Loss: 0.238                     (Diversity_weight = 0)
Epoch: 2 Step: 34 Total Loss: 0.355, BCE loss: 0.355, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 2 Step: 35 Total Loss: 0.472, BCE loss: 0.472, Diversity Loss: 0.290                     (Diversity_weight = 0)
Epoch: 2 Step: 36 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 2 Step: 37 Total Loss: 0.514, BCE loss: 0.514, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 2 Step: 38 Total Loss: 0.449, BCE loss: 0.449, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 2 Step: 39 Total Loss: 0.489, BCE loss: 0.489, Diversity Loss: 0.153                     (Diversity_weight = 0)
Epoch: 2 Step: 40 Total Loss: 0.385, BCE loss: 0.385, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 2 Step: 41 Total Loss: 0.489, BCE loss: 0.489, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 2 Step: 42 Total Loss: 0.447, BCE loss: 0.447, Diversity Loss: 0.228                     (Diversity_weight = 0)
Epoch: 2 Step: 43 Total Loss: 0.738, BCE loss: 0.738, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 44 Total Loss: 0.481, BCE loss: 0.481, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 2 Step: 45 Total Loss: 0.349, BCE loss: 0.349, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 2 Step: 46 Total Loss: 0.495, BCE loss: 0.495, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 47 Total Loss: 0.382, BCE loss: 0.382, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 2 Step: 48 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 2 Step: 49 Total Loss: 0.609, BCE loss: 0.609, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 2 Step: 50 Total Loss: 0.485, BCE loss: 0.485, Diversity Loss: 0.143                     (Diversity_weight = 0)
Epoch: 2 Step: 51 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 2 Step: 52 Total Loss: 0.467, BCE loss: 0.467, Diversity Loss: 0.137                     (Diversity_weight = 0)
Epoch: 2 Step: 53 Total Loss: 0.602, BCE loss: 0.602, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 2 Step: 54 Total Loss: 0.415, BCE loss: 0.415, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 2 Step: 55 Total Loss: 0.504, BCE loss: 0.504, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 2 Step: 56 Total Loss: 0.265, BCE loss: 0.265, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 2 Step: 57 Total Loss: 0.512, BCE loss: 0.512, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 2 Step: 58 Total Loss: 0.375, BCE loss: 0.375, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 2 Step: 59 Total Loss: 0.455, BCE loss: 0.455, Diversity Loss: 0.260                     (Diversity_weight = 0)
Epoch: 2 Step: 60 Total Loss: 0.479, BCE loss: 0.479, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 2 Step: 61 Total Loss: 0.571, BCE loss: 0.571, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 2 Step: 62 Total Loss: 0.404, BCE loss: 0.404, Diversity Loss: 0.200                     (Diversity_weight = 0)
{'accuracy': 0.718436873747495, 'roc_auc': 0.7862050920801047, 'pr_auc': 0.7485416400295155, 'conicity_mean': 0.19589637, 'conicity_std': 0.04591882}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.716    0.721      0.718      0.718         0.718
precision    0.760    0.682      0.718      0.721         0.723
recall       0.677    0.764      0.718      0.721         0.718
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7862050920801047
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:25:40_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.333, BCE loss: 0.333, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 3 Step: 1 Total Loss: 0.285, BCE loss: 0.285, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 3 Step: 2 Total Loss: 0.364, BCE loss: 0.364, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 3 Step: 3 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 3 Step: 4 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 3 Step: 5 Total Loss: 0.297, BCE loss: 0.297, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 3 Step: 6 Total Loss: 0.235, BCE loss: 0.235, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 3 Step: 7 Total Loss: 0.355, BCE loss: 0.355, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 3 Step: 8 Total Loss: 0.318, BCE loss: 0.318, Diversity Loss: 0.245                     (Diversity_weight = 0)
Epoch: 3 Step: 9 Total Loss: 0.302, BCE loss: 0.302, Diversity Loss: 0.237                     (Diversity_weight = 0)
Epoch: 3 Step: 10 Total Loss: 0.325, BCE loss: 0.325, Diversity Loss: 0.204                     (Diversity_weight = 0)
Epoch: 3 Step: 11 Total Loss: 0.165, BCE loss: 0.165, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 12 Total Loss: 0.494, BCE loss: 0.494, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 3 Step: 13 Total Loss: 0.420, BCE loss: 0.420, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 3 Step: 14 Total Loss: 0.410, BCE loss: 0.410, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 3 Step: 15 Total Loss: 0.282, BCE loss: 0.282, Diversity Loss: 0.228                     (Diversity_weight = 0)
Epoch: 3 Step: 16 Total Loss: 0.274, BCE loss: 0.274, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 3 Step: 17 Total Loss: 0.563, BCE loss: 0.563, Diversity Loss: 0.244                     (Diversity_weight = 0)
Epoch: 3 Step: 18 Total Loss: 0.428, BCE loss: 0.428, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 3 Step: 19 Total Loss: 0.287, BCE loss: 0.287, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 3 Step: 20 Total Loss: 0.345, BCE loss: 0.345, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 3 Step: 21 Total Loss: 0.296, BCE loss: 0.296, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 3 Step: 22 Total Loss: 0.329, BCE loss: 0.329, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 3 Step: 23 Total Loss: 0.307, BCE loss: 0.307, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 3 Step: 24 Total Loss: 0.181, BCE loss: 0.181, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 3 Step: 25 Total Loss: 0.184, BCE loss: 0.184, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 3 Step: 26 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.236                     (Diversity_weight = 0)
Epoch: 3 Step: 27 Total Loss: 0.392, BCE loss: 0.392, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 3 Step: 28 Total Loss: 0.352, BCE loss: 0.352, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 3 Step: 29 Total Loss: 0.450, BCE loss: 0.450, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 3 Step: 30 Total Loss: 0.230, BCE loss: 0.230, Diversity Loss: 0.263                     (Diversity_weight = 0)
Epoch: 3 Step: 31 Total Loss: 0.259, BCE loss: 0.259, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 3 Step: 32 Total Loss: 0.283, BCE loss: 0.283, Diversity Loss: 0.258                     (Diversity_weight = 0)
Epoch: 3 Step: 33 Total Loss: 0.588, BCE loss: 0.588, Diversity Loss: 0.136                     (Diversity_weight = 0)
Epoch: 3 Step: 34 Total Loss: 0.395, BCE loss: 0.395, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 3 Step: 35 Total Loss: 0.491, BCE loss: 0.491, Diversity Loss: 0.284                     (Diversity_weight = 0)
Epoch: 3 Step: 36 Total Loss: 0.449, BCE loss: 0.449, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 3 Step: 37 Total Loss: 0.396, BCE loss: 0.396, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 3 Step: 38 Total Loss: 0.461, BCE loss: 0.461, Diversity Loss: 0.190                     (Diversity_weight = 0)
Epoch: 3 Step: 39 Total Loss: 0.286, BCE loss: 0.286, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 3 Step: 40 Total Loss: 0.324, BCE loss: 0.324, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 3 Step: 41 Total Loss: 0.506, BCE loss: 0.506, Diversity Loss: 0.330                     (Diversity_weight = 0)
Epoch: 3 Step: 42 Total Loss: 0.378, BCE loss: 0.378, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 3 Step: 43 Total Loss: 0.208, BCE loss: 0.208, Diversity Loss: 0.123                     (Diversity_weight = 0)
Epoch: 3 Step: 44 Total Loss: 0.301, BCE loss: 0.301, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 3 Step: 45 Total Loss: 0.374, BCE loss: 0.374, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 3 Step: 46 Total Loss: 0.354, BCE loss: 0.354, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 3 Step: 47 Total Loss: 0.371, BCE loss: 0.371, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 3 Step: 48 Total Loss: 0.418, BCE loss: 0.418, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 3 Step: 49 Total Loss: 0.428, BCE loss: 0.428, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 3 Step: 50 Total Loss: 0.330, BCE loss: 0.330, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 3 Step: 51 Total Loss: 0.355, BCE loss: 0.355, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 3 Step: 52 Total Loss: 0.285, BCE loss: 0.285, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 3 Step: 53 Total Loss: 0.239, BCE loss: 0.239, Diversity Loss: 0.210                     (Diversity_weight = 0)
Epoch: 3 Step: 54 Total Loss: 0.337, BCE loss: 0.337, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 3 Step: 55 Total Loss: 0.525, BCE loss: 0.525, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 3 Step: 56 Total Loss: 0.374, BCE loss: 0.374, Diversity Loss: 0.198                     (Diversity_weight = 0)
Epoch: 3 Step: 57 Total Loss: 0.413, BCE loss: 0.413, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 3 Step: 58 Total Loss: 0.293, BCE loss: 0.293, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 3 Step: 59 Total Loss: 0.222, BCE loss: 0.222, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 3 Step: 60 Total Loss: 0.246, BCE loss: 0.246, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 3 Step: 61 Total Loss: 0.265, BCE loss: 0.265, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 3 Step: 62 Total Loss: 0.497, BCE loss: 0.497, Diversity Loss: 0.187                     (Diversity_weight = 0)
{'accuracy': 0.7194388777555111, 'roc_auc': 0.784260843312871, 'pr_auc': 0.7430893322177419, 'conicity_mean': 0.1960125, 'conicity_std': 0.045409247}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.713    0.726      0.719      0.719         0.719
precision    0.769    0.678      0.719      0.724         0.726
recall       0.663    0.781      0.719      0.722         0.719
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.784260843312871
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:25:40_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.080, BCE loss: 0.080, Diversity Loss: 0.229                     (Diversity_weight = 0)
Epoch: 4 Step: 1 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 4 Step: 2 Total Loss: 0.162, BCE loss: 0.162, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 4 Step: 3 Total Loss: 0.233, BCE loss: 0.233, Diversity Loss: 0.334                     (Diversity_weight = 0)
Epoch: 4 Step: 4 Total Loss: 0.119, BCE loss: 0.119, Diversity Loss: 0.120                     (Diversity_weight = 0)
Epoch: 4 Step: 5 Total Loss: 0.189, BCE loss: 0.189, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 4 Step: 6 Total Loss: 0.233, BCE loss: 0.233, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 4 Step: 7 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 4 Step: 8 Total Loss: 0.204, BCE loss: 0.204, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 4 Step: 9 Total Loss: 0.291, BCE loss: 0.291, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 4 Step: 10 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 4 Step: 11 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 4 Step: 12 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 4 Step: 13 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 4 Step: 14 Total Loss: 0.309, BCE loss: 0.309, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 4 Step: 15 Total Loss: 0.366, BCE loss: 0.366, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 4 Step: 16 Total Loss: 0.202, BCE loss: 0.202, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 4 Step: 17 Total Loss: 0.382, BCE loss: 0.382, Diversity Loss: 0.237                     (Diversity_weight = 0)
Epoch: 4 Step: 18 Total Loss: 0.276, BCE loss: 0.276, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 4 Step: 19 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 4 Step: 20 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 4 Step: 21 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 4 Step: 22 Total Loss: 0.370, BCE loss: 0.370, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 4 Step: 23 Total Loss: 0.299, BCE loss: 0.299, Diversity Loss: 0.174                     (Diversity_weight = 0)
Epoch: 4 Step: 24 Total Loss: 0.334, BCE loss: 0.334, Diversity Loss: 0.283                     (Diversity_weight = 0)
Epoch: 4 Step: 25 Total Loss: 0.130, BCE loss: 0.130, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 4 Step: 26 Total Loss: 0.300, BCE loss: 0.300, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 4 Step: 27 Total Loss: 0.122, BCE loss: 0.122, Diversity Loss: 0.246                     (Diversity_weight = 0)
Epoch: 4 Step: 28 Total Loss: 0.450, BCE loss: 0.450, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 4 Step: 29 Total Loss: 0.209, BCE loss: 0.209, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 4 Step: 30 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 4 Step: 31 Total Loss: 0.386, BCE loss: 0.386, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 4 Step: 32 Total Loss: 0.273, BCE loss: 0.273, Diversity Loss: 0.201                     (Diversity_weight = 0)
Epoch: 4 Step: 33 Total Loss: 0.247, BCE loss: 0.247, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 4 Step: 34 Total Loss: 0.182, BCE loss: 0.182, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 4 Step: 35 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 4 Step: 36 Total Loss: 0.476, BCE loss: 0.476, Diversity Loss: 0.221                     (Diversity_weight = 0)
Epoch: 4 Step: 37 Total Loss: 0.493, BCE loss: 0.493, Diversity Loss: 0.132                     (Diversity_weight = 0)
Epoch: 4 Step: 38 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 4 Step: 39 Total Loss: 0.243, BCE loss: 0.243, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 4 Step: 40 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 4 Step: 41 Total Loss: 0.227, BCE loss: 0.227, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 4 Step: 42 Total Loss: 0.310, BCE loss: 0.310, Diversity Loss: 0.218                     (Diversity_weight = 0)
Epoch: 4 Step: 43 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 4 Step: 44 Total Loss: 0.205, BCE loss: 0.205, Diversity Loss: 0.231                     (Diversity_weight = 0)
Epoch: 4 Step: 45 Total Loss: 0.304, BCE loss: 0.304, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 4 Step: 46 Total Loss: 0.274, BCE loss: 0.274, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 4 Step: 47 Total Loss: 0.283, BCE loss: 0.283, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 4 Step: 48 Total Loss: 0.147, BCE loss: 0.147, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 4 Step: 49 Total Loss: 0.171, BCE loss: 0.171, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 4 Step: 50 Total Loss: 0.187, BCE loss: 0.187, Diversity Loss: 0.250                     (Diversity_weight = 0)
Epoch: 4 Step: 51 Total Loss: 0.137, BCE loss: 0.137, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 4 Step: 52 Total Loss: 0.134, BCE loss: 0.134, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 4 Step: 53 Total Loss: 0.475, BCE loss: 0.475, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 4 Step: 54 Total Loss: 0.269, BCE loss: 0.269, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 4 Step: 55 Total Loss: 0.370, BCE loss: 0.370, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 4 Step: 56 Total Loss: 0.226, BCE loss: 0.226, Diversity Loss: 0.144                     (Diversity_weight = 0)
Epoch: 4 Step: 57 Total Loss: 0.412, BCE loss: 0.412, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 4 Step: 58 Total Loss: 0.203, BCE loss: 0.203, Diversity Loss: 0.258                     (Diversity_weight = 0)
Epoch: 4 Step: 59 Total Loss: 0.290, BCE loss: 0.290, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 4 Step: 60 Total Loss: 0.403, BCE loss: 0.403, Diversity Loss: 0.157                     (Diversity_weight = 0)
Epoch: 4 Step: 61 Total Loss: 0.265, BCE loss: 0.265, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 4 Step: 62 Total Loss: 0.342, BCE loss: 0.342, Diversity Loss: 0.142                     (Diversity_weight = 0)
{'accuracy': 0.7124248496993988, 'roc_auc': 0.7823850256616686, 'pr_auc': 0.7401021124001774, 'conicity_mean': 0.19114874, 'conicity_std': 0.04436976}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.687    0.734      0.712      0.711         0.709
precision    0.799    0.656      0.712      0.728         0.731
recall       0.602    0.834      0.712      0.718         0.712
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7823850256616686
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:25:40_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.139, BCE loss: 0.139, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 5 Step: 1 Total Loss: 0.198, BCE loss: 0.198, Diversity Loss: 0.126                     (Diversity_weight = 0)
Epoch: 5 Step: 2 Total Loss: 0.209, BCE loss: 0.209, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 5 Step: 3 Total Loss: 0.075, BCE loss: 0.075, Diversity Loss: 0.212                     (Diversity_weight = 0)
Epoch: 5 Step: 4 Total Loss: 0.131, BCE loss: 0.131, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 5 Step: 5 Total Loss: 0.127, BCE loss: 0.127, Diversity Loss: 0.152                     (Diversity_weight = 0)
Epoch: 5 Step: 6 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.158                     (Diversity_weight = 0)
Epoch: 5 Step: 7 Total Loss: 0.127, BCE loss: 0.127, Diversity Loss: 0.166                     (Diversity_weight = 0)
Epoch: 5 Step: 8 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 5 Step: 9 Total Loss: 0.095, BCE loss: 0.095, Diversity Loss: 0.223                     (Diversity_weight = 0)
Epoch: 5 Step: 10 Total Loss: 0.194, BCE loss: 0.194, Diversity Loss: 0.160                     (Diversity_weight = 0)
Epoch: 5 Step: 11 Total Loss: 0.114, BCE loss: 0.114, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 5 Step: 12 Total Loss: 0.150, BCE loss: 0.150, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 5 Step: 13 Total Loss: 0.087, BCE loss: 0.087, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 5 Step: 14 Total Loss: 0.141, BCE loss: 0.141, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 5 Step: 15 Total Loss: 0.104, BCE loss: 0.104, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 5 Step: 16 Total Loss: 0.124, BCE loss: 0.124, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 5 Step: 17 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.250                     (Diversity_weight = 0)
Epoch: 5 Step: 18 Total Loss: 0.076, BCE loss: 0.076, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 5 Step: 19 Total Loss: 0.109, BCE loss: 0.109, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 5 Step: 20 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.222                     (Diversity_weight = 0)
Epoch: 5 Step: 21 Total Loss: 0.170, BCE loss: 0.170, Diversity Loss: 0.197                     (Diversity_weight = 0)
Epoch: 5 Step: 22 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 5 Step: 23 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 5 Step: 24 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 5 Step: 25 Total Loss: 0.093, BCE loss: 0.093, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 5 Step: 26 Total Loss: 0.050, BCE loss: 0.050, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 5 Step: 27 Total Loss: 0.130, BCE loss: 0.130, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 5 Step: 28 Total Loss: 0.148, BCE loss: 0.148, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 5 Step: 29 Total Loss: 0.209, BCE loss: 0.209, Diversity Loss: 0.145                     (Diversity_weight = 0)
Epoch: 5 Step: 30 Total Loss: 0.178, BCE loss: 0.178, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 5 Step: 31 Total Loss: 0.189, BCE loss: 0.189, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 5 Step: 32 Total Loss: 0.097, BCE loss: 0.097, Diversity Loss: 0.122                     (Diversity_weight = 0)
Epoch: 5 Step: 33 Total Loss: 0.084, BCE loss: 0.084, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 5 Step: 34 Total Loss: 0.135, BCE loss: 0.135, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 5 Step: 35 Total Loss: 0.054, BCE loss: 0.054, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 5 Step: 36 Total Loss: 0.157, BCE loss: 0.157, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 5 Step: 37 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 5 Step: 38 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.225                     (Diversity_weight = 0)
Epoch: 5 Step: 39 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.329                     (Diversity_weight = 0)
Epoch: 5 Step: 40 Total Loss: 0.264, BCE loss: 0.264, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 5 Step: 41 Total Loss: 0.099, BCE loss: 0.099, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 5 Step: 42 Total Loss: 0.154, BCE loss: 0.154, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 5 Step: 43 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.258                     (Diversity_weight = 0)
Epoch: 5 Step: 44 Total Loss: 0.068, BCE loss: 0.068, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 5 Step: 45 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 5 Step: 46 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 5 Step: 47 Total Loss: 0.073, BCE loss: 0.073, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 5 Step: 48 Total Loss: 0.341, BCE loss: 0.341, Diversity Loss: 0.271                     (Diversity_weight = 0)
Epoch: 5 Step: 49 Total Loss: 0.071, BCE loss: 0.071, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 5 Step: 50 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.244                     (Diversity_weight = 0)
Epoch: 5 Step: 51 Total Loss: 0.155, BCE loss: 0.155, Diversity Loss: 0.168                     (Diversity_weight = 0)
Epoch: 5 Step: 52 Total Loss: 0.149, BCE loss: 0.149, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 5 Step: 53 Total Loss: 0.120, BCE loss: 0.120, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 5 Step: 54 Total Loss: 0.199, BCE loss: 0.199, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 5 Step: 55 Total Loss: 0.121, BCE loss: 0.121, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 5 Step: 56 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 5 Step: 57 Total Loss: 0.183, BCE loss: 0.183, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 5 Step: 58 Total Loss: 0.081, BCE loss: 0.081, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 5 Step: 59 Total Loss: 0.161, BCE loss: 0.161, Diversity Loss: 0.163                     (Diversity_weight = 0)
Epoch: 5 Step: 60 Total Loss: 0.108, BCE loss: 0.108, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 5 Step: 61 Total Loss: 0.106, BCE loss: 0.106, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 5 Step: 62 Total Loss: 0.078, BCE loss: 0.078, Diversity Loss: 0.174                     (Diversity_weight = 0)
{'accuracy': 0.7084168336673347, 'roc_auc': 0.7654986414410788, 'pr_auc': 0.722332761913048, 'conicity_mean': 0.19050063, 'conicity_std': 0.044816613}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.708    0.709      0.708      0.708         0.708
precision    0.746    0.675      0.708      0.710         0.712
recall       0.673    0.747      0.708      0.710         0.708
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7654986414410788
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:25:40_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.031, BCE loss: 0.031, Diversity Loss: 0.215                     (Diversity_weight = 0)
Epoch: 6 Step: 1 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.207                     (Diversity_weight = 0)
Epoch: 6 Step: 2 Total Loss: 0.161, BCE loss: 0.161, Diversity Loss: 0.202                     (Diversity_weight = 0)
Epoch: 6 Step: 3 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.169                     (Diversity_weight = 0)
Epoch: 6 Step: 4 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 6 Step: 5 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.177                     (Diversity_weight = 0)
Epoch: 6 Step: 6 Total Loss: 0.052, BCE loss: 0.052, Diversity Loss: 0.189                     (Diversity_weight = 0)
Epoch: 6 Step: 7 Total Loss: 0.053, BCE loss: 0.053, Diversity Loss: 0.155                     (Diversity_weight = 0)
Epoch: 6 Step: 8 Total Loss: 0.132, BCE loss: 0.132, Diversity Loss: 0.146                     (Diversity_weight = 0)
Epoch: 6 Step: 9 Total Loss: 0.057, BCE loss: 0.057, Diversity Loss: 0.175                     (Diversity_weight = 0)
Epoch: 6 Step: 10 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 6 Step: 11 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 6 Step: 12 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 6 Step: 13 Total Loss: 0.163, BCE loss: 0.163, Diversity Loss: 0.276                     (Diversity_weight = 0)
Epoch: 6 Step: 14 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 6 Step: 15 Total Loss: 0.033, BCE loss: 0.033, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 6 Step: 16 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 6 Step: 17 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.118                     (Diversity_weight = 0)
Epoch: 6 Step: 18 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 6 Step: 19 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 6 Step: 20 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 6 Step: 21 Total Loss: 0.174, BCE loss: 0.174, Diversity Loss: 0.335                     (Diversity_weight = 0)
Epoch: 6 Step: 22 Total Loss: 0.107, BCE loss: 0.107, Diversity Loss: 0.224                     (Diversity_weight = 0)
Epoch: 6 Step: 23 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 6 Step: 24 Total Loss: 0.065, BCE loss: 0.065, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 6 Step: 25 Total Loss: 0.046, BCE loss: 0.046, Diversity Loss: 0.232                     (Diversity_weight = 0)
Epoch: 6 Step: 26 Total Loss: 0.037, BCE loss: 0.037, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 6 Step: 27 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 6 Step: 28 Total Loss: 0.100, BCE loss: 0.100, Diversity Loss: 0.217                     (Diversity_weight = 0)
Epoch: 6 Step: 29 Total Loss: 0.079, BCE loss: 0.079, Diversity Loss: 0.240                     (Diversity_weight = 0)
Epoch: 6 Step: 30 Total Loss: 0.049, BCE loss: 0.049, Diversity Loss: 0.248                     (Diversity_weight = 0)
Epoch: 6 Step: 31 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 6 Step: 32 Total Loss: 0.051, BCE loss: 0.051, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 6 Step: 33 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 6 Step: 34 Total Loss: 0.025, BCE loss: 0.025, Diversity Loss: 0.149                     (Diversity_weight = 0)
Epoch: 6 Step: 35 Total Loss: 0.230, BCE loss: 0.230, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 6 Step: 36 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.141                     (Diversity_weight = 0)
Epoch: 6 Step: 37 Total Loss: 0.024, BCE loss: 0.024, Diversity Loss: 0.211                     (Diversity_weight = 0)
Epoch: 6 Step: 38 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.184                     (Diversity_weight = 0)
Epoch: 6 Step: 39 Total Loss: 0.070, BCE loss: 0.070, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 6 Step: 40 Total Loss: 0.082, BCE loss: 0.082, Diversity Loss: 0.229                     (Diversity_weight = 0)
Epoch: 6 Step: 41 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.138                     (Diversity_weight = 0)
Epoch: 6 Step: 42 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 6 Step: 43 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 6 Step: 44 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.214                     (Diversity_weight = 0)
Epoch: 6 Step: 45 Total Loss: 0.042, BCE loss: 0.042, Diversity Loss: 0.134                     (Diversity_weight = 0)
Epoch: 6 Step: 46 Total Loss: 0.032, BCE loss: 0.032, Diversity Loss: 0.165                     (Diversity_weight = 0)
Epoch: 6 Step: 47 Total Loss: 0.103, BCE loss: 0.103, Diversity Loss: 0.128                     (Diversity_weight = 0)
Epoch: 6 Step: 48 Total Loss: 0.036, BCE loss: 0.036, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 6 Step: 49 Total Loss: 0.175, BCE loss: 0.175, Diversity Loss: 0.167                     (Diversity_weight = 0)
Epoch: 6 Step: 50 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 6 Step: 51 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.186                     (Diversity_weight = 0)
Epoch: 6 Step: 52 Total Loss: 0.029, BCE loss: 0.029, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 6 Step: 53 Total Loss: 0.074, BCE loss: 0.074, Diversity Loss: 0.265                     (Diversity_weight = 0)
Epoch: 6 Step: 54 Total Loss: 0.021, BCE loss: 0.021, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 6 Step: 55 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.181                     (Diversity_weight = 0)
Epoch: 6 Step: 56 Total Loss: 0.014, BCE loss: 0.014, Diversity Loss: 0.208                     (Diversity_weight = 0)
Epoch: 6 Step: 57 Total Loss: 0.060, BCE loss: 0.060, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 6 Step: 58 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 6 Step: 59 Total Loss: 0.040, BCE loss: 0.040, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 6 Step: 60 Total Loss: 0.011, BCE loss: 0.011, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 6 Step: 61 Total Loss: 0.043, BCE loss: 0.043, Diversity Loss: 0.171                     (Diversity_weight = 0)
Epoch: 6 Step: 62 Total Loss: 0.067, BCE loss: 0.067, Diversity Loss: 0.221                     (Diversity_weight = 0)
{'accuracy': 0.7154308617234469, 'roc_auc': 0.7627533460803059, 'pr_auc': 0.7131830888249897, 'conicity_mean': 0.1891308, 'conicity_std': 0.044854086}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.721    0.710      0.715      0.715         0.716
precision    0.741    0.690      0.715      0.716         0.717
recall       0.702    0.731      0.715      0.716         0.715
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7627533460803059
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:25:40_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.192                     (Diversity_weight = 0)
Epoch: 7 Step: 1 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.156                     (Diversity_weight = 0)
Epoch: 7 Step: 2 Total Loss: 0.126, BCE loss: 0.126, Diversity Loss: 0.332                     (Diversity_weight = 0)
Epoch: 7 Step: 3 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.193                     (Diversity_weight = 0)
Epoch: 7 Step: 4 Total Loss: 0.030, BCE loss: 0.030, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 7 Step: 5 Total Loss: 0.111, BCE loss: 0.111, Diversity Loss: 0.273                     (Diversity_weight = 0)
Epoch: 7 Step: 6 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.129                     (Diversity_weight = 0)
Epoch: 7 Step: 7 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 7 Step: 8 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 7 Step: 9 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 7 Step: 10 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 7 Step: 11 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.150                     (Diversity_weight = 0)
Epoch: 7 Step: 12 Total Loss: 0.006, BCE loss: 0.006, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 7 Step: 13 Total Loss: 0.010, BCE loss: 0.010, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 7 Step: 14 Total Loss: 0.035, BCE loss: 0.035, Diversity Loss: 0.127                     (Diversity_weight = 0)
Epoch: 7 Step: 15 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.253                     (Diversity_weight = 0)
Epoch: 7 Step: 16 Total Loss: 0.004, BCE loss: 0.004, Diversity Loss: 0.176                     (Diversity_weight = 0)
Epoch: 7 Step: 17 Total Loss: 0.008, BCE loss: 0.008, Diversity Loss: 0.182                     (Diversity_weight = 0)
Epoch: 7 Step: 18 Total Loss: 0.006, BCE loss: 0.006, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 7 Step: 19 Total Loss: 0.028, BCE loss: 0.028, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 7 Step: 20 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 7 Step: 21 Total Loss: 0.090, BCE loss: 0.090, Diversity Loss: 0.139                     (Diversity_weight = 0)
Epoch: 7 Step: 22 Total Loss: 0.077, BCE loss: 0.077, Diversity Loss: 0.227                     (Diversity_weight = 0)
Epoch: 7 Step: 23 Total Loss: 0.018, BCE loss: 0.018, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 7 Step: 24 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 7 Step: 25 Total Loss: 0.011, BCE loss: 0.011, Diversity Loss: 0.173                     (Diversity_weight = 0)
Epoch: 7 Step: 26 Total Loss: 0.017, BCE loss: 0.017, Diversity Loss: 0.209                     (Diversity_weight = 0)
Epoch: 7 Step: 27 Total Loss: 0.056, BCE loss: 0.056, Diversity Loss: 0.235                     (Diversity_weight = 0)
Epoch: 7 Step: 28 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 7 Step: 29 Total Loss: 0.007, BCE loss: 0.007, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 7 Step: 30 Total Loss: 0.006, BCE loss: 0.006, Diversity Loss: 0.164                     (Diversity_weight = 0)
Epoch: 7 Step: 31 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.172                     (Diversity_weight = 0)
Epoch: 7 Step: 32 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 7 Step: 33 Total Loss: 0.007, BCE loss: 0.007, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 7 Step: 34 Total Loss: 0.006, BCE loss: 0.006, Diversity Loss: 0.162                     (Diversity_weight = 0)
Epoch: 7 Step: 35 Total Loss: 0.102, BCE loss: 0.102, Diversity Loss: 0.148                     (Diversity_weight = 0)
Epoch: 7 Step: 36 Total Loss: 0.011, BCE loss: 0.011, Diversity Loss: 0.260                     (Diversity_weight = 0)
Epoch: 7 Step: 37 Total Loss: 0.152, BCE loss: 0.152, Diversity Loss: 0.205                     (Diversity_weight = 0)
Epoch: 7 Step: 38 Total Loss: 0.039, BCE loss: 0.039, Diversity Loss: 0.178                     (Diversity_weight = 0)
Epoch: 7 Step: 39 Total Loss: 0.007, BCE loss: 0.007, Diversity Loss: 0.206                     (Diversity_weight = 0)
Epoch: 7 Step: 40 Total Loss: 0.008, BCE loss: 0.008, Diversity Loss: 0.179                     (Diversity_weight = 0)
Epoch: 7 Step: 41 Total Loss: 0.016, BCE loss: 0.016, Diversity Loss: 0.200                     (Diversity_weight = 0)
Epoch: 7 Step: 42 Total Loss: 0.022, BCE loss: 0.022, Diversity Loss: 0.213                     (Diversity_weight = 0)
Epoch: 7 Step: 43 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.194                     (Diversity_weight = 0)
Epoch: 7 Step: 44 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.188                     (Diversity_weight = 0)
Epoch: 7 Step: 45 Total Loss: 0.005, BCE loss: 0.005, Diversity Loss: 0.216                     (Diversity_weight = 0)
Epoch: 7 Step: 46 Total Loss: 0.008, BCE loss: 0.008, Diversity Loss: 0.230                     (Diversity_weight = 0)
Epoch: 7 Step: 47 Total Loss: 0.195, BCE loss: 0.195, Diversity Loss: 0.170                     (Diversity_weight = 0)
Epoch: 7 Step: 48 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.183                     (Diversity_weight = 0)
Epoch: 7 Step: 49 Total Loss: 0.041, BCE loss: 0.041, Diversity Loss: 0.161                     (Diversity_weight = 0)
Epoch: 7 Step: 50 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.185                     (Diversity_weight = 0)
Epoch: 7 Step: 51 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.203                     (Diversity_weight = 0)
Epoch: 7 Step: 52 Total Loss: 0.011, BCE loss: 0.011, Diversity Loss: 0.140                     (Diversity_weight = 0)
Epoch: 7 Step: 53 Total Loss: 0.069, BCE loss: 0.069, Diversity Loss: 0.191                     (Diversity_weight = 0)
Epoch: 7 Step: 54 Total Loss: 0.005, BCE loss: 0.005, Diversity Loss: 0.117                     (Diversity_weight = 0)
Epoch: 7 Step: 55 Total Loss: 0.014, BCE loss: 0.014, Diversity Loss: 0.195                     (Diversity_weight = 0)
Epoch: 7 Step: 56 Total Loss: 0.009, BCE loss: 0.009, Diversity Loss: 0.220                     (Diversity_weight = 0)
Epoch: 7 Step: 57 Total Loss: 0.020, BCE loss: 0.020, Diversity Loss: 0.196                     (Diversity_weight = 0)
Epoch: 7 Step: 58 Total Loss: 0.034, BCE loss: 0.034, Diversity Loss: 0.199                     (Diversity_weight = 0)
Epoch: 7 Step: 59 Total Loss: 0.023, BCE loss: 0.023, Diversity Loss: 0.219                     (Diversity_weight = 0)
Epoch: 7 Step: 60 Total Loss: 0.013, BCE loss: 0.013, Diversity Loss: 0.159                     (Diversity_weight = 0)
Epoch: 7 Step: 61 Total Loss: 0.015, BCE loss: 0.015, Diversity Loss: 0.187                     (Diversity_weight = 0)
Epoch: 7 Step: 62 Total Loss: 0.019, BCE loss: 0.019, Diversity Loss: 0.181                     (Diversity_weight = 0)
{'accuracy': 0.7044088176352705, 'roc_auc': 0.7516171882862032, 'pr_auc': 0.7002767459787258, 'conicity_mean': 0.19153997, 'conicity_std': 0.045233987}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.711    0.697      0.704      0.704         0.705
precision    0.729    0.680      0.704      0.704         0.706
recall       0.694    0.716      0.704      0.705         0.704
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7516171882862032
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
./experiments/cls_jp/cls_jp/ortho_lstm+tanh/Sun_Jan_17_19:25:40_2021
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:85: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/ortholstm.py:86: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:26:59,211 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:26:59,211 - type = ortholstm
INFO - 2021-01-17 19:26:59,211 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:26:59,211 - vocab_size = 548
INFO - 2021-01-17 19:26:59,211 - embed_size = 200
INFO - 2021-01-17 19:26:59,212 - hidden_size = 128
INFO - 2021-01-17 19:26:59,212 - pre_embed = None
INFO - 2021-01-17 19:26:59,225 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:26:59,225 - hidden_size = 256
INFO - 2021-01-17 19:26:59,226 - output_size = 1
INFO - 2021-01-17 19:26:59,226 - use_attention = True
INFO - 2021-01-17 19:26:59,226 - regularizer_attention = None
INFO - 2021-01-17 19:26:59,226 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b7a51599e10> and extras set()
INFO - 2021-01-17 19:26:59,226 - attention.type = tanh
INFO - 2021-01-17 19:26:59,226 - type = tanh
INFO - 2021-01-17 19:26:59,226 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b7a51599e10> and extras set()
INFO - 2021-01-17 19:26:59,226 - attention.hidden_size = 256
INFO - 2021-01-17 19:26:59,226 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7387387387387387, 'roc_auc': 0.8094053836882282, 'pr_auc': 0.8154376948133961, 'conicity_mean': '0.19425648', 'conicity_std': '0.041196447'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.730    0.747      0.739      0.738         0.739
precision    0.717    0.759      0.739      0.738         0.739
recall       0.743    0.735      0.739      0.739         0.739
support    475.000  524.000    999.000    999.000       999.000
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:27:01,005 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'ortholstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:27:01,005 - type = ortholstm
INFO - 2021-01-17 19:27:01,005 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:27:01,005 - vocab_size = 548
INFO - 2021-01-17 19:27:01,005 - embed_size = 200
INFO - 2021-01-17 19:27:01,005 - hidden_size = 128
INFO - 2021-01-17 19:27:01,005 - pre_embed = None
INFO - 2021-01-17 19:27:01,018 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:27:01,019 - hidden_size = 256
INFO - 2021-01-17 19:27:01,019 - output_size = 1
INFO - 2021-01-17 19:27:01,019 - use_attention = True
INFO - 2021-01-17 19:27:01,019 - regularizer_attention = None
INFO - 2021-01-17 19:27:01,019 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b7a3513b990> and extras set()
INFO - 2021-01-17 19:27:01,019 - attention.type = tanh
INFO - 2021-01-17 19:27:01,019 - type = tanh
INFO - 2021-01-17 19:27:01,019 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b7a3513b990> and extras set()
INFO - 2021-01-17 19:27:01,019 - attention.hidden_size = 256
INFO - 2021-01-17 19:27:01,019 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/ortho_lstm+tanh'}}
Running on device: cuda:0
{'accuracy': 0.7387387387387387, 'roc_auc': 0.8094053836882282, 'pr_auc': 0.8154376948133961, 'conicity_mean': '0.19425648', 'conicity_std': '0.041196447'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.730    0.747      0.739      0.738         0.739
precision    0.717    0.759      0.739      0.738         0.739
recall       0.743    0.735      0.739      0.739         0.739
support    475.000  524.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

cls_jp diversity_lstm
============================================================================================================================================================================================================================================================================================================
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:27:12,808 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:27:12,809 - type = vanillalstm
INFO - 2021-01-17 19:27:12,809 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:27:12,824 - Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
INFO - 2021-01-17 19:27:13,146 - vocab_size = 548
INFO - 2021-01-17 19:27:13,146 - embed_size = 200
INFO - 2021-01-17 19:27:13,146 - hidden_size = 128
INFO - 2021-01-17 19:27:13,146 - pre_embed = None
INFO - 2021-01-17 19:27:16,236 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:27:16,237 - hidden_size = 256
INFO - 2021-01-17 19:27:16,237 - output_size = 1
INFO - 2021-01-17 19:27:16,237 - use_attention = True
INFO - 2021-01-17 19:27:16,237 - regularizer_attention = None
INFO - 2021-01-17 19:27:16,237 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b35d263a290> and extras set()
INFO - 2021-01-17 19:27:16,237 - attention.type = tanh
INFO - 2021-01-17 19:27:16,237 - type = tanh
INFO - 2021-01-17 19:27:16,237 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b35d263a290> and extras set()
INFO - 2021-01-17 19:27:16,238 - attention.hidden_size = 256
INFO - 2021-01-17 19:27:16,238 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
Starting Epoch: 0
Epoch: 0 Step: 0 Total Loss: 0.949, BCE loss: 0.699, Diversity Loss: 0.501                     (Diversity_weight = 0.5)
Epoch: 0 Step: 1 Total Loss: 0.968, BCE loss: 0.729, Diversity Loss: 0.476                     (Diversity_weight = 0.5)
Epoch: 0 Step: 2 Total Loss: 0.894, BCE loss: 0.689, Diversity Loss: 0.410                     (Diversity_weight = 0.5)
Epoch: 0 Step: 3 Total Loss: 0.921, BCE loss: 0.688, Diversity Loss: 0.467                     (Diversity_weight = 0.5)
Epoch: 0 Step: 4 Total Loss: 0.861, BCE loss: 0.692, Diversity Loss: 0.338                     (Diversity_weight = 0.5)
Epoch: 0 Step: 5 Total Loss: 0.837, BCE loss: 0.692, Diversity Loss: 0.290                     (Diversity_weight = 0.5)
Epoch: 0 Step: 6 Total Loss: 0.848, BCE loss: 0.694, Diversity Loss: 0.308                     (Diversity_weight = 0.5)
Epoch: 0 Step: 7 Total Loss: 0.806, BCE loss: 0.691, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 0 Step: 8 Total Loss: 0.803, BCE loss: 0.689, Diversity Loss: 0.227                     (Diversity_weight = 0.5)
Epoch: 0 Step: 9 Total Loss: 0.856, BCE loss: 0.684, Diversity Loss: 0.345                     (Diversity_weight = 0.5)
Epoch: 0 Step: 10 Total Loss: 0.816, BCE loss: 0.694, Diversity Loss: 0.244                     (Diversity_weight = 0.5)
Epoch: 0 Step: 11 Total Loss: 0.805, BCE loss: 0.693, Diversity Loss: 0.224                     (Diversity_weight = 0.5)
Epoch: 0 Step: 12 Total Loss: 0.793, BCE loss: 0.695, Diversity Loss: 0.197                     (Diversity_weight = 0.5)
Epoch: 0 Step: 13 Total Loss: 0.792, BCE loss: 0.684, Diversity Loss: 0.215                     (Diversity_weight = 0.5)
Epoch: 0 Step: 14 Total Loss: 0.792, BCE loss: 0.691, Diversity Loss: 0.203                     (Diversity_weight = 0.5)
Epoch: 0 Step: 15 Total Loss: 0.773, BCE loss: 0.691, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 0 Step: 16 Total Loss: 0.805, BCE loss: 0.690, Diversity Loss: 0.230                     (Diversity_weight = 0.5)
Epoch: 0 Step: 17 Total Loss: 0.866, BCE loss: 0.680, Diversity Loss: 0.371                     (Diversity_weight = 0.5)
Epoch: 0 Step: 18 Total Loss: 0.798, BCE loss: 0.691, Diversity Loss: 0.214                     (Diversity_weight = 0.5)
Epoch: 0 Step: 19 Total Loss: 0.793, BCE loss: 0.693, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 0 Step: 20 Total Loss: 0.796, BCE loss: 0.672, Diversity Loss: 0.248                     (Diversity_weight = 0.5)
Epoch: 0 Step: 21 Total Loss: 0.773, BCE loss: 0.689, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 0 Step: 22 Total Loss: 0.779, BCE loss: 0.676, Diversity Loss: 0.205                     (Diversity_weight = 0.5)
Epoch: 0 Step: 23 Total Loss: 0.805, BCE loss: 0.669, Diversity Loss: 0.272                     (Diversity_weight = 0.5)
Epoch: 0 Step: 24 Total Loss: 0.851, BCE loss: 0.671, Diversity Loss: 0.361                     (Diversity_weight = 0.5)
Epoch: 0 Step: 25 Total Loss: 0.772, BCE loss: 0.687, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 0 Step: 26 Total Loss: 0.781, BCE loss: 0.681, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 0 Step: 27 Total Loss: 0.777, BCE loss: 0.694, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 0 Step: 28 Total Loss: 0.778, BCE loss: 0.692, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 0 Step: 29 Total Loss: 0.783, BCE loss: 0.681, Diversity Loss: 0.204                     (Diversity_weight = 0.5)
Epoch: 0 Step: 30 Total Loss: 0.764, BCE loss: 0.677, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 0 Step: 31 Total Loss: 0.797, BCE loss: 0.689, Diversity Loss: 0.214                     (Diversity_weight = 0.5)
Epoch: 0 Step: 32 Total Loss: 0.801, BCE loss: 0.674, Diversity Loss: 0.254                     (Diversity_weight = 0.5)
Epoch: 0 Step: 33 Total Loss: 0.762, BCE loss: 0.688, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 0 Step: 34 Total Loss: 0.780, BCE loss: 0.677, Diversity Loss: 0.205                     (Diversity_weight = 0.5)
Epoch: 0 Step: 35 Total Loss: 0.793, BCE loss: 0.688, Diversity Loss: 0.209                     (Diversity_weight = 0.5)
Epoch: 0 Step: 36 Total Loss: 0.773, BCE loss: 0.675, Diversity Loss: 0.196                     (Diversity_weight = 0.5)
Epoch: 0 Step: 37 Total Loss: 0.876, BCE loss: 0.667, Diversity Loss: 0.419                     (Diversity_weight = 0.5)
Epoch: 0 Step: 38 Total Loss: 0.751, BCE loss: 0.659, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 0 Step: 39 Total Loss: 0.757, BCE loss: 0.686, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 0 Step: 40 Total Loss: 0.763, BCE loss: 0.673, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 0 Step: 41 Total Loss: 0.745, BCE loss: 0.657, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 0 Step: 42 Total Loss: 0.761, BCE loss: 0.633, Diversity Loss: 0.256                     (Diversity_weight = 0.5)
Epoch: 0 Step: 43 Total Loss: 0.767, BCE loss: 0.650, Diversity Loss: 0.235                     (Diversity_weight = 0.5)
Epoch: 0 Step: 44 Total Loss: 0.790, BCE loss: 0.723, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 0 Step: 45 Total Loss: 0.771, BCE loss: 0.704, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 0 Step: 46 Total Loss: 0.789, BCE loss: 0.703, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 0 Step: 47 Total Loss: 0.756, BCE loss: 0.669, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 0 Step: 48 Total Loss: 0.736, BCE loss: 0.672, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 0 Step: 49 Total Loss: 0.754, BCE loss: 0.690, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 0 Step: 50 Total Loss: 0.753, BCE loss: 0.668, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 0 Step: 51 Total Loss: 0.735, BCE loss: 0.661, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 0 Step: 52 Total Loss: 0.742, BCE loss: 0.672, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 0 Step: 53 Total Loss: 0.742, BCE loss: 0.676, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 0 Step: 54 Total Loss: 0.747, BCE loss: 0.681, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 0 Step: 55 Total Loss: 0.731, BCE loss: 0.669, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 0 Step: 56 Total Loss: 0.742, BCE loss: 0.636, Diversity Loss: 0.212                     (Diversity_weight = 0.5)
Epoch: 0 Step: 57 Total Loss: 0.736, BCE loss: 0.665, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 0 Step: 58 Total Loss: 0.748, BCE loss: 0.649, Diversity Loss: 0.196                     (Diversity_weight = 0.5)
Epoch: 0 Step: 59 Total Loss: 0.709, BCE loss: 0.630, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 0 Step: 60 Total Loss: 0.731, BCE loss: 0.635, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 0 Step: 61 Total Loss: 0.783, BCE loss: 0.673, Diversity Loss: 0.220                     (Diversity_weight = 0.5)
Epoch: 0 Step: 62 Total Loss: 0.750, BCE loss: 0.666, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
{'accuracy': 0.6583166332665331, 'roc_auc': 0.7038261044580859, 'pr_auc': 0.6487840021924371, 'conicity_mean': 0.16614969, 'conicity_std': 0.058238916}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.638    0.676      0.658      0.657         0.656
precision    0.717    0.616      0.658      0.666         0.669
recall       0.576    0.749      0.658      0.662         0.658
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7038261044580859
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:27:16_2021
Starting Epoch: 1
Epoch: 1 Step: 0 Total Loss: 0.701, BCE loss: 0.613, Diversity Loss: 0.177                     (Diversity_weight = 0.5)
Epoch: 1 Step: 1 Total Loss: 0.759, BCE loss: 0.605, Diversity Loss: 0.307                     (Diversity_weight = 0.5)
Epoch: 1 Step: 2 Total Loss: 0.729, BCE loss: 0.659, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 1 Step: 3 Total Loss: 0.655, BCE loss: 0.586, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 1 Step: 4 Total Loss: 0.651, BCE loss: 0.564, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 1 Step: 5 Total Loss: 0.625, BCE loss: 0.542, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 1 Step: 6 Total Loss: 0.744, BCE loss: 0.615, Diversity Loss: 0.257                     (Diversity_weight = 0.5)
Epoch: 1 Step: 7 Total Loss: 0.740, BCE loss: 0.634, Diversity Loss: 0.212                     (Diversity_weight = 0.5)
Epoch: 1 Step: 8 Total Loss: 0.676, BCE loss: 0.594, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 1 Step: 9 Total Loss: 0.649, BCE loss: 0.574, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 1 Step: 10 Total Loss: 0.764, BCE loss: 0.669, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 1 Step: 11 Total Loss: 0.712, BCE loss: 0.626, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 1 Step: 12 Total Loss: 0.713, BCE loss: 0.632, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 1 Step: 13 Total Loss: 0.633, BCE loss: 0.572, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 1 Step: 14 Total Loss: 0.901, BCE loss: 0.778, Diversity Loss: 0.245                     (Diversity_weight = 0.5)
Epoch: 1 Step: 15 Total Loss: 0.836, BCE loss: 0.733, Diversity Loss: 0.204                     (Diversity_weight = 0.5)
Epoch: 1 Step: 16 Total Loss: 0.701, BCE loss: 0.630, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 1 Step: 17 Total Loss: 0.677, BCE loss: 0.602, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 1 Step: 18 Total Loss: 0.714, BCE loss: 0.638, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 1 Step: 19 Total Loss: 0.717, BCE loss: 0.631, Diversity Loss: 0.172                     (Diversity_weight = 0.5)
Epoch: 1 Step: 20 Total Loss: 0.782, BCE loss: 0.701, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 1 Step: 21 Total Loss: 0.707, BCE loss: 0.633, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 1 Step: 22 Total Loss: 0.702, BCE loss: 0.624, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 1 Step: 23 Total Loss: 0.682, BCE loss: 0.590, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 1 Step: 24 Total Loss: 0.722, BCE loss: 0.655, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 1 Step: 25 Total Loss: 0.706, BCE loss: 0.639, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 1 Step: 26 Total Loss: 0.776, BCE loss: 0.691, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 1 Step: 27 Total Loss: 0.645, BCE loss: 0.573, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 1 Step: 28 Total Loss: 0.698, BCE loss: 0.621, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 1 Step: 29 Total Loss: 0.722, BCE loss: 0.667, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 1 Step: 30 Total Loss: 0.813, BCE loss: 0.650, Diversity Loss: 0.327                     (Diversity_weight = 0.5)
Epoch: 1 Step: 31 Total Loss: 0.737, BCE loss: 0.677, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 1 Step: 32 Total Loss: 0.706, BCE loss: 0.632, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 1 Step: 33 Total Loss: 0.774, BCE loss: 0.715, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 1 Step: 34 Total Loss: 0.686, BCE loss: 0.631, Diversity Loss: 0.109                     (Diversity_weight = 0.5)
Epoch: 1 Step: 35 Total Loss: 0.677, BCE loss: 0.610, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 1 Step: 36 Total Loss: 0.692, BCE loss: 0.630, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 1 Step: 37 Total Loss: 0.676, BCE loss: 0.594, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 1 Step: 38 Total Loss: 0.647, BCE loss: 0.544, Diversity Loss: 0.205                     (Diversity_weight = 0.5)
Epoch: 1 Step: 39 Total Loss: 0.721, BCE loss: 0.626, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 1 Step: 40 Total Loss: 0.666, BCE loss: 0.604, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 1 Step: 41 Total Loss: 0.723, BCE loss: 0.660, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 1 Step: 42 Total Loss: 0.726, BCE loss: 0.653, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 1 Step: 43 Total Loss: 0.666, BCE loss: 0.589, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 1 Step: 44 Total Loss: 0.612, BCE loss: 0.556, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 1 Step: 45 Total Loss: 0.679, BCE loss: 0.602, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 1 Step: 46 Total Loss: 0.713, BCE loss: 0.654, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 1 Step: 47 Total Loss: 0.575, BCE loss: 0.478, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 1 Step: 48 Total Loss: 0.730, BCE loss: 0.650, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 1 Step: 49 Total Loss: 0.523, BCE loss: 0.431, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 1 Step: 50 Total Loss: 0.679, BCE loss: 0.563, Diversity Loss: 0.231                     (Diversity_weight = 0.5)
Epoch: 1 Step: 51 Total Loss: 0.691, BCE loss: 0.567, Diversity Loss: 0.247                     (Diversity_weight = 0.5)
Epoch: 1 Step: 52 Total Loss: 0.629, BCE loss: 0.544, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 1 Step: 53 Total Loss: 0.646, BCE loss: 0.568, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 1 Step: 54 Total Loss: 0.565, BCE loss: 0.493, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 1 Step: 55 Total Loss: 0.804, BCE loss: 0.730, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 1 Step: 56 Total Loss: 0.659, BCE loss: 0.556, Diversity Loss: 0.206                     (Diversity_weight = 0.5)
Epoch: 1 Step: 57 Total Loss: 0.744, BCE loss: 0.670, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 1 Step: 58 Total Loss: 0.696, BCE loss: 0.635, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 1 Step: 59 Total Loss: 0.707, BCE loss: 0.651, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 1 Step: 60 Total Loss: 0.650, BCE loss: 0.592, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 1 Step: 61 Total Loss: 0.714, BCE loss: 0.660, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 1 Step: 62 Total Loss: 0.622, BCE loss: 0.551, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
{'accuracy': 0.62625250501002, 'roc_auc': 0.7345315487571701, 'pr_auc': 0.7005656315748894, 'conicity_mean': 0.15266071, 'conicity_std': 0.044679083}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.509    0.698      0.626      0.604         0.599
precision    0.818    0.567      0.626      0.692         0.698
recall       0.369    0.909      0.626      0.639         0.626
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7345315487571701
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:27:16_2021
Starting Epoch: 2
Epoch: 2 Step: 0 Total Loss: 0.801, BCE loss: 0.729, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 2 Step: 1 Total Loss: 0.632, BCE loss: 0.556, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 2 Step: 2 Total Loss: 0.641, BCE loss: 0.578, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 2 Step: 3 Total Loss: 0.610, BCE loss: 0.498, Diversity Loss: 0.225                     (Diversity_weight = 0.5)
Epoch: 2 Step: 4 Total Loss: 0.652, BCE loss: 0.580, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 2 Step: 5 Total Loss: 0.560, BCE loss: 0.500, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 2 Step: 6 Total Loss: 0.622, BCE loss: 0.556, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 2 Step: 7 Total Loss: 0.608, BCE loss: 0.518, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 2 Step: 8 Total Loss: 0.675, BCE loss: 0.594, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 2 Step: 9 Total Loss: 0.683, BCE loss: 0.626, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 2 Step: 10 Total Loss: 0.660, BCE loss: 0.595, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 2 Step: 11 Total Loss: 0.756, BCE loss: 0.661, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 2 Step: 12 Total Loss: 0.673, BCE loss: 0.612, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 2 Step: 13 Total Loss: 0.582, BCE loss: 0.497, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 2 Step: 14 Total Loss: 0.697, BCE loss: 0.533, Diversity Loss: 0.327                     (Diversity_weight = 0.5)
Epoch: 2 Step: 15 Total Loss: 0.615, BCE loss: 0.539, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 2 Step: 16 Total Loss: 0.643, BCE loss: 0.533, Diversity Loss: 0.221                     (Diversity_weight = 0.5)
Epoch: 2 Step: 17 Total Loss: 0.596, BCE loss: 0.550, Diversity Loss: 0.093                     (Diversity_weight = 0.5)
Epoch: 2 Step: 18 Total Loss: 0.629, BCE loss: 0.564, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 2 Step: 19 Total Loss: 0.629, BCE loss: 0.548, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 2 Step: 20 Total Loss: 0.723, BCE loss: 0.636, Diversity Loss: 0.174                     (Diversity_weight = 0.5)
Epoch: 2 Step: 21 Total Loss: 0.575, BCE loss: 0.470, Diversity Loss: 0.210                     (Diversity_weight = 0.5)
Epoch: 2 Step: 22 Total Loss: 0.566, BCE loss: 0.467, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 2 Step: 23 Total Loss: 0.548, BCE loss: 0.475, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 2 Step: 24 Total Loss: 0.573, BCE loss: 0.508, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 2 Step: 25 Total Loss: 0.692, BCE loss: 0.601, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 2 Step: 26 Total Loss: 0.559, BCE loss: 0.475, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 2 Step: 27 Total Loss: 0.605, BCE loss: 0.534, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 2 Step: 28 Total Loss: 0.584, BCE loss: 0.521, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 2 Step: 29 Total Loss: 0.604, BCE loss: 0.541, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 2 Step: 30 Total Loss: 0.657, BCE loss: 0.535, Diversity Loss: 0.243                     (Diversity_weight = 0.5)
Epoch: 2 Step: 31 Total Loss: 0.600, BCE loss: 0.539, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 2 Step: 32 Total Loss: 0.471, BCE loss: 0.393, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 2 Step: 33 Total Loss: 0.715, BCE loss: 0.638, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 2 Step: 34 Total Loss: 0.650, BCE loss: 0.589, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 2 Step: 35 Total Loss: 0.691, BCE loss: 0.618, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 2 Step: 36 Total Loss: 0.600, BCE loss: 0.531, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 2 Step: 37 Total Loss: 0.600, BCE loss: 0.544, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 2 Step: 38 Total Loss: 0.508, BCE loss: 0.452, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 2 Step: 39 Total Loss: 0.631, BCE loss: 0.562, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 2 Step: 40 Total Loss: 0.629, BCE loss: 0.557, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 2 Step: 41 Total Loss: 0.520, BCE loss: 0.453, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 2 Step: 42 Total Loss: 0.643, BCE loss: 0.592, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 2 Step: 43 Total Loss: 0.502, BCE loss: 0.426, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 2 Step: 44 Total Loss: 0.611, BCE loss: 0.553, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 2 Step: 45 Total Loss: 0.600, BCE loss: 0.534, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 2 Step: 46 Total Loss: 0.666, BCE loss: 0.597, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 2 Step: 47 Total Loss: 0.649, BCE loss: 0.595, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 2 Step: 48 Total Loss: 0.656, BCE loss: 0.519, Diversity Loss: 0.273                     (Diversity_weight = 0.5)
Epoch: 2 Step: 49 Total Loss: 0.691, BCE loss: 0.632, Diversity Loss: 0.118                     (Diversity_weight = 0.5)
Epoch: 2 Step: 50 Total Loss: 0.585, BCE loss: 0.511, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 2 Step: 51 Total Loss: 0.691, BCE loss: 0.645, Diversity Loss: 0.092                     (Diversity_weight = 0.5)
Epoch: 2 Step: 52 Total Loss: 0.697, BCE loss: 0.630, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 2 Step: 53 Total Loss: 0.560, BCE loss: 0.467, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 2 Step: 54 Total Loss: 0.662, BCE loss: 0.588, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 2 Step: 55 Total Loss: 0.735, BCE loss: 0.654, Diversity Loss: 0.162                     (Diversity_weight = 0.5)
Epoch: 2 Step: 56 Total Loss: 0.590, BCE loss: 0.508, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 2 Step: 57 Total Loss: 0.562, BCE loss: 0.483, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 2 Step: 58 Total Loss: 0.523, BCE loss: 0.462, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 2 Step: 59 Total Loss: 0.649, BCE loss: 0.554, Diversity Loss: 0.189                     (Diversity_weight = 0.5)
Epoch: 2 Step: 60 Total Loss: 0.463, BCE loss: 0.385, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 2 Step: 61 Total Loss: 0.607, BCE loss: 0.502, Diversity Loss: 0.210                     (Diversity_weight = 0.5)
Epoch: 2 Step: 62 Total Loss: 0.633, BCE loss: 0.548, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
{'accuracy': 0.6763527054108216, 'roc_auc': 0.7555399013786857, 'pr_auc': 0.7193435344256839, 'conicity_mean': 0.15951927, 'conicity_std': 0.051664215}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.723    0.610      0.676      0.667         0.670
precision    0.655    0.715      0.676      0.685         0.684
recall       0.807    0.533      0.676      0.670         0.676
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7555399013786857
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:27:16_2021
Starting Epoch: 3
Epoch: 3 Step: 0 Total Loss: 0.605, BCE loss: 0.522, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 3 Step: 1 Total Loss: 0.610, BCE loss: 0.543, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 3 Step: 2 Total Loss: 0.481, BCE loss: 0.378, Diversity Loss: 0.207                     (Diversity_weight = 0.5)
Epoch: 3 Step: 3 Total Loss: 0.433, BCE loss: 0.354, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 3 Step: 4 Total Loss: 0.581, BCE loss: 0.513, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 3 Step: 5 Total Loss: 0.362, BCE loss: 0.285, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 3 Step: 6 Total Loss: 0.670, BCE loss: 0.521, Diversity Loss: 0.297                     (Diversity_weight = 0.5)
Epoch: 3 Step: 7 Total Loss: 0.608, BCE loss: 0.537, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 3 Step: 8 Total Loss: 0.499, BCE loss: 0.441, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 3 Step: 9 Total Loss: 0.568, BCE loss: 0.500, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 3 Step: 10 Total Loss: 0.595, BCE loss: 0.533, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 3 Step: 11 Total Loss: 0.558, BCE loss: 0.497, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 3 Step: 12 Total Loss: 0.666, BCE loss: 0.582, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 3 Step: 13 Total Loss: 0.480, BCE loss: 0.400, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 3 Step: 14 Total Loss: 0.556, BCE loss: 0.450, Diversity Loss: 0.211                     (Diversity_weight = 0.5)
Epoch: 3 Step: 15 Total Loss: 0.703, BCE loss: 0.643, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 3 Step: 16 Total Loss: 0.475, BCE loss: 0.408, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 3 Step: 17 Total Loss: 0.482, BCE loss: 0.417, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 3 Step: 18 Total Loss: 0.580, BCE loss: 0.523, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 3 Step: 19 Total Loss: 0.464, BCE loss: 0.373, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 3 Step: 20 Total Loss: 0.440, BCE loss: 0.355, Diversity Loss: 0.170                     (Diversity_weight = 0.5)
Epoch: 3 Step: 21 Total Loss: 0.480, BCE loss: 0.359, Diversity Loss: 0.241                     (Diversity_weight = 0.5)
Epoch: 3 Step: 22 Total Loss: 0.539, BCE loss: 0.433, Diversity Loss: 0.212                     (Diversity_weight = 0.5)
Epoch: 3 Step: 23 Total Loss: 0.643, BCE loss: 0.564, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 3 Step: 24 Total Loss: 0.576, BCE loss: 0.460, Diversity Loss: 0.233                     (Diversity_weight = 0.5)
Epoch: 3 Step: 25 Total Loss: 0.585, BCE loss: 0.515, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 3 Step: 26 Total Loss: 0.581, BCE loss: 0.503, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 3 Step: 27 Total Loss: 0.465, BCE loss: 0.367, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 3 Step: 28 Total Loss: 0.541, BCE loss: 0.444, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 3 Step: 29 Total Loss: 0.544, BCE loss: 0.479, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 3 Step: 30 Total Loss: 0.581, BCE loss: 0.522, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 3 Step: 31 Total Loss: 0.522, BCE loss: 0.474, Diversity Loss: 0.097                     (Diversity_weight = 0.5)
Epoch: 3 Step: 32 Total Loss: 0.394, BCE loss: 0.333, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 3 Step: 33 Total Loss: 0.531, BCE loss: 0.456, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 3 Step: 34 Total Loss: 0.481, BCE loss: 0.411, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 3 Step: 35 Total Loss: 0.529, BCE loss: 0.466, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 3 Step: 36 Total Loss: 0.407, BCE loss: 0.345, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 3 Step: 37 Total Loss: 0.502, BCE loss: 0.432, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 3 Step: 38 Total Loss: 0.531, BCE loss: 0.444, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 3 Step: 39 Total Loss: 0.453, BCE loss: 0.383, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 3 Step: 40 Total Loss: 0.408, BCE loss: 0.338, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 3 Step: 41 Total Loss: 0.518, BCE loss: 0.442, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 3 Step: 42 Total Loss: 0.588, BCE loss: 0.496, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 3 Step: 43 Total Loss: 0.399, BCE loss: 0.343, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 3 Step: 44 Total Loss: 0.584, BCE loss: 0.533, Diversity Loss: 0.103                     (Diversity_weight = 0.5)
Epoch: 3 Step: 45 Total Loss: 0.484, BCE loss: 0.415, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 3 Step: 46 Total Loss: 0.457, BCE loss: 0.391, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 3 Step: 47 Total Loss: 0.638, BCE loss: 0.565, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 3 Step: 48 Total Loss: 0.421, BCE loss: 0.343, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 3 Step: 49 Total Loss: 0.460, BCE loss: 0.387, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 3 Step: 50 Total Loss: 0.433, BCE loss: 0.350, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 3 Step: 51 Total Loss: 0.464, BCE loss: 0.408, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 3 Step: 52 Total Loss: 0.606, BCE loss: 0.512, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
Epoch: 3 Step: 53 Total Loss: 0.575, BCE loss: 0.507, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 3 Step: 54 Total Loss: 0.524, BCE loss: 0.448, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 3 Step: 55 Total Loss: 0.679, BCE loss: 0.542, Diversity Loss: 0.275                     (Diversity_weight = 0.5)
Epoch: 3 Step: 56 Total Loss: 0.481, BCE loss: 0.419, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 3 Step: 57 Total Loss: 0.592, BCE loss: 0.504, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 3 Step: 58 Total Loss: 0.620, BCE loss: 0.545, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 3 Step: 59 Total Loss: 0.589, BCE loss: 0.509, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 3 Step: 60 Total Loss: 0.596, BCE loss: 0.550, Diversity Loss: 0.092                     (Diversity_weight = 0.5)
Epoch: 3 Step: 61 Total Loss: 0.729, BCE loss: 0.655, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 3 Step: 62 Total Loss: 0.432, BCE loss: 0.375, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
{'accuracy': 0.7044088176352705, 'roc_auc': 0.7887088658548856, 'pr_auc': 0.7572295201951273, 'conicity_mean': 0.1517367, 'conicity_std': 0.048988823}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.684    0.722      0.704      0.703         0.702
precision    0.778    0.653      0.704      0.716         0.719
recall       0.610    0.808      0.704      0.709         0.704
support    523.000  475.000    998.000    998.000       998.000
Model Saved on  roc_auc 0.7887088658548856
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:27:16_2021
Starting Epoch: 4
Epoch: 4 Step: 0 Total Loss: 0.446, BCE loss: 0.391, Diversity Loss: 0.110                     (Diversity_weight = 0.5)
Epoch: 4 Step: 1 Total Loss: 0.421, BCE loss: 0.338, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 4 Step: 2 Total Loss: 0.615, BCE loss: 0.490, Diversity Loss: 0.251                     (Diversity_weight = 0.5)
Epoch: 4 Step: 3 Total Loss: 0.342, BCE loss: 0.274, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 4 Step: 4 Total Loss: 0.484, BCE loss: 0.418, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 4 Step: 5 Total Loss: 0.387, BCE loss: 0.330, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 4 Step: 6 Total Loss: 0.393, BCE loss: 0.310, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 4 Step: 7 Total Loss: 0.514, BCE loss: 0.417, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 4 Step: 8 Total Loss: 0.425, BCE loss: 0.353, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 4 Step: 9 Total Loss: 0.362, BCE loss: 0.290, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 4 Step: 10 Total Loss: 0.334, BCE loss: 0.271, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 4 Step: 11 Total Loss: 0.542, BCE loss: 0.471, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 4 Step: 12 Total Loss: 0.429, BCE loss: 0.356, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 4 Step: 13 Total Loss: 0.454, BCE loss: 0.391, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 4 Step: 14 Total Loss: 0.356, BCE loss: 0.274, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 4 Step: 15 Total Loss: 0.535, BCE loss: 0.473, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 4 Step: 16 Total Loss: 0.382, BCE loss: 0.331, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 4 Step: 17 Total Loss: 0.405, BCE loss: 0.342, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 4 Step: 18 Total Loss: 0.343, BCE loss: 0.265, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 4 Step: 19 Total Loss: 0.393, BCE loss: 0.349, Diversity Loss: 0.088                     (Diversity_weight = 0.5)
Epoch: 4 Step: 20 Total Loss: 0.469, BCE loss: 0.379, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 4 Step: 21 Total Loss: 0.748, BCE loss: 0.656, Diversity Loss: 0.185                     (Diversity_weight = 0.5)
Epoch: 4 Step: 22 Total Loss: 0.644, BCE loss: 0.586, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 4 Step: 23 Total Loss: 0.500, BCE loss: 0.387, Diversity Loss: 0.227                     (Diversity_weight = 0.5)
Epoch: 4 Step: 24 Total Loss: 0.386, BCE loss: 0.315, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 4 Step: 25 Total Loss: 0.454, BCE loss: 0.394, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 4 Step: 26 Total Loss: 0.508, BCE loss: 0.453, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 4 Step: 27 Total Loss: 0.576, BCE loss: 0.508, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 4 Step: 28 Total Loss: 0.454, BCE loss: 0.385, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 4 Step: 29 Total Loss: 0.386, BCE loss: 0.279, Diversity Loss: 0.213                     (Diversity_weight = 0.5)
Epoch: 4 Step: 30 Total Loss: 0.288, BCE loss: 0.205, Diversity Loss: 0.166                     (Diversity_weight = 0.5)
Epoch: 4 Step: 31 Total Loss: 0.432, BCE loss: 0.370, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 4 Step: 32 Total Loss: 0.527, BCE loss: 0.427, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 4 Step: 33 Total Loss: 0.362, BCE loss: 0.282, Diversity Loss: 0.160                     (Diversity_weight = 0.5)
Epoch: 4 Step: 34 Total Loss: 0.346, BCE loss: 0.264, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 4 Step: 35 Total Loss: 0.581, BCE loss: 0.502, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 4 Step: 36 Total Loss: 0.289, BCE loss: 0.231, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 4 Step: 37 Total Loss: 0.436, BCE loss: 0.380, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 4 Step: 38 Total Loss: 0.454, BCE loss: 0.381, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 4 Step: 39 Total Loss: 0.406, BCE loss: 0.361, Diversity Loss: 0.092                     (Diversity_weight = 0.5)
Epoch: 4 Step: 40 Total Loss: 0.418, BCE loss: 0.343, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 4 Step: 41 Total Loss: 0.378, BCE loss: 0.266, Diversity Loss: 0.224                     (Diversity_weight = 0.5)
Epoch: 4 Step: 42 Total Loss: 0.281, BCE loss: 0.209, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 4 Step: 43 Total Loss: 0.486, BCE loss: 0.416, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 4 Step: 44 Total Loss: 0.289, BCE loss: 0.194, Diversity Loss: 0.192                     (Diversity_weight = 0.5)
Epoch: 4 Step: 45 Total Loss: 0.370, BCE loss: 0.239, Diversity Loss: 0.262                     (Diversity_weight = 0.5)
Epoch: 4 Step: 46 Total Loss: 0.516, BCE loss: 0.438, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 4 Step: 47 Total Loss: 0.776, BCE loss: 0.625, Diversity Loss: 0.302                     (Diversity_weight = 0.5)
Epoch: 4 Step: 48 Total Loss: 0.308, BCE loss: 0.233, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 4 Step: 49 Total Loss: 0.426, BCE loss: 0.349, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 4 Step: 50 Total Loss: 0.314, BCE loss: 0.261, Diversity Loss: 0.107                     (Diversity_weight = 0.5)
Epoch: 4 Step: 51 Total Loss: 0.491, BCE loss: 0.429, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 4 Step: 52 Total Loss: 0.431, BCE loss: 0.365, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 4 Step: 53 Total Loss: 0.485, BCE loss: 0.410, Diversity Loss: 0.149                     (Diversity_weight = 0.5)
Epoch: 4 Step: 54 Total Loss: 0.355, BCE loss: 0.288, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 4 Step: 55 Total Loss: 0.249, BCE loss: 0.182, Diversity Loss: 0.132                     (Diversity_weight = 0.5)
Epoch: 4 Step: 56 Total Loss: 0.424, BCE loss: 0.354, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 4 Step: 57 Total Loss: 0.390, BCE loss: 0.329, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 4 Step: 58 Total Loss: 0.463, BCE loss: 0.394, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 4 Step: 59 Total Loss: 0.429, BCE loss: 0.362, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 4 Step: 60 Total Loss: 0.501, BCE loss: 0.404, Diversity Loss: 0.193                     (Diversity_weight = 0.5)
Epoch: 4 Step: 61 Total Loss: 0.609, BCE loss: 0.523, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 4 Step: 62 Total Loss: 0.350, BCE loss: 0.257, Diversity Loss: 0.187                     (Diversity_weight = 0.5)
{'accuracy': 0.7164328657314629, 'roc_auc': 0.7830170071450137, 'pr_auc': 0.7396239847341257, 'conicity_mean': 0.15529457, 'conicity_std': 0.051029436}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.742    0.685      0.716      0.714         0.715
precision    0.709    0.726      0.716      0.718         0.717
recall       0.778    0.648      0.716      0.713         0.716
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7830170071450137
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:27:16_2021
Starting Epoch: 5
Epoch: 5 Step: 0 Total Loss: 0.321, BCE loss: 0.232, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 5 Step: 1 Total Loss: 0.463, BCE loss: 0.379, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 5 Step: 2 Total Loss: 0.344, BCE loss: 0.284, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 5 Step: 3 Total Loss: 0.350, BCE loss: 0.274, Diversity Loss: 0.152                     (Diversity_weight = 0.5)
Epoch: 5 Step: 4 Total Loss: 0.254, BCE loss: 0.182, Diversity Loss: 0.144                     (Diversity_weight = 0.5)
Epoch: 5 Step: 5 Total Loss: 0.331, BCE loss: 0.218, Diversity Loss: 0.225                     (Diversity_weight = 0.5)
Epoch: 5 Step: 6 Total Loss: 0.249, BCE loss: 0.180, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 5 Step: 7 Total Loss: 0.366, BCE loss: 0.302, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 5 Step: 8 Total Loss: 0.462, BCE loss: 0.317, Diversity Loss: 0.290                     (Diversity_weight = 0.5)
Epoch: 5 Step: 9 Total Loss: 0.423, BCE loss: 0.341, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 5 Step: 10 Total Loss: 0.338, BCE loss: 0.282, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 5 Step: 11 Total Loss: 0.414, BCE loss: 0.365, Diversity Loss: 0.098                     (Diversity_weight = 0.5)
Epoch: 5 Step: 12 Total Loss: 0.254, BCE loss: 0.192, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 5 Step: 13 Total Loss: 0.285, BCE loss: 0.224, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 5 Step: 14 Total Loss: 0.361, BCE loss: 0.304, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 5 Step: 15 Total Loss: 0.274, BCE loss: 0.217, Diversity Loss: 0.115                     (Diversity_weight = 0.5)
Epoch: 5 Step: 16 Total Loss: 0.215, BCE loss: 0.141, Diversity Loss: 0.148                     (Diversity_weight = 0.5)
Epoch: 5 Step: 17 Total Loss: 0.270, BCE loss: 0.209, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 5 Step: 18 Total Loss: 0.279, BCE loss: 0.208, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 5 Step: 19 Total Loss: 0.523, BCE loss: 0.467, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 5 Step: 20 Total Loss: 0.248, BCE loss: 0.184, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 5 Step: 21 Total Loss: 0.374, BCE loss: 0.282, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 5 Step: 22 Total Loss: 0.174, BCE loss: 0.123, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 5 Step: 23 Total Loss: 0.334, BCE loss: 0.269, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 5 Step: 24 Total Loss: 0.322, BCE loss: 0.249, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 5 Step: 25 Total Loss: 0.285, BCE loss: 0.201, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 5 Step: 26 Total Loss: 0.251, BCE loss: 0.176, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 5 Step: 27 Total Loss: 0.337, BCE loss: 0.275, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 5 Step: 28 Total Loss: 0.231, BCE loss: 0.164, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 5 Step: 29 Total Loss: 0.255, BCE loss: 0.179, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 5 Step: 30 Total Loss: 0.331, BCE loss: 0.261, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 5 Step: 31 Total Loss: 0.314, BCE loss: 0.205, Diversity Loss: 0.218                     (Diversity_weight = 0.5)
Epoch: 5 Step: 32 Total Loss: 0.365, BCE loss: 0.286, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 5 Step: 33 Total Loss: 0.420, BCE loss: 0.353, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 5 Step: 34 Total Loss: 0.354, BCE loss: 0.229, Diversity Loss: 0.251                     (Diversity_weight = 0.5)
Epoch: 5 Step: 35 Total Loss: 0.328, BCE loss: 0.259, Diversity Loss: 0.139                     (Diversity_weight = 0.5)
Epoch: 5 Step: 36 Total Loss: 0.459, BCE loss: 0.374, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 5 Step: 37 Total Loss: 0.362, BCE loss: 0.301, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 5 Step: 38 Total Loss: 0.232, BCE loss: 0.151, Diversity Loss: 0.163                     (Diversity_weight = 0.5)
Epoch: 5 Step: 39 Total Loss: 0.499, BCE loss: 0.365, Diversity Loss: 0.267                     (Diversity_weight = 0.5)
Epoch: 5 Step: 40 Total Loss: 0.288, BCE loss: 0.217, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 5 Step: 41 Total Loss: 0.246, BCE loss: 0.183, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 5 Step: 42 Total Loss: 0.343, BCE loss: 0.254, Diversity Loss: 0.178                     (Diversity_weight = 0.5)
Epoch: 5 Step: 43 Total Loss: 0.253, BCE loss: 0.178, Diversity Loss: 0.151                     (Diversity_weight = 0.5)
Epoch: 5 Step: 44 Total Loss: 0.338, BCE loss: 0.258, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 5 Step: 45 Total Loss: 0.475, BCE loss: 0.367, Diversity Loss: 0.217                     (Diversity_weight = 0.5)
Epoch: 5 Step: 46 Total Loss: 0.310, BCE loss: 0.220, Diversity Loss: 0.179                     (Diversity_weight = 0.5)
Epoch: 5 Step: 47 Total Loss: 0.265, BCE loss: 0.165, Diversity Loss: 0.200                     (Diversity_weight = 0.5)
Epoch: 5 Step: 48 Total Loss: 0.200, BCE loss: 0.128, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 5 Step: 49 Total Loss: 0.276, BCE loss: 0.186, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 5 Step: 50 Total Loss: 0.441, BCE loss: 0.394, Diversity Loss: 0.095                     (Diversity_weight = 0.5)
Epoch: 5 Step: 51 Total Loss: 0.337, BCE loss: 0.265, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 5 Step: 52 Total Loss: 0.457, BCE loss: 0.390, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 5 Step: 53 Total Loss: 0.304, BCE loss: 0.231, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 5 Step: 54 Total Loss: 0.283, BCE loss: 0.188, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 5 Step: 55 Total Loss: 0.457, BCE loss: 0.359, Diversity Loss: 0.196                     (Diversity_weight = 0.5)
Epoch: 5 Step: 56 Total Loss: 0.173, BCE loss: 0.104, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 5 Step: 57 Total Loss: 0.227, BCE loss: 0.164, Diversity Loss: 0.126                     (Diversity_weight = 0.5)
Epoch: 5 Step: 58 Total Loss: 0.317, BCE loss: 0.242, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 5 Step: 59 Total Loss: 0.388, BCE loss: 0.334, Diversity Loss: 0.108                     (Diversity_weight = 0.5)
Epoch: 5 Step: 60 Total Loss: 0.328, BCE loss: 0.250, Diversity Loss: 0.156                     (Diversity_weight = 0.5)
Epoch: 5 Step: 61 Total Loss: 0.246, BCE loss: 0.181, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 5 Step: 62 Total Loss: 0.213, BCE loss: 0.157, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
{'accuracy': 0.7244488977955912, 'roc_auc': 0.7827271812418236, 'pr_auc': 0.737686438092898, 'conicity_mean': 0.15218015, 'conicity_std': 0.049221877}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.740    0.707      0.724      0.723         0.724
precision    0.731    0.716      0.724      0.724         0.724
recall       0.750    0.697      0.724      0.723         0.724
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7827271812418236
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:27:16_2021
Starting Epoch: 6
Epoch: 6 Step: 0 Total Loss: 0.305, BCE loss: 0.247, Diversity Loss: 0.116                     (Diversity_weight = 0.5)
Epoch: 6 Step: 1 Total Loss: 0.138, BCE loss: 0.076, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 6 Step: 2 Total Loss: 0.258, BCE loss: 0.194, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 6 Step: 3 Total Loss: 0.412, BCE loss: 0.281, Diversity Loss: 0.264                     (Diversity_weight = 0.5)
Epoch: 6 Step: 4 Total Loss: 0.245, BCE loss: 0.174, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 6 Step: 5 Total Loss: 0.212, BCE loss: 0.134, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 6 Step: 6 Total Loss: 0.255, BCE loss: 0.198, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 6 Step: 7 Total Loss: 0.253, BCE loss: 0.159, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 6 Step: 8 Total Loss: 0.193, BCE loss: 0.111, Diversity Loss: 0.165                     (Diversity_weight = 0.5)
Epoch: 6 Step: 9 Total Loss: 0.421, BCE loss: 0.274, Diversity Loss: 0.294                     (Diversity_weight = 0.5)
Epoch: 6 Step: 10 Total Loss: 0.303, BCE loss: 0.234, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 6 Step: 11 Total Loss: 0.174, BCE loss: 0.090, Diversity Loss: 0.168                     (Diversity_weight = 0.5)
Epoch: 6 Step: 12 Total Loss: 0.151, BCE loss: 0.086, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 6 Step: 13 Total Loss: 0.140, BCE loss: 0.054, Diversity Loss: 0.171                     (Diversity_weight = 0.5)
Epoch: 6 Step: 14 Total Loss: 0.215, BCE loss: 0.141, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 6 Step: 15 Total Loss: 0.290, BCE loss: 0.193, Diversity Loss: 0.194                     (Diversity_weight = 0.5)
Epoch: 6 Step: 16 Total Loss: 0.216, BCE loss: 0.137, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 6 Step: 17 Total Loss: 0.222, BCE loss: 0.164, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 6 Step: 18 Total Loss: 0.235, BCE loss: 0.135, Diversity Loss: 0.199                     (Diversity_weight = 0.5)
Epoch: 6 Step: 19 Total Loss: 0.266, BCE loss: 0.199, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 6 Step: 20 Total Loss: 0.178, BCE loss: 0.116, Diversity Loss: 0.123                     (Diversity_weight = 0.5)
Epoch: 6 Step: 21 Total Loss: 0.144, BCE loss: 0.079, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 6 Step: 22 Total Loss: 0.198, BCE loss: 0.138, Diversity Loss: 0.119                     (Diversity_weight = 0.5)
Epoch: 6 Step: 23 Total Loss: 0.313, BCE loss: 0.201, Diversity Loss: 0.223                     (Diversity_weight = 0.5)
Epoch: 6 Step: 24 Total Loss: 0.394, BCE loss: 0.320, Diversity Loss: 0.147                     (Diversity_weight = 0.5)
Epoch: 6 Step: 25 Total Loss: 0.347, BCE loss: 0.274, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 6 Step: 26 Total Loss: 0.185, BCE loss: 0.105, Diversity Loss: 0.159                     (Diversity_weight = 0.5)
Epoch: 6 Step: 27 Total Loss: 0.254, BCE loss: 0.164, Diversity Loss: 0.180                     (Diversity_weight = 0.5)
Epoch: 6 Step: 28 Total Loss: 0.251, BCE loss: 0.148, Diversity Loss: 0.205                     (Diversity_weight = 0.5)
Epoch: 6 Step: 29 Total Loss: 0.126, BCE loss: 0.059, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 6 Step: 30 Total Loss: 0.247, BCE loss: 0.194, Diversity Loss: 0.105                     (Diversity_weight = 0.5)
Epoch: 6 Step: 31 Total Loss: 0.394, BCE loss: 0.339, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 6 Step: 32 Total Loss: 0.166, BCE loss: 0.118, Diversity Loss: 0.095                     (Diversity_weight = 0.5)
Epoch: 6 Step: 33 Total Loss: 0.224, BCE loss: 0.152, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 6 Step: 34 Total Loss: 0.308, BCE loss: 0.239, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 6 Step: 35 Total Loss: 0.187, BCE loss: 0.099, Diversity Loss: 0.176                     (Diversity_weight = 0.5)
Epoch: 6 Step: 36 Total Loss: 0.189, BCE loss: 0.122, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 6 Step: 37 Total Loss: 0.229, BCE loss: 0.178, Diversity Loss: 0.102                     (Diversity_weight = 0.5)
Epoch: 6 Step: 38 Total Loss: 0.279, BCE loss: 0.202, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 6 Step: 39 Total Loss: 0.217, BCE loss: 0.144, Diversity Loss: 0.145                     (Diversity_weight = 0.5)
Epoch: 6 Step: 40 Total Loss: 0.166, BCE loss: 0.101, Diversity Loss: 0.129                     (Diversity_weight = 0.5)
Epoch: 6 Step: 41 Total Loss: 0.267, BCE loss: 0.144, Diversity Loss: 0.245                     (Diversity_weight = 0.5)
Epoch: 6 Step: 42 Total Loss: 0.239, BCE loss: 0.155, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 6 Step: 43 Total Loss: 0.170, BCE loss: 0.091, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 6 Step: 44 Total Loss: 0.285, BCE loss: 0.175, Diversity Loss: 0.221                     (Diversity_weight = 0.5)
Epoch: 6 Step: 45 Total Loss: 0.391, BCE loss: 0.299, Diversity Loss: 0.184                     (Diversity_weight = 0.5)
Epoch: 6 Step: 46 Total Loss: 0.194, BCE loss: 0.129, Diversity Loss: 0.131                     (Diversity_weight = 0.5)
Epoch: 6 Step: 47 Total Loss: 0.113, BCE loss: 0.054, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 6 Step: 48 Total Loss: 0.231, BCE loss: 0.149, Diversity Loss: 0.164                     (Diversity_weight = 0.5)
Epoch: 6 Step: 49 Total Loss: 0.380, BCE loss: 0.313, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 6 Step: 50 Total Loss: 0.124, BCE loss: 0.050, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 6 Step: 51 Total Loss: 0.372, BCE loss: 0.277, Diversity Loss: 0.190                     (Diversity_weight = 0.5)
Epoch: 6 Step: 52 Total Loss: 0.093, BCE loss: 0.049, Diversity Loss: 0.089                     (Diversity_weight = 0.5)
Epoch: 6 Step: 53 Total Loss: 0.339, BCE loss: 0.268, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 6 Step: 54 Total Loss: 0.327, BCE loss: 0.270, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 6 Step: 55 Total Loss: 0.279, BCE loss: 0.216, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 6 Step: 56 Total Loss: 0.198, BCE loss: 0.125, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 6 Step: 57 Total Loss: 0.230, BCE loss: 0.150, Diversity Loss: 0.161                     (Diversity_weight = 0.5)
Epoch: 6 Step: 58 Total Loss: 0.247, BCE loss: 0.185, Diversity Loss: 0.124                     (Diversity_weight = 0.5)
Epoch: 6 Step: 59 Total Loss: 0.254, BCE loss: 0.190, Diversity Loss: 0.127                     (Diversity_weight = 0.5)
Epoch: 6 Step: 60 Total Loss: 0.244, BCE loss: 0.176, Diversity Loss: 0.135                     (Diversity_weight = 0.5)
Epoch: 6 Step: 61 Total Loss: 0.435, BCE loss: 0.358, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 6 Step: 62 Total Loss: 0.113, BCE loss: 0.042, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
{'accuracy': 0.7014028056112225, 'roc_auc': 0.7763892522894233, 'pr_auc': 0.7286460621864205, 'conicity_mean': 0.15036322, 'conicity_std': 0.04576735}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.695    0.707      0.701      0.701         0.701
precision    0.747    0.663      0.701      0.705         0.707
recall       0.650    0.758      0.701      0.704         0.701
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7763892522894233
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:27:16_2021
Starting Epoch: 7
Epoch: 7 Step: 0 Total Loss: 0.112, BCE loss: 0.052, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 7 Step: 1 Total Loss: 0.167, BCE loss: 0.097, Diversity Loss: 0.141                     (Diversity_weight = 0.5)
Epoch: 7 Step: 2 Total Loss: 0.140, BCE loss: 0.081, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 7 Step: 3 Total Loss: 0.241, BCE loss: 0.164, Diversity Loss: 0.154                     (Diversity_weight = 0.5)
Epoch: 7 Step: 4 Total Loss: 0.112, BCE loss: 0.064, Diversity Loss: 0.098                     (Diversity_weight = 0.5)
Epoch: 7 Step: 5 Total Loss: 0.198, BCE loss: 0.131, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 7 Step: 6 Total Loss: 0.226, BCE loss: 0.161, Diversity Loss: 0.128                     (Diversity_weight = 0.5)
Epoch: 7 Step: 7 Total Loss: 0.248, BCE loss: 0.176, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 7 Step: 8 Total Loss: 0.230, BCE loss: 0.162, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 7 Step: 9 Total Loss: 0.197, BCE loss: 0.102, Diversity Loss: 0.191                     (Diversity_weight = 0.5)
Epoch: 7 Step: 10 Total Loss: 0.157, BCE loss: 0.081, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 7 Step: 11 Total Loss: 0.105, BCE loss: 0.048, Diversity Loss: 0.113                     (Diversity_weight = 0.5)
Epoch: 7 Step: 12 Total Loss: 0.220, BCE loss: 0.143, Diversity Loss: 0.155                     (Diversity_weight = 0.5)
Epoch: 7 Step: 13 Total Loss: 0.255, BCE loss: 0.192, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 7 Step: 14 Total Loss: 0.169, BCE loss: 0.113, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 7 Step: 15 Total Loss: 0.090, BCE loss: 0.033, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 7 Step: 16 Total Loss: 0.175, BCE loss: 0.085, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 7 Step: 17 Total Loss: 0.145, BCE loss: 0.073, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
Epoch: 7 Step: 18 Total Loss: 0.141, BCE loss: 0.075, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 7 Step: 19 Total Loss: 0.376, BCE loss: 0.292, Diversity Loss: 0.167                     (Diversity_weight = 0.5)
Epoch: 7 Step: 20 Total Loss: 0.144, BCE loss: 0.057, Diversity Loss: 0.173                     (Diversity_weight = 0.5)
Epoch: 7 Step: 21 Total Loss: 0.146, BCE loss: 0.078, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 7 Step: 22 Total Loss: 0.154, BCE loss: 0.063, Diversity Loss: 0.181                     (Diversity_weight = 0.5)
Epoch: 7 Step: 23 Total Loss: 0.179, BCE loss: 0.063, Diversity Loss: 0.232                     (Diversity_weight = 0.5)
Epoch: 7 Step: 24 Total Loss: 0.148, BCE loss: 0.056, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 7 Step: 25 Total Loss: 0.127, BCE loss: 0.025, Diversity Loss: 0.204                     (Diversity_weight = 0.5)
Epoch: 7 Step: 26 Total Loss: 0.150, BCE loss: 0.071, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 7 Step: 27 Total Loss: 0.138, BCE loss: 0.088, Diversity Loss: 0.100                     (Diversity_weight = 0.5)
Epoch: 7 Step: 28 Total Loss: 0.116, BCE loss: 0.047, Diversity Loss: 0.138                     (Diversity_weight = 0.5)
Epoch: 7 Step: 29 Total Loss: 0.127, BCE loss: 0.049, Diversity Loss: 0.157                     (Diversity_weight = 0.5)
Epoch: 7 Step: 30 Total Loss: 0.324, BCE loss: 0.256, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 7 Step: 31 Total Loss: 0.130, BCE loss: 0.061, Diversity Loss: 0.137                     (Diversity_weight = 0.5)
Epoch: 7 Step: 32 Total Loss: 0.159, BCE loss: 0.052, Diversity Loss: 0.214                     (Diversity_weight = 0.5)
Epoch: 7 Step: 33 Total Loss: 0.263, BCE loss: 0.178, Diversity Loss: 0.169                     (Diversity_weight = 0.5)
Epoch: 7 Step: 34 Total Loss: 0.158, BCE loss: 0.095, Diversity Loss: 0.125                     (Diversity_weight = 0.5)
Epoch: 7 Step: 35 Total Loss: 0.204, BCE loss: 0.146, Diversity Loss: 0.117                     (Diversity_weight = 0.5)
Epoch: 7 Step: 36 Total Loss: 0.222, BCE loss: 0.162, Diversity Loss: 0.120                     (Diversity_weight = 0.5)
Epoch: 7 Step: 37 Total Loss: 0.095, BCE loss: 0.051, Diversity Loss: 0.088                     (Diversity_weight = 0.5)
Epoch: 7 Step: 38 Total Loss: 0.107, BCE loss: 0.042, Diversity Loss: 0.130                     (Diversity_weight = 0.5)
Epoch: 7 Step: 39 Total Loss: 0.316, BCE loss: 0.225, Diversity Loss: 0.183                     (Diversity_weight = 0.5)
Epoch: 7 Step: 40 Total Loss: 0.129, BCE loss: 0.062, Diversity Loss: 0.133                     (Diversity_weight = 0.5)
Epoch: 7 Step: 41 Total Loss: 0.127, BCE loss: 0.055, Diversity Loss: 0.143                     (Diversity_weight = 0.5)
Epoch: 7 Step: 42 Total Loss: 0.116, BCE loss: 0.049, Diversity Loss: 0.134                     (Diversity_weight = 0.5)
Epoch: 7 Step: 43 Total Loss: 0.135, BCE loss: 0.062, Diversity Loss: 0.146                     (Diversity_weight = 0.5)
Epoch: 7 Step: 44 Total Loss: 0.472, BCE loss: 0.416, Diversity Loss: 0.112                     (Diversity_weight = 0.5)
Epoch: 7 Step: 45 Total Loss: 0.160, BCE loss: 0.084, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 7 Step: 46 Total Loss: 0.085, BCE loss: 0.017, Diversity Loss: 0.136                     (Diversity_weight = 0.5)
Epoch: 7 Step: 47 Total Loss: 0.216, BCE loss: 0.159, Diversity Loss: 0.114                     (Diversity_weight = 0.5)
Epoch: 7 Step: 48 Total Loss: 0.303, BCE loss: 0.228, Diversity Loss: 0.150                     (Diversity_weight = 0.5)
Epoch: 7 Step: 49 Total Loss: 0.305, BCE loss: 0.168, Diversity Loss: 0.275                     (Diversity_weight = 0.5)
Epoch: 7 Step: 50 Total Loss: 0.344, BCE loss: 0.233, Diversity Loss: 0.222                     (Diversity_weight = 0.5)
Epoch: 7 Step: 51 Total Loss: 0.440, BCE loss: 0.292, Diversity Loss: 0.298                     (Diversity_weight = 0.5)
Epoch: 7 Step: 52 Total Loss: 0.136, BCE loss: 0.066, Diversity Loss: 0.140                     (Diversity_weight = 0.5)
Epoch: 7 Step: 53 Total Loss: 0.125, BCE loss: 0.049, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 7 Step: 54 Total Loss: 0.172, BCE loss: 0.078, Diversity Loss: 0.188                     (Diversity_weight = 0.5)
Epoch: 7 Step: 55 Total Loss: 0.154, BCE loss: 0.063, Diversity Loss: 0.182                     (Diversity_weight = 0.5)
Epoch: 7 Step: 56 Total Loss: 0.403, BCE loss: 0.280, Diversity Loss: 0.245                     (Diversity_weight = 0.5)
Epoch: 7 Step: 57 Total Loss: 0.223, BCE loss: 0.146, Diversity Loss: 0.153                     (Diversity_weight = 0.5)
Epoch: 7 Step: 58 Total Loss: 0.262, BCE loss: 0.183, Diversity Loss: 0.158                     (Diversity_weight = 0.5)
Epoch: 7 Step: 59 Total Loss: 0.127, BCE loss: 0.072, Diversity Loss: 0.111                     (Diversity_weight = 0.5)
Epoch: 7 Step: 60 Total Loss: 0.161, BCE loss: 0.101, Diversity Loss: 0.122                     (Diversity_weight = 0.5)
Epoch: 7 Step: 61 Total Loss: 0.111, BCE loss: 0.051, Diversity Loss: 0.121                     (Diversity_weight = 0.5)
Epoch: 7 Step: 62 Total Loss: 0.127, BCE loss: 0.056, Diversity Loss: 0.142                     (Diversity_weight = 0.5)
{'accuracy': 0.7164328657314629, 'roc_auc': 0.7717399617590822, 'pr_auc': 0.7224210251626958, 'conicity_mean': 0.15173362, 'conicity_std': 0.047921423}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.730    0.701      0.716      0.716         0.716
precision    0.728    0.703      0.716      0.716         0.716
recall       0.732    0.699      0.716      0.716         0.716
support    523.000  475.000    998.000    998.000       998.000
Model not saved on  roc_auc 0.7717399617590822
saved config  {'model': {'encoder': {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128}, 'decoder': {'attention': {'type': 'tanh'}, 'output_size': 1}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
./experiments/cls_jp/cls_jp/lstm+tanh__diversity_weight_0.5/Sun_Jan_17_19:27:16_2021
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/torch/nn/modules/rnn.py:54: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1
  "num_layers={}".format(dropout, num_layers))
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:75: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  hx = (Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))),
/home/kjk1n18/Transparency/Transparency/model/modules/lstm.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  Variable(nn.init.xavier_uniform(torch.empty(self.num_layers, batch_size, self.hidden_size,device=device))))
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:413: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
INFO - 2021-01-17 19:28:04,042 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:28:04,042 - type = vanillalstm
INFO - 2021-01-17 19:28:04,043 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:28:04,043 - vocab_size = 548
INFO - 2021-01-17 19:28:04,043 - embed_size = 200
INFO - 2021-01-17 19:28:04,043 - hidden_size = 128
INFO - 2021-01-17 19:28:04,043 - pre_embed = None
INFO - 2021-01-17 19:28:04,057 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:28:04,057 - hidden_size = 256
INFO - 2021-01-17 19:28:04,057 - output_size = 1
INFO - 2021-01-17 19:28:04,057 - use_attention = True
INFO - 2021-01-17 19:28:04,057 - regularizer_attention = None
INFO - 2021-01-17 19:28:04,057 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b35d286a5d0> and extras set()
INFO - 2021-01-17 19:28:04,057 - attention.type = tanh
INFO - 2021-01-17 19:28:04,057 - type = tanh
INFO - 2021-01-17 19:28:04,058 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b35d286a5d0> and extras set()
INFO - 2021-01-17 19:28:04,058 - attention.hidden_size = 256
INFO - 2021-01-17 19:28:04,058 - hidden_size = 256
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7237237237237237, 'roc_auc': 0.8033427079148253, 'pr_auc': 0.8156729733270266, 'conicity_mean': '0.14984746', 'conicity_std': '0.04579032'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.686    0.754      0.724      0.720         0.721
precision    0.747    0.708      0.724      0.727         0.727
recall       0.634    0.805      0.724      0.720         0.724
support    475.000  524.000    999.000    999.000       999.000
encoder params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None}
INFO - 2021-01-17 19:28:05,219 - instantiating class <class 'Transparency.model.modules.Encoder.Encoder'> from params {'vocab_size': 548, 'embed_size': 200, 'type': 'vanillalstm', 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:28:05,219 - type = vanillalstm
INFO - 2021-01-17 19:28:05,219 - instantiating class <class 'Transparency.model.modules.Encoder.EncoderorthoRNN'> from params {'vocab_size': 548, 'embed_size': 200, 'hidden_size': 128, 'pre_embed': None} and extras set()
INFO - 2021-01-17 19:28:05,219 - vocab_size = 548
INFO - 2021-01-17 19:28:05,219 - embed_size = 200
INFO - 2021-01-17 19:28:05,219 - hidden_size = 128
INFO - 2021-01-17 19:28:05,219 - pre_embed = None
INFO - 2021-01-17 19:28:05,233 - instantiating class <class 'Transparency.model.modules.Decoder.AttnDecoder'> from params {'attention': {'type': 'tanh'}, 'output_size': 1, 'hidden_size': 256} and extras set()
INFO - 2021-01-17 19:28:05,233 - hidden_size = 256
INFO - 2021-01-17 19:28:05,233 - output_size = 1
INFO - 2021-01-17 19:28:05,233 - use_attention = True
INFO - 2021-01-17 19:28:05,233 - regularizer_attention = None
INFO - 2021-01-17 19:28:05,233 - instantiating class <class 'Transparency.model.modules.Attention.Attention'> from params <allennlp.common.params.Params object at 0x2b35d263a090> and extras set()
INFO - 2021-01-17 19:28:05,234 - attention.type = tanh
INFO - 2021-01-17 19:28:05,234 - type = tanh
INFO - 2021-01-17 19:28:05,234 - instantiating class <class 'Transparency.model.modules.Attention.TanhAttention'> from params <allennlp.common.params.Params object at 0x2b35d263a090> and extras set()
INFO - 2021-01-17 19:28:05,234 - attention.hidden_size = 256
INFO - 2021-01-17 19:28:05,234 - hidden_size = 256
/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py:603: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
  if (len(np.array(data).shape) == 3):
config  {'model': {'encoder': {}, 'decoder': {}, 'generator': {'hidden_size': 64, 'sparsity_lambda': 0.5}}, 'training': {'bsize': 32, 'weight_decay': 1e-05, 'pos_weight': [1.0], 'basepath': './experiments/cls_jp', 'exp_dirname': 'cls_jp/lstm+tanh__diversity_weight_0.5', 'diversity_weight': 0.5, 'context_weight': 0}}
Running on device: cuda:0
{'accuracy': 0.7237237237237237, 'roc_auc': 0.8033427079148253, 'pr_auc': 0.8156729733270266, 'conicity_mean': '0.14984746', 'conicity_std': '0.04579032'}
                 0        1  micro avg  macro avg  weighted avg
f1-score     0.686    0.754      0.724      0.720         0.721
precision    0.747    0.708      0.724      0.727         0.727
recall       0.634    0.805      0.724      0.720         0.724
support    475.000  524.000    999.000    999.000       999.000
Running Gradients Expt ...
Dumping Gradients Outputs
Running Analysis by Parts-of-speech Expt ...
Traceback (most recent call last):
  File "train_and_run_experiments_bc.py", line 23, in <module>
    train_dataset_on_encoders(dataset, encoders)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 22, in train_dataset_on_encoders
    run_experiments_on_latest_model(dataset, e)
  File "/home/kjk1n18/Transparency/Transparency/ExperimentsBC.py", line 50, in run_experiments_on_latest_model
    evaluator.quantitative_analysis_experiment(test_data, dataset, force_run=force_run)
  File "/home/kjk1n18/Transparency/Transparency/Trainers/TrainerBC.py", line 191, in quantitative_analysis_experiment
    quant_output = self.model.quantitative_analysis(test_data.X,test_data.y,dataset)
  File "/home/kjk1n18/Transparency/Transparency/model/Binary_Classification.py", line 534, in quantitative_analysis
    tags = nltk.tag.pos_tag(words_pos)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 161, in pos_tag
    tagger = _get_tagger(lang)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/__init__.py", line 107, in _get_tagger
    tagger = PerceptronTagger()
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/tag/perceptron.py", line 162, in __init__
    find('taggers/averaged_perceptron_tagger/' + PICKLE)
  File "/home/kjk1n18/.conda/envs/maka_paper/lib/python3.7/site-packages/nltk/data.py", line 701, in find
    raise LookupError(resource_not_found)
LookupError: 
**********************************************************************
  Resource [93maveraged_perceptron_tagger[0m not found.
  Please use the NLTK Downloader to obtain the resource:

  [31m>>> import nltk
  >>> nltk.download('averaged_perceptron_tagger')
  [0m
  For more information see: https://www.nltk.org/data.html

  Attempted to load [93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle[0m

  Searched in:
    - '/home/kjk1n18/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/share/nltk_data'
    - '/home/kjk1n18/.conda/envs/maka_paper/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
**********************************************************************

==============================================================================
Running epilogue script on indigo51.

Submit time  : 2021-01-17T19:11:01
Start time   : 2021-01-17T19:11:22
End time     : 2021-01-17T19:28:00
Elapsed time : 00:16:38 (Timelimit=1-06:00:00)

Job ID: 1248468
Cluster: i5
User/Group: kjk1n18/fp
State: FAILED (exit code 1)
Cores: 1
CPU Utilized: 00:16:14
CPU Efficiency: 97.60% of 00:16:38 core-walltime
Job Wall-clock time: 00:16:38
Memory Utilized: 2.13 GB
Memory Efficiency: 0.00% of 0.00 MB

